{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6ehqRF7SZEEshF0Bvgd4d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaunck96/github_repo_inferencer/blob/main/Github_Repo_Inferencer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.35.2\n",
        "!pip install requests==2.31.0\n",
        "!pip install tqdm==4.66.1\n",
        "!pip install openai==0.28\n",
        "!pip install eyed3==0.9.7\n",
        "!pip install tiktoken==0.5.1\n",
        "!pip install langchain==0.0.340\n",
        "!pip install langchain_community\n",
        "!pip install google\n",
        "!pip install GitPython\n",
        "!pip install pydriller"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhIrlWnXlwHm",
        "outputId": "6d3619c3-6b2f-4dcd-af08-8dc364ada9fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.35.2 in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (2023.11.17)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (2023.11.17)\n",
            "Requirement already satisfied: tqdm==4.66.1 in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n",
            "Collecting eyed3==0.9.7\n",
            "  Downloading eyed3-0.9.7-py3-none-any.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.1/246.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coverage[toml]<6.0.0,>=5.3.1 (from eyed3==0.9.7)\n",
            "  Downloading coverage-5.5-cp310-cp310-manylinux1_x86_64.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecation<3.0.0,>=2.1.0 (from eyed3==0.9.7)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.0.7 (from eyed3==0.9.7)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from coverage[toml]<6.0.0,>=5.3.1->eyed3==0.9.7) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deprecation<3.0.0,>=2.1.0->eyed3==0.9.7) (23.2)\n",
            "Installing collected packages: filetype, deprecation, coverage, eyed3\n",
            "Successfully installed coverage-5.5 deprecation-2.1.0 eyed3-0.9.7 filetype-1.2.0\n",
            "Collecting tiktoken==0.5.1\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.1) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.1) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.5.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.5.1) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.1\n",
            "Collecting langchain==0.0.340\n",
            "  Downloading langchain-0.0.340-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (3.9.1)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.340)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.340)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.63 (from langchain==0.0.340)\n",
            "  Downloading langsmith-0.0.79-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.340)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.340) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.340) (3.0.3)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.340) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.340 langsmith-0.0.79 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.0.11-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.3)\n",
            "Collecting langchain-core<0.2,>=0.1.8 (from langchain_community)\n",
            "  Downloading langchain_core-0.1.9-py3-none-any.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.0.79)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.8->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.8->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.8->langchain_community) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.8->langchain_community) (1.10.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2023.11.17)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.8->langchain_community) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.8->langchain_community) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.8->langchain_community) (2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Installing collected packages: langchain-core, langchain_community\n",
            "Successfully installed langchain-core-0.1.9 langchain_community-0.0.11\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from google) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->google) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import os\n",
        "import openai\n",
        "import regex as re\n",
        "import json\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from googleapiclient.discovery import build\n",
        "import regex as re\n",
        "from transformers import pipeline\n",
        "import tiktoken\n",
        "import pandas as pd\n",
        "import re\n",
        "from urllib import parse\n",
        "import torch\n",
        "import ast\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from itertools import islice\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from itertools import islice\n",
        "import base64\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import os\n",
        "import torch\n",
        "import os\n",
        "import requests\n",
        "import googleapiclient.discovery\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
        "from langchain.llms import OpenAI\n",
        "import requests\n",
        "import requests\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.docstore.document import Document\n",
        "import nltk\n",
        "import time\n",
        "from typing import List\n",
        "from googlesearch import search\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvlXK1cklwFP",
        "outputId": "95a05e36-01c1-4028-97e2-98cbe1672de8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!# We need PyDriller to pull git repository information\n",
        "from pydriller import Repository\n",
        "\n",
        "# Replace this path with your own repository of interest\n",
        "path = 'https://github.com/chambbj/kpconv-pdal'\n",
        "repo = Repository(path)\n",
        "\n",
        "# Loop over each PyDriller commit to transform it to a commit usable for analysis later\n",
        "# NOTE: This can take a LONG time if there are many commits\n",
        "\n",
        "commits = []\n",
        "for commit in repo.traverse_commits():\n",
        "\n",
        "    hash = commit.hash\n",
        "\n",
        "    # Gather a list of files modified in the commit\n",
        "    files = []\n",
        "    try:\n",
        "        for f in commit.modified_files:\n",
        "            if f.new_path is not None:\n",
        "                files.append(f.new_path)\n",
        "    except Exception:\n",
        "        print('Could not read files for commit ' + hash)\n",
        "        continue\n",
        "\n",
        "    # Capture information about the commit in object format so I can reference it later\n",
        "    record = {\n",
        "        'hash': hash,\n",
        "        'message': commit.msg,\n",
        "        'author_name': commit.author.name,\n",
        "        'author_email': commit.author.email,\n",
        "        'author_date': commit.author_date,\n",
        "        'author_tz': commit.author_timezone,\n",
        "        'committer_name': commit.committer.name,\n",
        "        'committer_email': commit.committer.email,\n",
        "        'committer_date': commit.committer_date,\n",
        "        'committer_tz': commit.committer_timezone,\n",
        "        'in_main': commit.in_main_branch,\n",
        "        'is_merge': commit.merge,\n",
        "        'num_deletes': commit.deletions,\n",
        "        'num_inserts': commit.insertions,\n",
        "        'net_lines': commit.insertions - commit.deletions,\n",
        "        'num_files': commit.files,\n",
        "        'branches': ', '.join(commit.branches), # Comma separated list of branches the commit is found in\n",
        "        'files': ', '.join(files), # Comma separated list of files the commit modifies\n",
        "        'parents': ', '.join(commit.parents), # Comma separated list of parents\n",
        "        # PyDriller Open Source Delta Maintainability Model (OS-DMM) stat. See https://pydriller.readthedocs.io/en/latest/deltamaintainability.html for metric definitions\n",
        "        'dmm_unit_size': commit.dmm_unit_size,\n",
        "        'dmm_unit_complexity': commit.dmm_unit_complexity,\n",
        "        'dmm_unit_interfacing': commit.dmm_unit_interfacing,\n",
        "    }\n",
        "    # Omitted: modified_files (list), project_path, project_name\n",
        "    commits.append(record)\n",
        "\n",
        "# Translate this list of commits to a Pandas data frame\n",
        "df_commits = pd.DataFrame(commits)\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "df_commits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "-J8JS2wkK-YZ",
        "outputId": "067a94e9-ea26-404b-f57d-d5fd05797735"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       hash  \\\n",
              "0  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "1  375aadf9fa84cbb63ce866933398f022ae8e2952   \n",
              "\n",
              "                                             message         author_name  \\\n",
              "0                                     Initial commit  Bradley J Chambers   \n",
              "1  Add conda environment and install steps to REA...             chambbj   \n",
              "\n",
              "              author_email                author_date  author_tz  \\\n",
              "0  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "1  brad.chambers@gmail.com  2021-05-24 09:54:00-05:00      18000   \n",
              "\n",
              "       committer_name          committer_email             committer_date  \\\n",
              "0  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "1              GitHub       noreply@github.com  2021-05-24 09:54:00-05:00   \n",
              "\n",
              "   committer_tz  ...  num_deletes  num_inserts  net_lines  num_files  \\\n",
              "0         18000  ...            0        10849      10849         30   \n",
              "1         18000  ...            1           44         43          3   \n",
              "\n",
              "   branches                                              files  \\\n",
              "0      main  .gitignore, README.md, cpp_wrappers/compile_wr...   \n",
              "1      main        README.md, datasets/LAS.py, environment.yml   \n",
              "\n",
              "                                    parents dmm_unit_size dmm_unit_complexity  \\\n",
              "0                                                0.181067            0.263334   \n",
              "1  57647a552f45674ced3687d9629fa3aeacde9c4a      1.000000            1.000000   \n",
              "\n",
              "   dmm_unit_interfacing  \n",
              "0              0.257626  \n",
              "1              1.000000  \n",
              "\n",
              "[2 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0a5869c8-ca5c-49fa-97c4-ff6d981222d2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hash</th>\n",
              "      <th>message</th>\n",
              "      <th>author_name</th>\n",
              "      <th>author_email</th>\n",
              "      <th>author_date</th>\n",
              "      <th>author_tz</th>\n",
              "      <th>committer_name</th>\n",
              "      <th>committer_email</th>\n",
              "      <th>committer_date</th>\n",
              "      <th>committer_tz</th>\n",
              "      <th>...</th>\n",
              "      <th>num_deletes</th>\n",
              "      <th>num_inserts</th>\n",
              "      <th>net_lines</th>\n",
              "      <th>num_files</th>\n",
              "      <th>branches</th>\n",
              "      <th>files</th>\n",
              "      <th>parents</th>\n",
              "      <th>dmm_unit_size</th>\n",
              "      <th>dmm_unit_complexity</th>\n",
              "      <th>dmm_unit_interfacing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>.gitignore, README.md, cpp_wrappers/compile_wr...</td>\n",
              "      <td></td>\n",
              "      <td>0.181067</td>\n",
              "      <td>0.263334</td>\n",
              "      <td>0.257626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>375aadf9fa84cbb63ce866933398f022ae8e2952</td>\n",
              "      <td>Add conda environment and install steps to REA...</td>\n",
              "      <td>chambbj</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:54:00-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>GitHub</td>\n",
              "      <td>noreply@github.com</td>\n",
              "      <td>2021-05-24 09:54:00-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>44</td>\n",
              "      <td>43</td>\n",
              "      <td>3</td>\n",
              "      <td>main</td>\n",
              "      <td>README.md, datasets/LAS.py, environment.yml</td>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 22 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a5869c8-ca5c-49fa-97c4-ff6d981222d2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0a5869c8-ca5c-49fa-97c4-ff6d981222d2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0a5869c8-ca5c-49fa-97c4-ff6d981222d2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2b56e807-dcfc-4401-91d2-58869cc9d1d2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2b56e807-dcfc-4401-91d2-58869cc9d1d2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2b56e807-dcfc-4401-91d2-58869cc9d1d2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_dec2c2a7-9eef-4c20-b782-a447edff353d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_commits')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_dec2c2a7-9eef-4c20-b782-a447edff353d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_commits');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "commits = []\n",
        "\n",
        "for commit in repo.traverse_commits():\n",
        "    hash = commit.hash\n",
        "    try:\n",
        "        for f in commit.modified_files:\n",
        "            record = {\n",
        "                'hash': hash,\n",
        "                'message': commit.msg,\n",
        "                'author_name': commit.author.name,\n",
        "                'author_email': commit.author.email,\n",
        "                'author_date': commit.author_date,\n",
        "                'author_tz': commit.author_timezone,\n",
        "                'committer_name': commit.committer.name,\n",
        "                'committer_email': commit.committer.email,\n",
        "                'committer_date': commit.committer_date,\n",
        "                'committer_tz': commit.committer_timezone,\n",
        "                'in_main': commit.in_main_branch,\n",
        "                'is_merge': commit.merge,\n",
        "                'num_deletes': commit.deletions,\n",
        "                'num_inserts': commit.insertions,\n",
        "                'net_lines': commit.insertions - commit.deletions,\n",
        "                'num_files': commit.files,\n",
        "                'branches': ', '.join(commit.branches),\n",
        "                'filename': f.filename,\n",
        "                'old_path': f.old_path,\n",
        "                'new_path': f.new_path,\n",
        "                'project_name': commit.project_name,\n",
        "                'project_path': commit.project_path,\n",
        "                'parents': ', '.join(commit.parents),\n",
        "            }\n",
        "            # Omitted: modified_files (list), project_path, project_name\n",
        "            commits.append(record)\n",
        "    except Exception:\n",
        "        print('Problem reading commit ' + hash)\n",
        "        continue\n",
        "\n",
        "# Save it to FileCommits.csv\n",
        "df_file_commits = pd.DataFrame(commits)\n",
        "df_file_commits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y-nevAqzKegy",
        "outputId": "4148bec9-2a8f-420e-c15a-227163bfec12"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        hash  \\\n",
              "0   57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "1   57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "2   57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "3   57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "4   57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "5   57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "6   57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "7   57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "8   57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "9   57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "10  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "11  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "12  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "13  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "14  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "15  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "16  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "17  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "18  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "19  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "20  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "21  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "22  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "23  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "24  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "25  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "26  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "27  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "28  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "29  57647a552f45674ced3687d9629fa3aeacde9c4a   \n",
              "30  375aadf9fa84cbb63ce866933398f022ae8e2952   \n",
              "31  375aadf9fa84cbb63ce866933398f022ae8e2952   \n",
              "32  375aadf9fa84cbb63ce866933398f022ae8e2952   \n",
              "\n",
              "                                              message         author_name  \\\n",
              "0                                      Initial commit  Bradley J Chambers   \n",
              "1                                      Initial commit  Bradley J Chambers   \n",
              "2                                      Initial commit  Bradley J Chambers   \n",
              "3                                      Initial commit  Bradley J Chambers   \n",
              "4                                      Initial commit  Bradley J Chambers   \n",
              "5                                      Initial commit  Bradley J Chambers   \n",
              "6                                      Initial commit  Bradley J Chambers   \n",
              "7                                      Initial commit  Bradley J Chambers   \n",
              "8                                      Initial commit  Bradley J Chambers   \n",
              "9                                      Initial commit  Bradley J Chambers   \n",
              "10                                     Initial commit  Bradley J Chambers   \n",
              "11                                     Initial commit  Bradley J Chambers   \n",
              "12                                     Initial commit  Bradley J Chambers   \n",
              "13                                     Initial commit  Bradley J Chambers   \n",
              "14                                     Initial commit  Bradley J Chambers   \n",
              "15                                     Initial commit  Bradley J Chambers   \n",
              "16                                     Initial commit  Bradley J Chambers   \n",
              "17                                     Initial commit  Bradley J Chambers   \n",
              "18                                     Initial commit  Bradley J Chambers   \n",
              "19                                     Initial commit  Bradley J Chambers   \n",
              "20                                     Initial commit  Bradley J Chambers   \n",
              "21                                     Initial commit  Bradley J Chambers   \n",
              "22                                     Initial commit  Bradley J Chambers   \n",
              "23                                     Initial commit  Bradley J Chambers   \n",
              "24                                     Initial commit  Bradley J Chambers   \n",
              "25                                     Initial commit  Bradley J Chambers   \n",
              "26                                     Initial commit  Bradley J Chambers   \n",
              "27                                     Initial commit  Bradley J Chambers   \n",
              "28                                     Initial commit  Bradley J Chambers   \n",
              "29                                     Initial commit  Bradley J Chambers   \n",
              "30  Add conda environment and install steps to REA...             chambbj   \n",
              "31  Add conda environment and install steps to REA...             chambbj   \n",
              "32  Add conda environment and install steps to REA...             chambbj   \n",
              "\n",
              "               author_email                author_date  author_tz  \\\n",
              "0   brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "1   brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "2   brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "3   brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "4   brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "5   brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "6   brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "7   brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "8   brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "9   brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "10  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "11  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "12  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "13  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "14  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "15  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "16  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "17  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "18  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "19  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "20  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "21  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "22  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "23  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "24  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "25  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "26  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "27  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "28  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "29  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00      18000   \n",
              "30  brad.chambers@gmail.com  2021-05-24 09:54:00-05:00      18000   \n",
              "31  brad.chambers@gmail.com  2021-05-24 09:54:00-05:00      18000   \n",
              "32  brad.chambers@gmail.com  2021-05-24 09:54:00-05:00      18000   \n",
              "\n",
              "        committer_name          committer_email             committer_date  \\\n",
              "0   Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "1   Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "2   Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "3   Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "4   Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "5   Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "6   Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "7   Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "8   Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "9   Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "10  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "11  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "12  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "13  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "14  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "15  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "16  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "17  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "18  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "19  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "20  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "21  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "22  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "23  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "24  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "25  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "26  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "27  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "28  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "29  Bradley J Chambers  brad.chambers@gmail.com  2021-05-24 09:20:08-05:00   \n",
              "30              GitHub       noreply@github.com  2021-05-24 09:54:00-05:00   \n",
              "31              GitHub       noreply@github.com  2021-05-24 09:54:00-05:00   \n",
              "32              GitHub       noreply@github.com  2021-05-24 09:54:00-05:00   \n",
              "\n",
              "    committer_tz  ...  num_inserts  net_lines  num_files  branches  \\\n",
              "0          18000  ...        10849      10849         30      main   \n",
              "1          18000  ...        10849      10849         30      main   \n",
              "2          18000  ...        10849      10849         30      main   \n",
              "3          18000  ...        10849      10849         30      main   \n",
              "4          18000  ...        10849      10849         30      main   \n",
              "5          18000  ...        10849      10849         30      main   \n",
              "6          18000  ...        10849      10849         30      main   \n",
              "7          18000  ...        10849      10849         30      main   \n",
              "8          18000  ...        10849      10849         30      main   \n",
              "9          18000  ...        10849      10849         30      main   \n",
              "10         18000  ...        10849      10849         30      main   \n",
              "11         18000  ...        10849      10849         30      main   \n",
              "12         18000  ...        10849      10849         30      main   \n",
              "13         18000  ...        10849      10849         30      main   \n",
              "14         18000  ...        10849      10849         30      main   \n",
              "15         18000  ...        10849      10849         30      main   \n",
              "16         18000  ...        10849      10849         30      main   \n",
              "17         18000  ...        10849      10849         30      main   \n",
              "18         18000  ...        10849      10849         30      main   \n",
              "19         18000  ...        10849      10849         30      main   \n",
              "20         18000  ...        10849      10849         30      main   \n",
              "21         18000  ...        10849      10849         30      main   \n",
              "22         18000  ...        10849      10849         30      main   \n",
              "23         18000  ...        10849      10849         30      main   \n",
              "24         18000  ...        10849      10849         30      main   \n",
              "25         18000  ...        10849      10849         30      main   \n",
              "26         18000  ...        10849      10849         30      main   \n",
              "27         18000  ...        10849      10849         30      main   \n",
              "28         18000  ...        10849      10849         30      main   \n",
              "29         18000  ...        10849      10849         30      main   \n",
              "30         18000  ...           44         43          3      main   \n",
              "31         18000  ...           44         43          3      main   \n",
              "32         18000  ...           44         43          3      main   \n",
              "\n",
              "                            filename         old_path  \\\n",
              "0                         .gitignore             None   \n",
              "1                          README.md             None   \n",
              "2                compile_wrappers.sh             None   \n",
              "3                          build.bat             None   \n",
              "4                      neighbors.cpp             None   \n",
              "5                        neighbors.h             None   \n",
              "6                           setup.py             None   \n",
              "7                        wrapper.cpp             None   \n",
              "8                          build.bat             None   \n",
              "9               grid_subsampling.cpp             None   \n",
              "10                grid_subsampling.h             None   \n",
              "11                          setup.py             None   \n",
              "12                       wrapper.cpp             None   \n",
              "13                         cloud.cpp             None   \n",
              "14                           cloud.h             None   \n",
              "15                     nanoflann.hpp             None   \n",
              "16                            LAS.py             None   \n",
              "17                         common.py             None   \n",
              "18                  kernel_points.py             None   \n",
              "19                  architectures.py             None   \n",
              "20                         blocks.py             None   \n",
              "21                       test_LAS.py             None   \n",
              "22                      train_LAS.py             None   \n",
              "23                         config.py             None   \n",
              "24          jax-only-distribution.py             None   \n",
              "25  jax-only-sampled-distribution.py             None   \n",
              "26                            las.py             None   \n",
              "27                        metrics.py             None   \n",
              "28                         tester.py             None   \n",
              "29                        trainer.py             None   \n",
              "30                         README.md        README.md   \n",
              "31                            LAS.py  datasets/LAS.py   \n",
              "32                   environment.yml             None   \n",
              "\n",
              "                                             new_path project_name  \\\n",
              "0                                          .gitignore  kpconv-pdal   \n",
              "1                                           README.md  kpconv-pdal   \n",
              "2                    cpp_wrappers/compile_wrappers.sh  kpconv-pdal   \n",
              "3                cpp_wrappers/cpp_neighbors/build.bat  kpconv-pdal   \n",
              "4   cpp_wrappers/cpp_neighbors/neighbors/neighbors...  kpconv-pdal   \n",
              "5    cpp_wrappers/cpp_neighbors/neighbors/neighbors.h  kpconv-pdal   \n",
              "6                 cpp_wrappers/cpp_neighbors/setup.py  kpconv-pdal   \n",
              "7              cpp_wrappers/cpp_neighbors/wrapper.cpp  kpconv-pdal   \n",
              "8              cpp_wrappers/cpp_subsampling/build.bat  kpconv-pdal   \n",
              "9   cpp_wrappers/cpp_subsampling/grid_subsampling/...  kpconv-pdal   \n",
              "10  cpp_wrappers/cpp_subsampling/grid_subsampling/...  kpconv-pdal   \n",
              "11              cpp_wrappers/cpp_subsampling/setup.py  kpconv-pdal   \n",
              "12           cpp_wrappers/cpp_subsampling/wrapper.cpp  kpconv-pdal   \n",
              "13             cpp_wrappers/cpp_utils/cloud/cloud.cpp  kpconv-pdal   \n",
              "14               cpp_wrappers/cpp_utils/cloud/cloud.h  kpconv-pdal   \n",
              "15     cpp_wrappers/cpp_utils/nanoflann/nanoflann.hpp  kpconv-pdal   \n",
              "16                                    datasets/LAS.py  kpconv-pdal   \n",
              "17                                 datasets/common.py  kpconv-pdal   \n",
              "18                           kernels/kernel_points.py  kpconv-pdal   \n",
              "19                            models/architectures.py  kpconv-pdal   \n",
              "20                                   models/blocks.py  kpconv-pdal   \n",
              "21                                        test_LAS.py  kpconv-pdal   \n",
              "22                                       train_LAS.py  kpconv-pdal   \n",
              "23                                    utils/config.py  kpconv-pdal   \n",
              "24                     utils/jax-only-distribution.py  kpconv-pdal   \n",
              "25             utils/jax-only-sampled-distribution.py  kpconv-pdal   \n",
              "26                                       utils/las.py  kpconv-pdal   \n",
              "27                                   utils/metrics.py  kpconv-pdal   \n",
              "28                                    utils/tester.py  kpconv-pdal   \n",
              "29                                   utils/trainer.py  kpconv-pdal   \n",
              "30                                          README.md  kpconv-pdal   \n",
              "31                                    datasets/LAS.py  kpconv-pdal   \n",
              "32                                    environment.yml  kpconv-pdal   \n",
              "\n",
              "                    project_path                                   parents  \n",
              "0   /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "1   /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "2   /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "3   /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "4   /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "5   /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "6   /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "7   /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "8   /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "9   /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "10  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "11  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "12  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "13  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "14  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "15  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "16  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "17  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "18  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "19  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "20  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "21  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "22  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "23  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "24  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "25  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "26  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "27  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "28  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "29  /tmp/tmphhm26oiq/kpconv-pdal                                            \n",
              "30  /tmp/tmphhm26oiq/kpconv-pdal  57647a552f45674ced3687d9629fa3aeacde9c4a  \n",
              "31  /tmp/tmphhm26oiq/kpconv-pdal  57647a552f45674ced3687d9629fa3aeacde9c4a  \n",
              "32  /tmp/tmphhm26oiq/kpconv-pdal  57647a552f45674ced3687d9629fa3aeacde9c4a  \n",
              "\n",
              "[33 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24158e3f-65a2-426a-8e8f-902221ed55c0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hash</th>\n",
              "      <th>message</th>\n",
              "      <th>author_name</th>\n",
              "      <th>author_email</th>\n",
              "      <th>author_date</th>\n",
              "      <th>author_tz</th>\n",
              "      <th>committer_name</th>\n",
              "      <th>committer_email</th>\n",
              "      <th>committer_date</th>\n",
              "      <th>committer_tz</th>\n",
              "      <th>...</th>\n",
              "      <th>num_inserts</th>\n",
              "      <th>net_lines</th>\n",
              "      <th>num_files</th>\n",
              "      <th>branches</th>\n",
              "      <th>filename</th>\n",
              "      <th>old_path</th>\n",
              "      <th>new_path</th>\n",
              "      <th>project_name</th>\n",
              "      <th>project_path</th>\n",
              "      <th>parents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>.gitignore</td>\n",
              "      <td>None</td>\n",
              "      <td>.gitignore</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>README.md</td>\n",
              "      <td>None</td>\n",
              "      <td>README.md</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>compile_wrappers.sh</td>\n",
              "      <td>None</td>\n",
              "      <td>cpp_wrappers/compile_wrappers.sh</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>build.bat</td>\n",
              "      <td>None</td>\n",
              "      <td>cpp_wrappers/cpp_neighbors/build.bat</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>neighbors.cpp</td>\n",
              "      <td>None</td>\n",
              "      <td>cpp_wrappers/cpp_neighbors/neighbors/neighbors...</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>neighbors.h</td>\n",
              "      <td>None</td>\n",
              "      <td>cpp_wrappers/cpp_neighbors/neighbors/neighbors.h</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>setup.py</td>\n",
              "      <td>None</td>\n",
              "      <td>cpp_wrappers/cpp_neighbors/setup.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>wrapper.cpp</td>\n",
              "      <td>None</td>\n",
              "      <td>cpp_wrappers/cpp_neighbors/wrapper.cpp</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>build.bat</td>\n",
              "      <td>None</td>\n",
              "      <td>cpp_wrappers/cpp_subsampling/build.bat</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>grid_subsampling.cpp</td>\n",
              "      <td>None</td>\n",
              "      <td>cpp_wrappers/cpp_subsampling/grid_subsampling/...</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>grid_subsampling.h</td>\n",
              "      <td>None</td>\n",
              "      <td>cpp_wrappers/cpp_subsampling/grid_subsampling/...</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>setup.py</td>\n",
              "      <td>None</td>\n",
              "      <td>cpp_wrappers/cpp_subsampling/setup.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>wrapper.cpp</td>\n",
              "      <td>None</td>\n",
              "      <td>cpp_wrappers/cpp_subsampling/wrapper.cpp</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>cloud.cpp</td>\n",
              "      <td>None</td>\n",
              "      <td>cpp_wrappers/cpp_utils/cloud/cloud.cpp</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>cloud.h</td>\n",
              "      <td>None</td>\n",
              "      <td>cpp_wrappers/cpp_utils/cloud/cloud.h</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>nanoflann.hpp</td>\n",
              "      <td>None</td>\n",
              "      <td>cpp_wrappers/cpp_utils/nanoflann/nanoflann.hpp</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>LAS.py</td>\n",
              "      <td>None</td>\n",
              "      <td>datasets/LAS.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>common.py</td>\n",
              "      <td>None</td>\n",
              "      <td>datasets/common.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>kernel_points.py</td>\n",
              "      <td>None</td>\n",
              "      <td>kernels/kernel_points.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>architectures.py</td>\n",
              "      <td>None</td>\n",
              "      <td>models/architectures.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>blocks.py</td>\n",
              "      <td>None</td>\n",
              "      <td>models/blocks.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>test_LAS.py</td>\n",
              "      <td>None</td>\n",
              "      <td>test_LAS.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>train_LAS.py</td>\n",
              "      <td>None</td>\n",
              "      <td>train_LAS.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>config.py</td>\n",
              "      <td>None</td>\n",
              "      <td>utils/config.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>jax-only-distribution.py</td>\n",
              "      <td>None</td>\n",
              "      <td>utils/jax-only-distribution.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>jax-only-sampled-distribution.py</td>\n",
              "      <td>None</td>\n",
              "      <td>utils/jax-only-sampled-distribution.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>las.py</td>\n",
              "      <td>None</td>\n",
              "      <td>utils/las.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>metrics.py</td>\n",
              "      <td>None</td>\n",
              "      <td>utils/metrics.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>tester.py</td>\n",
              "      <td>None</td>\n",
              "      <td>utils/tester.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "      <td>Initial commit</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>Bradley J Chambers</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:20:08-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>10849</td>\n",
              "      <td>10849</td>\n",
              "      <td>30</td>\n",
              "      <td>main</td>\n",
              "      <td>trainer.py</td>\n",
              "      <td>None</td>\n",
              "      <td>utils/trainer.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>375aadf9fa84cbb63ce866933398f022ae8e2952</td>\n",
              "      <td>Add conda environment and install steps to REA...</td>\n",
              "      <td>chambbj</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:54:00-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>GitHub</td>\n",
              "      <td>noreply@github.com</td>\n",
              "      <td>2021-05-24 09:54:00-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>44</td>\n",
              "      <td>43</td>\n",
              "      <td>3</td>\n",
              "      <td>main</td>\n",
              "      <td>README.md</td>\n",
              "      <td>README.md</td>\n",
              "      <td>README.md</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>375aadf9fa84cbb63ce866933398f022ae8e2952</td>\n",
              "      <td>Add conda environment and install steps to REA...</td>\n",
              "      <td>chambbj</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:54:00-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>GitHub</td>\n",
              "      <td>noreply@github.com</td>\n",
              "      <td>2021-05-24 09:54:00-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>44</td>\n",
              "      <td>43</td>\n",
              "      <td>3</td>\n",
              "      <td>main</td>\n",
              "      <td>LAS.py</td>\n",
              "      <td>datasets/LAS.py</td>\n",
              "      <td>datasets/LAS.py</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>375aadf9fa84cbb63ce866933398f022ae8e2952</td>\n",
              "      <td>Add conda environment and install steps to REA...</td>\n",
              "      <td>chambbj</td>\n",
              "      <td>brad.chambers@gmail.com</td>\n",
              "      <td>2021-05-24 09:54:00-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>GitHub</td>\n",
              "      <td>noreply@github.com</td>\n",
              "      <td>2021-05-24 09:54:00-05:00</td>\n",
              "      <td>18000</td>\n",
              "      <td>...</td>\n",
              "      <td>44</td>\n",
              "      <td>43</td>\n",
              "      <td>3</td>\n",
              "      <td>main</td>\n",
              "      <td>environment.yml</td>\n",
              "      <td>None</td>\n",
              "      <td>environment.yml</td>\n",
              "      <td>kpconv-pdal</td>\n",
              "      <td>/tmp/tmphhm26oiq/kpconv-pdal</td>\n",
              "      <td>57647a552f45674ced3687d9629fa3aeacde9c4a</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>33 rows × 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24158e3f-65a2-426a-8e8f-902221ed55c0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-24158e3f-65a2-426a-8e8f-902221ed55c0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-24158e3f-65a2-426a-8e8f-902221ed55c0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-496e03b9-78d1-453e-9ca7-89835e5951dd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-496e03b9-78d1-453e-9ca7-89835e5951dd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-496e03b9-78d1-453e-9ca7-89835e5951dd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_97307f4e-5a21-485b-aa72-d11743a89a46\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_file_commits')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_97307f4e-5a21-485b-aa72-d11743a89a46 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_file_commits');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from git import Repo\n",
        "\n",
        "# Clone the repository (replace with your repository URL)\n",
        "repo_url = 'https://github.com/chambbj/kpconv-pdal.git'\n",
        "repo_path = '/content/kpconv-pdal-4'\n",
        "Repo.clone_from(repo_url, repo_path)\n",
        "\n",
        "# Initialize a list to store data\n",
        "data = []\n",
        "\n",
        "# Walk through the repository\n",
        "for root, dirs, files in os.walk(repo_path):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file_content:\n",
        "            try:\n",
        "                content = file_content.read()\n",
        "            except Exception as e:\n",
        "                content = f\"Error reading file: {e}\"\n",
        "\n",
        "            data.append({\n",
        "                'folder': os.path.basename(root),\n",
        "                'file_name': file,\n",
        "                'content': content\n",
        "            })\n",
        "\n",
        "# Create a DataFrame\n",
        "repo_contents = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "repo_contents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yVk2GXunM8GR",
        "outputId": "c8bba6ce-334c-4e6d-b93c-8ab8197834f6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              folder                                          file_name  \\\n",
              "0      kpconv-pdal-4                                       train_LAS.py   \n",
              "1      kpconv-pdal-4                                        test_LAS.py   \n",
              "2      kpconv-pdal-4                                          README.md   \n",
              "3      kpconv-pdal-4                                         .gitignore   \n",
              "4      kpconv-pdal-4                                    environment.yml   \n",
              "5               .git                                               HEAD   \n",
              "6               .git                                        description   \n",
              "7               .git                                        packed-refs   \n",
              "8               .git                                              index   \n",
              "9               .git                                             config   \n",
              "10            origin                                               HEAD   \n",
              "11             heads                                               main   \n",
              "12              logs                                               HEAD   \n",
              "13            origin                                               HEAD   \n",
              "14             heads                                               main   \n",
              "15              pack  pack-6ec1a2b6ae05c77a50473fa5ffcc00f3b85b2946....   \n",
              "16              pack  pack-6ec1a2b6ae05c77a50473fa5ffcc00f3b85b2946.idx   \n",
              "17             hooks                          fsmonitor-watchman.sample   \n",
              "18             hooks                                  pre-rebase.sample   \n",
              "19             hooks                                 post-update.sample   \n",
              "20             hooks                                 pre-receive.sample   \n",
              "21             hooks                              applypatch-msg.sample   \n",
              "22             hooks                            push-to-checkout.sample   \n",
              "23             hooks                                      update.sample   \n",
              "24             hooks                              pre-applypatch.sample   \n",
              "25             hooks                            pre-merge-commit.sample   \n",
              "26             hooks                                  commit-msg.sample   \n",
              "27             hooks                                  pre-commit.sample   \n",
              "28             hooks                          prepare-commit-msg.sample   \n",
              "29             hooks                                    pre-push.sample   \n",
              "30              info                                            exclude   \n",
              "31             utils                   jax-only-sampled-distribution.py   \n",
              "32             utils                                         trainer.py   \n",
              "33             utils                                          tester.py   \n",
              "34             utils                                          config.py   \n",
              "35             utils                           jax-only-distribution.py   \n",
              "36             utils                                         metrics.py   \n",
              "37             utils                                             las.py   \n",
              "38      cpp_wrappers                                compile_wrappers.sh   \n",
              "39     cpp_neighbors                                          build.bat   \n",
              "40     cpp_neighbors                                           setup.py   \n",
              "41     cpp_neighbors                                        wrapper.cpp   \n",
              "42         neighbors                                      neighbors.cpp   \n",
              "43         neighbors                                        neighbors.h   \n",
              "44   cpp_subsampling                                          build.bat   \n",
              "45   cpp_subsampling                                           setup.py   \n",
              "46   cpp_subsampling                                        wrapper.cpp   \n",
              "47  grid_subsampling                                 grid_subsampling.h   \n",
              "48  grid_subsampling                               grid_subsampling.cpp   \n",
              "49             cloud                                          cloud.cpp   \n",
              "50             cloud                                            cloud.h   \n",
              "51         nanoflann                                      nanoflann.hpp   \n",
              "52            models                                   architectures.py   \n",
              "53            models                                          blocks.py   \n",
              "54           kernels                                   kernel_points.py   \n",
              "55          datasets                                          common.py   \n",
              "56          datasets                                             LAS.py   \n",
              "\n",
              "                                              content  \n",
              "0   #\\n#\\n#      0================================...  \n",
              "1   #\\n#\\n#      0================================...  \n",
              "2   Derivative work of https://github.com/HuguesTH...  \n",
              "3   __pycache__\\n*.o\\n*.so\\n.vscode\\nresults/\\nrun...  \n",
              "4   name: kpconv-pdal-env\\nchannels:\\n  - conda-fo...  \n",
              "5                              ref: refs/heads/main\\n  \n",
              "6   Unnamed repository; edit this file 'descriptio...  \n",
              "7   # pack-refs with: peeled fully-peeled sorted \\...  \n",
              "8   Error reading file: 'utf-8' codec can't decode...  \n",
              "9   [core]\\n\\trepositoryformatversion = 0\\n\\tfilem...  \n",
              "10                    ref: refs/remotes/origin/main\\n  \n",
              "11         375aadf9fa84cbb63ce866933398f022ae8e2952\\n  \n",
              "12  0000000000000000000000000000000000000000 375aa...  \n",
              "13  0000000000000000000000000000000000000000 375aa...  \n",
              "14  0000000000000000000000000000000000000000 375aa...  \n",
              "15  Error reading file: 'utf-8' codec can't decode...  \n",
              "16  Error reading file: 'utf-8' codec can't decode...  \n",
              "17  #!/usr/bin/perl\\n\\nuse strict;\\nuse warnings;\\...  \n",
              "18  #!/bin/sh\\n#\\n# Copyright (c) 2006, 2008 Junio...  \n",
              "19  #!/bin/sh\\n#\\n# An example hook script to prep...  \n",
              "20  #!/bin/sh\\n#\\n# An example hook script to make...  \n",
              "21  #!/bin/sh\\n#\\n# An example hook script to chec...  \n",
              "22  #!/bin/sh\\n\\n# An example hook script to updat...  \n",
              "23  #!/bin/sh\\n#\\n# An example hook script to bloc...  \n",
              "24  #!/bin/sh\\n#\\n# An example hook script to veri...  \n",
              "25  #!/bin/sh\\n#\\n# An example hook script to veri...  \n",
              "26  #!/bin/sh\\n#\\n# An example hook script to chec...  \n",
              "27  #!/bin/sh\\n#\\n# An example hook script to veri...  \n",
              "28  #!/bin/sh\\n#\\n# An example hook script to prep...  \n",
              "29  #!/bin/sh\\n\\n# An example hook script to verif...  \n",
              "30  # git ls-files --others --exclude-from=.git/in...  \n",
              "31  from os import listdir, path, rename\\n\\nimport...  \n",
              "32  #\\n#\\n#      0================================...  \n",
              "33  #\\n#\\n#      0================================...  \n",
              "34  #\\n#\\n#      0================================...  \n",
              "35  from os import listdir, path, rename\\n\\nimport...  \n",
              "36  #\\n#\\n#      0================================...  \n",
              "37  import json\\nimport numpy as np\\nimport pdal\\n...  \n",
              "38  #!/bin/bash\\n\\n# Compile cpp subsampling\\ncd c...  \n",
              "39  @echo off\\npy setup.py build_ext --inplace\\n\\n...  \n",
              "40  from distutils.core import setup, Extension\\ni...  \n",
              "41  #include <Python.h>\\n#include <numpy/arrayobje...  \n",
              "42  \\n#include \"neighbors.h\"\\n\\n\\nvoid brute_neigh...  \n",
              "43  \\n\\n#include \"../../cpp_utils/cloud/cloud.h\"\\n...  \n",
              "44  @echo off\\npy setup.py build_ext --inplace\\n\\n...  \n",
              "45  from distutils.core import setup, Extension\\ni...  \n",
              "46  #include <Python.h>\\n#include <numpy/arrayobje...  \n",
              "47  \\n\\n#include \"../../cpp_utils/cloud/cloud.h\"\\n...  \n",
              "48  \\n#include \"grid_subsampling.h\"\\n\\n\\nvoid grid...  \n",
              "49  //\\n//\\n//\\t\\t0==========================0\\n//...  \n",
              "50  //\\n//\\n//\\t\\t0==========================0\\n//...  \n",
              "51  /*********************************************...  \n",
              "52  #\\n#\\n#      0================================...  \n",
              "53  #\\n#\\n#      0================================...  \n",
              "54  #\\n#\\n#      0================================...  \n",
              "55  #\\n#\\n#      0================================...  \n",
              "56  #\\n#\\n#      0================================...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90e6dea1-7391-4880-978e-762e388f7fe7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>folder</th>\n",
              "      <th>file_name</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>kpconv-pdal-4</td>\n",
              "      <td>train_LAS.py</td>\n",
              "      <td>#\\n#\\n#      0================================...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>kpconv-pdal-4</td>\n",
              "      <td>test_LAS.py</td>\n",
              "      <td>#\\n#\\n#      0================================...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>kpconv-pdal-4</td>\n",
              "      <td>README.md</td>\n",
              "      <td>Derivative work of https://github.com/HuguesTH...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>kpconv-pdal-4</td>\n",
              "      <td>.gitignore</td>\n",
              "      <td>__pycache__\\n*.o\\n*.so\\n.vscode\\nresults/\\nrun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>kpconv-pdal-4</td>\n",
              "      <td>environment.yml</td>\n",
              "      <td>name: kpconv-pdal-env\\nchannels:\\n  - conda-fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>.git</td>\n",
              "      <td>HEAD</td>\n",
              "      <td>ref: refs/heads/main\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>.git</td>\n",
              "      <td>description</td>\n",
              "      <td>Unnamed repository; edit this file 'descriptio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>.git</td>\n",
              "      <td>packed-refs</td>\n",
              "      <td># pack-refs with: peeled fully-peeled sorted \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>.git</td>\n",
              "      <td>index</td>\n",
              "      <td>Error reading file: 'utf-8' codec can't decode...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>.git</td>\n",
              "      <td>config</td>\n",
              "      <td>[core]\\n\\trepositoryformatversion = 0\\n\\tfilem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>origin</td>\n",
              "      <td>HEAD</td>\n",
              "      <td>ref: refs/remotes/origin/main\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>heads</td>\n",
              "      <td>main</td>\n",
              "      <td>375aadf9fa84cbb63ce866933398f022ae8e2952\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>logs</td>\n",
              "      <td>HEAD</td>\n",
              "      <td>0000000000000000000000000000000000000000 375aa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>origin</td>\n",
              "      <td>HEAD</td>\n",
              "      <td>0000000000000000000000000000000000000000 375aa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>heads</td>\n",
              "      <td>main</td>\n",
              "      <td>0000000000000000000000000000000000000000 375aa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>pack</td>\n",
              "      <td>pack-6ec1a2b6ae05c77a50473fa5ffcc00f3b85b2946....</td>\n",
              "      <td>Error reading file: 'utf-8' codec can't decode...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>pack</td>\n",
              "      <td>pack-6ec1a2b6ae05c77a50473fa5ffcc00f3b85b2946.idx</td>\n",
              "      <td>Error reading file: 'utf-8' codec can't decode...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>hooks</td>\n",
              "      <td>fsmonitor-watchman.sample</td>\n",
              "      <td>#!/usr/bin/perl\\n\\nuse strict;\\nuse warnings;\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>hooks</td>\n",
              "      <td>pre-rebase.sample</td>\n",
              "      <td>#!/bin/sh\\n#\\n# Copyright (c) 2006, 2008 Junio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>hooks</td>\n",
              "      <td>post-update.sample</td>\n",
              "      <td>#!/bin/sh\\n#\\n# An example hook script to prep...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>hooks</td>\n",
              "      <td>pre-receive.sample</td>\n",
              "      <td>#!/bin/sh\\n#\\n# An example hook script to make...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>hooks</td>\n",
              "      <td>applypatch-msg.sample</td>\n",
              "      <td>#!/bin/sh\\n#\\n# An example hook script to chec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>hooks</td>\n",
              "      <td>push-to-checkout.sample</td>\n",
              "      <td>#!/bin/sh\\n\\n# An example hook script to updat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>hooks</td>\n",
              "      <td>update.sample</td>\n",
              "      <td>#!/bin/sh\\n#\\n# An example hook script to bloc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>hooks</td>\n",
              "      <td>pre-applypatch.sample</td>\n",
              "      <td>#!/bin/sh\\n#\\n# An example hook script to veri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>hooks</td>\n",
              "      <td>pre-merge-commit.sample</td>\n",
              "      <td>#!/bin/sh\\n#\\n# An example hook script to veri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>hooks</td>\n",
              "      <td>commit-msg.sample</td>\n",
              "      <td>#!/bin/sh\\n#\\n# An example hook script to chec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>hooks</td>\n",
              "      <td>pre-commit.sample</td>\n",
              "      <td>#!/bin/sh\\n#\\n# An example hook script to veri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>hooks</td>\n",
              "      <td>prepare-commit-msg.sample</td>\n",
              "      <td>#!/bin/sh\\n#\\n# An example hook script to prep...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>hooks</td>\n",
              "      <td>pre-push.sample</td>\n",
              "      <td>#!/bin/sh\\n\\n# An example hook script to verif...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>info</td>\n",
              "      <td>exclude</td>\n",
              "      <td># git ls-files --others --exclude-from=.git/in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>utils</td>\n",
              "      <td>jax-only-sampled-distribution.py</td>\n",
              "      <td>from os import listdir, path, rename\\n\\nimport...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>utils</td>\n",
              "      <td>trainer.py</td>\n",
              "      <td>#\\n#\\n#      0================================...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>utils</td>\n",
              "      <td>tester.py</td>\n",
              "      <td>#\\n#\\n#      0================================...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>utils</td>\n",
              "      <td>config.py</td>\n",
              "      <td>#\\n#\\n#      0================================...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>utils</td>\n",
              "      <td>jax-only-distribution.py</td>\n",
              "      <td>from os import listdir, path, rename\\n\\nimport...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>utils</td>\n",
              "      <td>metrics.py</td>\n",
              "      <td>#\\n#\\n#      0================================...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>utils</td>\n",
              "      <td>las.py</td>\n",
              "      <td>import json\\nimport numpy as np\\nimport pdal\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>cpp_wrappers</td>\n",
              "      <td>compile_wrappers.sh</td>\n",
              "      <td>#!/bin/bash\\n\\n# Compile cpp subsampling\\ncd c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>cpp_neighbors</td>\n",
              "      <td>build.bat</td>\n",
              "      <td>@echo off\\npy setup.py build_ext --inplace\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>cpp_neighbors</td>\n",
              "      <td>setup.py</td>\n",
              "      <td>from distutils.core import setup, Extension\\ni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>cpp_neighbors</td>\n",
              "      <td>wrapper.cpp</td>\n",
              "      <td>#include &lt;Python.h&gt;\\n#include &lt;numpy/arrayobje...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>neighbors</td>\n",
              "      <td>neighbors.cpp</td>\n",
              "      <td>\\n#include \"neighbors.h\"\\n\\n\\nvoid brute_neigh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>neighbors</td>\n",
              "      <td>neighbors.h</td>\n",
              "      <td>\\n\\n#include \"../../cpp_utils/cloud/cloud.h\"\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>cpp_subsampling</td>\n",
              "      <td>build.bat</td>\n",
              "      <td>@echo off\\npy setup.py build_ext --inplace\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>cpp_subsampling</td>\n",
              "      <td>setup.py</td>\n",
              "      <td>from distutils.core import setup, Extension\\ni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>cpp_subsampling</td>\n",
              "      <td>wrapper.cpp</td>\n",
              "      <td>#include &lt;Python.h&gt;\\n#include &lt;numpy/arrayobje...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>grid_subsampling</td>\n",
              "      <td>grid_subsampling.h</td>\n",
              "      <td>\\n\\n#include \"../../cpp_utils/cloud/cloud.h\"\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>grid_subsampling</td>\n",
              "      <td>grid_subsampling.cpp</td>\n",
              "      <td>\\n#include \"grid_subsampling.h\"\\n\\n\\nvoid grid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>cloud</td>\n",
              "      <td>cloud.cpp</td>\n",
              "      <td>//\\n//\\n//\\t\\t0==========================0\\n//...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>cloud</td>\n",
              "      <td>cloud.h</td>\n",
              "      <td>//\\n//\\n//\\t\\t0==========================0\\n//...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>nanoflann</td>\n",
              "      <td>nanoflann.hpp</td>\n",
              "      <td>/*********************************************...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>models</td>\n",
              "      <td>architectures.py</td>\n",
              "      <td>#\\n#\\n#      0================================...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>models</td>\n",
              "      <td>blocks.py</td>\n",
              "      <td>#\\n#\\n#      0================================...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>kernels</td>\n",
              "      <td>kernel_points.py</td>\n",
              "      <td>#\\n#\\n#      0================================...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>datasets</td>\n",
              "      <td>common.py</td>\n",
              "      <td>#\\n#\\n#      0================================...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>datasets</td>\n",
              "      <td>LAS.py</td>\n",
              "      <td>#\\n#\\n#      0================================...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90e6dea1-7391-4880-978e-762e388f7fe7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-90e6dea1-7391-4880-978e-762e388f7fe7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-90e6dea1-7391-4880-978e-762e388f7fe7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e9904b9f-8a14-402b-aef8-788c37b46ff4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e9904b9f-8a14-402b-aef8-788c37b46ff4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e9904b9f-8a14-402b-aef8-788c37b46ff4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b7fbe188-c4e0-43e3-856b-17eef9313265\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('repo_contents')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b7fbe188-c4e0-43e3-856b-17eef9313265 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('repo_contents');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_contents_str = \"\"\n",
        "for foler_name in list(repo_contents['folder'].unique()):\n",
        "  repo_contents_str+=\"Folder: \"+foler_name+\"\\n\"\n",
        "  for file_name in list(repo_contents[repo_contents['folder'] == foler_name]['file_name'].unique()):\n",
        "      repo_contents_str=repo_contents_str+\"File Name: \"+file_name+\"\\n\"+\"Contents of File\"+file_name+\": \"+repo_contents[repo_contents['file_name'] == file_name]['content'].iloc[0]\n",
        "      repo_contents_str+=\"\\n\\n\"\n",
        "  repo_contents_str+=\"***\"\n",
        "repo_contents_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KHKN6M5qNL-y",
        "outputId": "dcaea471-2869-47a3-b7f0-486dba8ffa72"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Folder: kpconv-pdal-4\\nFile Name: train_LAS.py\\nContents of Filetrain_LAS.py: #\\n#\\n#      0=================================0\\n#      |    Kernel Point Convolutions    |\\n#      0=================================0\\n#\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Callable script to start a training on an LAS dataset\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Derived from train_S3DIS.py by Hugues THOMAS - 06/03/2020\\n#      Brad CHAMBERS - 05/13/2021\\n#\\n\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Imports and global variables\\n#       \\\\**********************************/\\n#\\n\\n# Common libs\\nimport signal\\nimport os\\nimport sys\\n\\n# Dataset\\nfrom datasets.LAS import *\\nfrom torch.utils.data import DataLoader\\n\\nfrom utils.config import Config\\nfrom utils.trainer import ModelTrainer\\nfrom models.architectures import KPFCNN\\n\\nfrom torch.utils.tensorboard import SummaryWriter\\n\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Config Class\\n#       \\\\******************/\\n#\\n\\nclass LASConfig(Config):\\n    \"\"\"\\n    Override the parameters you want to modify for this dataset\\n    \"\"\"\\n\\n    ####################\\n    # Dataset parameters\\n    ####################\\n\\n    # Dataset name\\n    dataset = \\'LAS\\'\\n\\n    # Number of classes in the dataset (This value is overwritten by dataset class when Initializating dataset).\\n    num_classes = None\\n\\n    # Type of task performed on this dataset (also overwritten)\\n    dataset_task = \\'\\'\\n\\n    # Number of CPU threads for the input pipeline\\n    input_threads = 10\\n\\n    #########################\\n    # Architecture definition\\n    #########################\\n\\n    # Define layers\\n    architecture = [\\'simple\\',\\n                    \\'resnetb\\',\\n                    \\'resnetb_strided\\',\\n                    \\'resnetb\\',\\n                    \\'resnetb\\',\\n                    \\'resnetb_strided\\',\\n                    \\'resnetb_deformable\\',\\n                    \\'resnetb_deformable\\',\\n                    \\'resnetb_deformable_strided\\',\\n                    \\'resnetb_deformable\\',\\n                    \\'resnetb_deformable\\',\\n                    \\'resnetb_deformable_strided\\',\\n                    \\'resnetb_deformable\\',\\n                    \\'resnetb_deformable\\',\\n                    \\'nearest_upsample\\',\\n                    \\'unary\\',\\n                    \\'nearest_upsample\\',\\n                    \\'unary\\',\\n                    \\'nearest_upsample\\',\\n                    \\'unary\\',\\n                    \\'nearest_upsample\\',\\n                    \\'unary\\']\\n\\n    ###################\\n    # KPConv parameters\\n    ###################\\n\\n    # Radius of the input sphere\\n    in_radius = 50.0\\n\\n    # Number of kernel points\\n    num_kernel_points = 15\\n\\n    # Size of the first subsampling grid in meter\\n    first_subsampling_dl = 2.0\\n\\n    # Radius of convolution in \"number grid cell\". (2.5 is the standard value)\\n    conv_radius = 2.5\\n\\n    # Radius of deformable convolution in \"number grid cell\". Larger so that deformed kernel can spread out\\n    deform_radius = 6.0\\n\\n    # Radius of the area of influence of each kernel point in \"number grid cell\". (1.0 is the standard value)\\n    KP_extent = 1.2\\n\\n    # Behavior of convolutions in (\\'constant\\', \\'linear\\', \\'gaussian\\')\\n    KP_influence = \\'linear\\'\\n\\n    # Aggregation function of KPConv in (\\'closest\\', \\'sum\\')\\n    aggregation_mode = \\'sum\\'\\n\\n    # Choice of input features\\n    first_features_dim = 128\\n    in_features_dim = 5\\n\\n    # Can the network learn modulations\\n    modulated = False\\n\\n    # Batch normalization parameters\\n    use_batch_norm = True\\n    batch_norm_momentum = 0.02\\n\\n    # Deformable offset loss\\n    # \\'point2point\\' fitting geometry by penalizing distance from deform point to input points\\n    # \\'point2plane\\' fitting geometry by penalizing distance from deform point to input point triplet (not implemented)\\n    deform_fitting_mode = \\'point2point\\'\\n    deform_fitting_power = 1.0              # Multiplier for the fitting/repulsive loss\\n    deform_lr_factor = 0.1                  # Multiplier for learning rate applied to the deformations\\n    repulse_extent = 1.2                    # Distance of repulsion for deformed kernel points\\n\\n    #####################\\n    # Training parameters\\n    #####################\\n\\n    # Maximal number of epochs\\n    max_epoch = 500\\n\\n    # Learning rate management\\n    learning_rate = 1e-2\\n    momentum = 0.98\\n    lr_decays = {i: 0.1 ** (1 / 150) for i in range(1, max_epoch)}\\n    grad_clip_norm = 100.0\\n\\n    # Number of batch\\n    batch_num = 8\\n\\n    # Number of steps per epochs\\n    epoch_steps = 500\\n\\n    # Number of validation examples per epoch\\n    validation_size = 50\\n\\n    # Number of epoch between each checkpoint\\n    checkpoint_gap = 50\\n\\n    # Augmentations\\n    augment_scale_anisotropic = True\\n    augment_symmetries = [True, False, False]\\n    augment_rotation = \\'vertical\\'\\n    augment_scale_min = 0.9\\n    augment_scale_max = 1.1\\n    augment_noise = 0.05\\n    augment_color = 0.8\\n\\n    # The way we balance segmentation loss\\n    #   > \\'none\\': Each point in the whole batch has the same contribution.\\n    #   > \\'class\\': Each class has the same contribution (points are weighted according to class balance)\\n    #   > \\'batch\\': Each cloud in the batch has the same contribution (points are weighted according cloud sizes)\\n    segloss_balance = \\'none\\'\\n\\n    # Do we nee to save convergence\\n    saving = True\\n    saving_path = None\\n\\n    # Dataset folder\\n    path = \\'/home/chambbj/data/ml-datasets/US3D/oma-only\\'\\n    writer = SummaryWriter(\\'/home/chambbj/data/tensorboard-runs/new/demo\\')\\n\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Main Call\\n#       \\\\***************/\\n#\\n\\nif __name__ == \\'__main__\\':\\n\\n    ############################\\n    # Initialize the environment\\n    ############################\\n\\n    # Set which gpu is going to be used\\n    GPU_ID = \\'0\\'\\n\\n    # Set GPU visible device\\n    os.environ[\\'CUDA_VISIBLE_DEVICES\\'] = GPU_ID\\n\\n    ###############\\n    # Previous chkp\\n    ###############\\n\\n    # Choose here if you want to start training from a previous snapshot (None for new training)\\n    # previous_training_path = \\'Log_2020-03-19_19-53-27\\'\\n    previous_training_path = \\'\\'\\n\\n    # Choose index of checkpoint to start from. If None, uses the latest chkp\\n    chkp_idx = None\\n    if previous_training_path:\\n\\n        # Find all snapshot in the chosen training folder\\n        chkp_path = os.path.join(\\'results\\', previous_training_path, \\'checkpoints\\')\\n        chkps = [f for f in os.listdir(chkp_path) if f[:4] == \\'chkp\\']\\n\\n        # Find which snapshot to restore\\n        if chkp_idx is None:\\n            chosen_chkp = \\'best_miou_chkp.tar\\'\\n        else:\\n            chosen_chkp = np.sort(chkps)[chkp_idx]\\n        chosen_chkp = os.path.join(\\'results\\', previous_training_path, \\'checkpoints\\', chosen_chkp)\\n\\n    else:\\n        chosen_chkp = None\\n\\n    ##############\\n    # Prepare Data\\n    ##############\\n\\n    print()\\n    print(\\'Data Preparation\\')\\n    print(\\'****************\\')\\n\\n    # Initialize configuration class\\n    config = LASConfig()\\n    if previous_training_path:\\n        config.load(os.path.join(\\'results\\', previous_training_path))\\n        config.saving_path = None\\n\\n    # Get path from argument if given\\n    if len(sys.argv) > 1:\\n        config.saving_path = sys.argv[1]\\n\\n    # Initialize datasets\\n    training_dataset = LASDataset(config, set=\\'training\\', use_potentials=True)\\n    test_dataset = LASDataset(config, set=\\'validation\\', use_potentials=True)\\n\\n    # Initialize samplers\\n    training_sampler = LASSampler(training_dataset)\\n    test_sampler = LASSampler(test_dataset)\\n\\n    # Initialize the dataloader\\n    training_loader = DataLoader(training_dataset,\\n                                 batch_size=1,\\n                                 sampler=training_sampler,\\n                                 collate_fn=LASCollate,\\n                                 num_workers=config.input_threads,\\n                                 pin_memory=True)\\n    test_loader = DataLoader(test_dataset,\\n                             batch_size=1,\\n                             sampler=test_sampler,\\n                             collate_fn=LASCollate,\\n                             num_workers=config.input_threads,\\n                             pin_memory=True)\\n\\n    # Calibrate samplers\\n    training_sampler.calibration(training_loader, verbose=True)\\n    test_sampler.calibration(test_loader, verbose=True)\\n\\n    # Optional debug functions\\n    # debug_timing(training_dataset, training_loader)\\n    # debug_timing(test_dataset, test_loader)\\n    # debug_upsampling(training_dataset, training_loader)\\n\\n    print(\\'\\\\nModel Preparation\\')\\n    print(\\'*****************\\')\\n\\n    # Define network model\\n    t1 = time.time()\\n    label_value_ids = np.array([training_dataset.label_to_idx[l] for l in training_dataset.label_values])\\n    net = KPFCNN(config, label_value_ids, training_dataset.ignored_labels)\\n\\n    debug = False\\n    if debug:\\n        print(\\'\\\\n*************************************\\\\n\\')\\n        print(net)\\n        print(\\'\\\\n*************************************\\\\n\\')\\n        for param in net.parameters():\\n            if param.requires_grad:\\n                print(param.shape)\\n        print(\\'\\\\n*************************************\\\\n\\')\\n        print(\"Model size %i\" % sum(param.numel() for param in net.parameters() if param.requires_grad))\\n        print(\\'\\\\n*************************************\\\\n\\')\\n\\n    # Define a trainer class\\n    trainer = ModelTrainer(net, config, chkp_path=chosen_chkp)\\n    print(\\'Done in {:.1f}s\\\\n\\'.format(time.time() - t1))\\n\\n    print(\\'\\\\nStart training\\')\\n    print(\\'**************\\')\\n\\n    # Training\\n    trainer.train(net, training_loader, test_loader, config)\\n\\n    print(\\'Forcing exit now\\')\\n    os.kill(os.getpid(), signal.SIGINT)\\n\\n\\nFile Name: test_LAS.py\\nContents of Filetest_LAS.py: #\\n#\\n#      0=================================0\\n#      |    Kernel Point Convolutions    |\\n#      0=================================0\\n#\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Callable script to start a training on LAS dataset\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Derived from train_S3DIS.py by Hugues THOMAS - 06/03/2020\\n#      Brad CHAMBERS - 05/13/2021\\n#\\n\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Imports and global variables\\n#       \\\\**********************************/\\n#\\n\\n# Common libs\\nimport signal\\nimport os\\nimport numpy as np\\nimport sys\\nimport torch\\n\\n# Dataset\\nfrom datasets.LAS import *\\nfrom torch.utils.data import DataLoader\\n\\nfrom utils.config import Config\\nfrom utils.tester import ModelTester\\nfrom models.architectures import KPCNN, KPFCNN\\n\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Main Call\\n#       \\\\***************/\\n#\\n\\ndef model_choice(chosen_log):\\n\\n    ###########################\\n    # Call the test initializer\\n    ###########################\\n\\n    # Automatically retrieve the last trained model\\n    if chosen_log in [\\'last_ModelNet40\\', \\'last_ShapeNetPart\\', \\'last_S3DIS\\']:\\n\\n        # Dataset name\\n        test_dataset = \\'_\\'.join(chosen_log.split(\\'_\\')[1:])\\n\\n        # List all training logs\\n        logs = np.sort([os.path.join(\\'results\\', f) for f in os.listdir(\\'results\\') if f.startswith(\\'Log\\')])\\n\\n        # Find the last log of asked dataset\\n        for log in logs[::-1]:\\n            log_config = Config()\\n            log_config.load(log)\\n            if log_config.dataset.startswith(test_dataset):\\n                chosen_log = log\\n                break\\n\\n        if chosen_log in [\\'last_ModelNet40\\', \\'last_ShapeNetPart\\', \\'last_S3DIS\\']:\\n            raise ValueError(\\'No log of the dataset \"\\' + test_dataset + \\'\" found\\')\\n\\n    # Check if log exists\\n    if not os.path.exists(chosen_log):\\n        raise ValueError(\\'The given log does not exists: \\' + chosen_log)\\n\\n    return chosen_log\\n\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Main Call\\n#       \\\\***************/\\n#\\n\\nif __name__ == \\'__main__\\':\\n\\n    ###############################\\n    # Choose the model to visualize\\n    ###############################\\n\\n    #   Here you can choose which model you want to test with the variable test_model. Here are the possible values :\\n    #\\n    #       > \\'last_XXX\\': Automatically retrieve the last trained model on dataset XXX\\n    #       > \\'(old_)results/Log_YYYY-MM-DD_HH-MM-SS\\': Directly provide the path of a trained model\\n\\n    chosen_log = \\'results/Log_2021-05-21_15-15-09\\'  # => ModelNet40\\n\\n    # Choose the index of the checkpoint to load OR None if you want to load the best checkpoint\\n    chkp_idx = None\\n\\n    # Choose to test on validation or test split\\n    on_val = False\\n\\n    # Deal with \\'last_XXXXXX\\' choices\\n    chosen_log = model_choice(chosen_log)\\n\\n    ############################\\n    # Initialize the environment\\n    ############################\\n\\n    # Set which gpu is going to be used\\n    GPU_ID = \\'0\\'\\n\\n    # Set GPU visible device\\n    os.environ[\\'CUDA_VISIBLE_DEVICES\\'] = GPU_ID\\n\\n    ###############\\n    # Previous chkp\\n    ###############\\n\\n    # Find all checkpoints in the chosen training folder\\n    chkp_path = os.path.join(chosen_log, \\'checkpoints\\')\\n    chkps = [f for f in os.listdir(chkp_path) if f[:4] == \\'chkp\\']\\n\\n    # Find which snapshot to restore\\n    if chkp_idx is None:\\n        chosen_chkp = \\'best_miou_chkp.tar\\'\\n    else:\\n        chosen_chkp = np.sort(chkps)[chkp_idx]\\n    chosen_chkp = os.path.join(chosen_log, \\'checkpoints\\', chosen_chkp)\\n\\n    # Initialize configuration class\\n    config = Config()\\n    config.load(chosen_log)\\n\\n    ##################################\\n    # Change model parameters for test\\n    ##################################\\n\\n    # Change parameters for the test here. For example, you can stop augmenting the input data.\\n\\n    #config.augment_noise = 0.0001\\n    #config.augment_symmetries = False\\n    #config.batch_num = 3\\n    #config.in_radius = 4\\n    config.validation_size = 200\\n    config.input_threads = 10\\n\\n    ##############\\n    # Prepare Data\\n    ##############\\n\\n    print()\\n    print(\\'Data Preparation\\')\\n    print(\\'****************\\')\\n\\n    if on_val:\\n        set = \\'validation\\'\\n    else:\\n        set = \\'test\\'\\n\\n    # Initiate dataset\\n    if config.dataset == \\'ModelNet40\\':\\n        test_dataset = ModelNet40Dataset(config, train=False)\\n        test_sampler = ModelNet40Sampler(test_dataset)\\n        collate_fn = ModelNet40Collate\\n    elif config.dataset == \\'S3DIS\\':\\n        test_dataset = S3DISDataset(config, set=\\'validation\\', use_potentials=True)\\n        test_sampler = S3DISSampler(test_dataset)\\n        collate_fn = S3DISCollate\\n    elif config.dataset == \\'SemanticKitti\\':\\n        test_dataset = SemanticKittiDataset(config, set=set, balance_classes=False)\\n        test_sampler = SemanticKittiSampler(test_dataset)\\n        collate_fn = SemanticKittiCollate\\n    elif config.dataset == \\'LAS\\':\\n        test_dataset = LASDataset(config, set=\\'test\\', use_potentials=True)\\n        test_sampler = LASSampler(test_dataset)\\n        collate_fn = LASCollate\\n    else:\\n        raise ValueError(\\'Unsupported dataset : \\' + config.dataset)\\n\\n    # Data loader\\n    test_loader = DataLoader(test_dataset,\\n                             batch_size=1,\\n                             sampler=test_sampler,\\n                             collate_fn=collate_fn,\\n                             num_workers=config.input_threads,\\n                             pin_memory=True)\\n\\n    # Calibrate samplers\\n    test_sampler.calibration(test_loader, verbose=True)\\n\\n    print(\\'\\\\nModel Preparation\\')\\n    print(\\'*****************\\')\\n\\n    # Define network model\\n    t1 = time.time()\\n    if config.dataset_task == \\'classification\\':\\n        net = KPCNN(config)\\n    elif config.dataset_task in [\\'cloud_segmentation\\', \\'slam_segmentation\\']:\\n        net = KPFCNN(config, test_dataset.label_values, test_dataset.ignored_labels)\\n    else:\\n        raise ValueError(\\'Unsupported dataset_task for testing: \\' + config.dataset_task)\\n\\n    # Define a visualizer class\\n    tester = ModelTester(net, chkp_path=chosen_chkp, on_gpu=True)\\n    print(\\'Done in {:.1f}s\\\\n\\'.format(time.time() - t1))\\n\\n    print(\\'\\\\nStart test\\')\\n    print(\\'**********\\\\n\\')\\n\\n    # Training\\n    if config.dataset_task == \\'classification\\':\\n        tester.classification_test(net, test_loader, config)\\n    elif config.dataset_task == \\'cloud_segmentation\\':\\n        tester.cloud_segmentation_test(net, test_loader, config, num_votes=10)\\n    elif config.dataset_task == \\'slam_segmentation\\':\\n        tester.slam_segmentation_test(net, test_loader, config)\\n    else:\\n        raise ValueError(\\'Unsupported dataset_task for testing: \\' + config.dataset_task)\\n\\n\\nFile Name: README.md\\nContents of FileREADME.md: Derivative work of https://github.com/HuguesTHOMAS/KPConv-PyTorch, aimed first at evaluating US3D dataset, and now expanding to adapt to general purpose LAS/LAZ datasets.\\n\\nWe\\'ll add the requisite LICENSE file in subsequent commits, but as with the source repository (https://github.com/HuguesTHOMAS/KPConv-PyTorch/blame/master/README.md#L55) this work will be released under the MIT license.\\n\\nTo create a Conda environment to run `kpconv-pdal`, run the following command (assuming you have Conda installed of course) from within the root directory of the repository.\\n\\n```bash\\nconda env create -f environment.yml\\nconda activate kpconv-pdal-env\\n```\\n\\nWe currently still use some of the C++ extensions provided in the original KPConv-Pytorch release. To compile these, from within the `cpp_wrappers` directory, issue the following command.\\n\\n```bash\\nsh compile_wrappers.sh\\n```\\n\\nAt this point, you should be able to train models and predict classification labels. After modifying the `LASConfig` class within `train_LAS.py`, namely to adjust paths to the dataset and the tensorboard run directory, simply execute the following.\\n\\n```bash\\npython train_LAS.py\\n```\\n\\nThe current means of predicting labels is to modify `test_LAS.py` to point to the results log directory, and to uncomment line 24 of `datasets/LAS.py`. You can then run the following.\\n\\n```bash\\npython test_LAS.py\\n```\\n\\n\\nFile Name: .gitignore\\nContents of File.gitignore: __pycache__\\n*.o\\n*.so\\n.vscode\\nresults/\\nruns/\\ntest/\\nkernels/dispositions/\\n\\n\\nFile Name: environment.yml\\nContents of Fileenvironment.yml: name: kpconv-pdal-env\\nchannels:\\n  - conda-forge\\n  - pytorch\\n  - defaults\\ndependencies:\\n  - cudatoolkit=10.1\\n  - torchvision==0.5.0\\n  - pytorch==1.4.0\\n  - pyyaml\\n  - numpy\\n  - mayavi\\n  - scikit-learn\\n  - pyqt=5\\n  - cudnn=7.6\\n  - matplotlib\\n  - python-pdal\\n  - tensorboard\\n\\n\\n***Folder: .git\\nFile Name: HEAD\\nContents of FileHEAD: ref: refs/heads/main\\n\\n\\nFile Name: description\\nContents of Filedescription: Unnamed repository; edit this file \\'description\\' to name the repository.\\n\\n\\nFile Name: packed-refs\\nContents of Filepacked-refs: # pack-refs with: peeled fully-peeled sorted \\nf681db9694c99dc48801e6324a9b81218d925bba refs/remotes/origin/for-hobu\\n375aadf9fa84cbb63ce866933398f022ae8e2952 refs/remotes/origin/main\\neb690a67d253f08786d60c56241dd1f0e184c90f refs/remotes/origin/wrap-hydra\\n\\n\\nFile Name: index\\nContents of Fileindex: Error reading file: \\'utf-8\\' codec can\\'t decode byte 0x9f in position 13: invalid start byte\\n\\nFile Name: config\\nContents of Fileconfig: [core]\\n\\trepositoryformatversion = 0\\n\\tfilemode = true\\n\\tbare = false\\n\\tlogallrefupdates = true\\n[remote \"origin\"]\\n\\turl = https://github.com/chambbj/kpconv-pdal.git\\n\\tfetch = +refs/heads/*:refs/remotes/origin/*\\n[branch \"main\"]\\n\\tremote = origin\\n\\tmerge = refs/heads/main\\n\\n\\n***Folder: origin\\nFile Name: HEAD\\nContents of FileHEAD: ref: refs/heads/main\\n\\n\\n***Folder: heads\\nFile Name: main\\nContents of Filemain: 375aadf9fa84cbb63ce866933398f022ae8e2952\\n\\n\\n***Folder: logs\\nFile Name: HEAD\\nContents of FileHEAD: ref: refs/heads/main\\n\\n\\n***Folder: pack\\nFile Name: pack-6ec1a2b6ae05c77a50473fa5ffcc00f3b85b2946.pack\\nContents of Filepack-6ec1a2b6ae05c77a50473fa5ffcc00f3b85b2946.pack: Error reading file: \\'utf-8\\' codec can\\'t decode byte 0x93 in position 12: invalid start byte\\n\\nFile Name: pack-6ec1a2b6ae05c77a50473fa5ffcc00f3b85b2946.idx\\nContents of Filepack-6ec1a2b6ae05c77a50473fa5ffcc00f3b85b2946.idx: Error reading file: \\'utf-8\\' codec can\\'t decode byte 0xff in position 0: invalid start byte\\n\\n***Folder: hooks\\nFile Name: fsmonitor-watchman.sample\\nContents of Filefsmonitor-watchman.sample: #!/usr/bin/perl\\n\\nuse strict;\\nuse warnings;\\nuse IPC::Open2;\\n\\n# An example hook script to integrate Watchman\\n# (https://facebook.github.io/watchman/) with git to speed up detecting\\n# new and modified files.\\n#\\n# The hook is passed a version (currently 2) and last update token\\n# formatted as a string and outputs to stdout a new update token and\\n# all files that have been modified since the update token. Paths must\\n# be relative to the root of the working tree and separated by a single NUL.\\n#\\n# To enable this hook, rename this file to \"query-watchman\" and set\\n# \\'git config core.fsmonitor .git/hooks/query-watchman\\'\\n#\\nmy ($version, $last_update_token) = @ARGV;\\n\\n# Uncomment for debugging\\n# print STDERR \"$0 $version $last_update_token\\\\n\";\\n\\n# Check the hook interface version\\nif ($version ne 2) {\\n\\tdie \"Unsupported query-fsmonitor hook version \\'$version\\'.\\\\n\" .\\n\\t    \"Falling back to scanning...\\\\n\";\\n}\\n\\nmy $git_work_tree = get_working_dir();\\n\\nmy $retry = 1;\\n\\nmy $json_pkg;\\neval {\\n\\trequire JSON::XS;\\n\\t$json_pkg = \"JSON::XS\";\\n\\t1;\\n} or do {\\n\\trequire JSON::PP;\\n\\t$json_pkg = \"JSON::PP\";\\n};\\n\\nlaunch_watchman();\\n\\nsub launch_watchman {\\n\\tmy $o = watchman_query();\\n\\tif (is_work_tree_watched($o)) {\\n\\t\\toutput_result($o->{clock}, @{$o->{files}});\\n\\t}\\n}\\n\\nsub output_result {\\n\\tmy ($clockid, @files) = @_;\\n\\n\\t# Uncomment for debugging watchman output\\n\\t# open (my $fh, \">\", \".git/watchman-output.out\");\\n\\t# binmode $fh, \":utf8\";\\n\\t# print $fh \"$clockid\\\\n@files\\\\n\";\\n\\t# close $fh;\\n\\n\\tbinmode STDOUT, \":utf8\";\\n\\tprint $clockid;\\n\\tprint \"\\\\0\";\\n\\tlocal $, = \"\\\\0\";\\n\\tprint @files;\\n}\\n\\nsub watchman_clock {\\n\\tmy $response = qx/watchman clock \"$git_work_tree\"/;\\n\\tdie \"Failed to get clock id on \\'$git_work_tree\\'.\\\\n\" .\\n\\t\\t\"Falling back to scanning...\\\\n\" if $? != 0;\\n\\n\\treturn $json_pkg->new->utf8->decode($response);\\n}\\n\\nsub watchman_query {\\n\\tmy $pid = open2(\\\\*CHLD_OUT, \\\\*CHLD_IN, \\'watchman -j --no-pretty\\')\\n\\tor die \"open2() failed: $!\\\\n\" .\\n\\t\"Falling back to scanning...\\\\n\";\\n\\n\\t# In the query expression below we\\'re asking for names of files that\\n\\t# changed since $last_update_token but not from the .git folder.\\n\\t#\\n\\t# To accomplish this, we\\'re using the \"since\" generator to use the\\n\\t# recency index to select candidate nodes and \"fields\" to limit the\\n\\t# output to file names only. Then we\\'re using the \"expression\" term to\\n\\t# further constrain the results.\\n\\tif (substr($last_update_token, 0, 1) eq \"c\") {\\n\\t\\t$last_update_token = \"\\\\\"$last_update_token\\\\\"\";\\n\\t}\\n\\tmy $query = <<\"\\tEND\";\\n\\t\\t[\"query\", \"$git_work_tree\", {\\n\\t\\t\\t\"since\": $last_update_token,\\n\\t\\t\\t\"fields\": [\"name\"],\\n\\t\\t\\t\"expression\": [\"not\", [\"dirname\", \".git\"]]\\n\\t\\t}]\\n\\tEND\\n\\n\\t# Uncomment for debugging the watchman query\\n\\t# open (my $fh, \">\", \".git/watchman-query.json\");\\n\\t# print $fh $query;\\n\\t# close $fh;\\n\\n\\tprint CHLD_IN $query;\\n\\tclose CHLD_IN;\\n\\tmy $response = do {local $/; <CHLD_OUT>};\\n\\n\\t# Uncomment for debugging the watch response\\n\\t# open ($fh, \">\", \".git/watchman-response.json\");\\n\\t# print $fh $response;\\n\\t# close $fh;\\n\\n\\tdie \"Watchman: command returned no output.\\\\n\" .\\n\\t\"Falling back to scanning...\\\\n\" if $response eq \"\";\\n\\tdie \"Watchman: command returned invalid output: $response\\\\n\" .\\n\\t\"Falling back to scanning...\\\\n\" unless $response =~ /^\\\\{/;\\n\\n\\treturn $json_pkg->new->utf8->decode($response);\\n}\\n\\nsub is_work_tree_watched {\\n\\tmy ($output) = @_;\\n\\tmy $error = $output->{error};\\n\\tif ($retry > 0 and $error and $error =~ m/unable to resolve root .* directory (.*) is not watched/) {\\n\\t\\t$retry--;\\n\\t\\tmy $response = qx/watchman watch \"$git_work_tree\"/;\\n\\t\\tdie \"Failed to make watchman watch \\'$git_work_tree\\'.\\\\n\" .\\n\\t\\t    \"Falling back to scanning...\\\\n\" if $? != 0;\\n\\t\\t$output = $json_pkg->new->utf8->decode($response);\\n\\t\\t$error = $output->{error};\\n\\t\\tdie \"Watchman: $error.\\\\n\" .\\n\\t\\t\"Falling back to scanning...\\\\n\" if $error;\\n\\n\\t\\t# Uncomment for debugging watchman output\\n\\t\\t# open (my $fh, \">\", \".git/watchman-output.out\");\\n\\t\\t# close $fh;\\n\\n\\t\\t# Watchman will always return all files on the first query so\\n\\t\\t# return the fast \"everything is dirty\" flag to git and do the\\n\\t\\t# Watchman query just to get it over with now so we won\\'t pay\\n\\t\\t# the cost in git to look up each individual file.\\n\\t\\tmy $o = watchman_clock();\\n\\t\\t$error = $output->{error};\\n\\n\\t\\tdie \"Watchman: $error.\\\\n\" .\\n\\t\\t\"Falling back to scanning...\\\\n\" if $error;\\n\\n\\t\\toutput_result($o->{clock}, (\"/\"));\\n\\t\\t$last_update_token = $o->{clock};\\n\\n\\t\\teval { launch_watchman() };\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tdie \"Watchman: $error.\\\\n\" .\\n\\t\"Falling back to scanning...\\\\n\" if $error;\\n\\n\\treturn 1;\\n}\\n\\nsub get_working_dir {\\n\\tmy $working_dir;\\n\\tif ($^O =~ \\'msys\\' || $^O =~ \\'cygwin\\') {\\n\\t\\t$working_dir = Win32::GetCwd();\\n\\t\\t$working_dir =~ tr/\\\\\\\\/\\\\//;\\n\\t} else {\\n\\t\\trequire Cwd;\\n\\t\\t$working_dir = Cwd::cwd();\\n\\t}\\n\\n\\treturn $working_dir;\\n}\\n\\n\\nFile Name: pre-rebase.sample\\nContents of Filepre-rebase.sample: #!/bin/sh\\n#\\n# Copyright (c) 2006, 2008 Junio C Hamano\\n#\\n# The \"pre-rebase\" hook is run just before \"git rebase\" starts doing\\n# its job, and can prevent the command from running by exiting with\\n# non-zero status.\\n#\\n# The hook is called with the following parameters:\\n#\\n# $1 -- the upstream the series was forked from.\\n# $2 -- the branch being rebased (or empty when rebasing the current branch).\\n#\\n# This sample shows how to prevent topic branches that are already\\n# merged to \\'next\\' branch from getting rebased, because allowing it\\n# would result in rebasing already published history.\\n\\npublish=next\\nbasebranch=\"$1\"\\nif test \"$#\" = 2\\nthen\\n\\ttopic=\"refs/heads/$2\"\\nelse\\n\\ttopic=`git symbolic-ref HEAD` ||\\n\\texit 0 ;# we do not interrupt rebasing detached HEAD\\nfi\\n\\ncase \"$topic\" in\\nrefs/heads/??/*)\\n\\t;;\\n*)\\n\\texit 0 ;# we do not interrupt others.\\n\\t;;\\nesac\\n\\n# Now we are dealing with a topic branch being rebased\\n# on top of master.  Is it OK to rebase it?\\n\\n# Does the topic really exist?\\ngit show-ref -q \"$topic\" || {\\n\\techo >&2 \"No such branch $topic\"\\n\\texit 1\\n}\\n\\n# Is topic fully merged to master?\\nnot_in_master=`git rev-list --pretty=oneline ^master \"$topic\"`\\nif test -z \"$not_in_master\"\\nthen\\n\\techo >&2 \"$topic is fully merged to master; better remove it.\"\\n\\texit 1 ;# we could allow it, but there is no point.\\nfi\\n\\n# Is topic ever merged to next?  If so you should not be rebasing it.\\nonly_next_1=`git rev-list ^master \"^$topic\" ${publish} | sort`\\nonly_next_2=`git rev-list ^master           ${publish} | sort`\\nif test \"$only_next_1\" = \"$only_next_2\"\\nthen\\n\\tnot_in_topic=`git rev-list \"^$topic\" master`\\n\\tif test -z \"$not_in_topic\"\\n\\tthen\\n\\t\\techo >&2 \"$topic is already up to date with master\"\\n\\t\\texit 1 ;# we could allow it, but there is no point.\\n\\telse\\n\\t\\texit 0\\n\\tfi\\nelse\\n\\tnot_in_next=`git rev-list --pretty=oneline ^${publish} \"$topic\"`\\n\\t/usr/bin/perl -e \\'\\n\\t\\tmy $topic = $ARGV[0];\\n\\t\\tmy $msg = \"* $topic has commits already merged to public branch:\\\\n\";\\n\\t\\tmy (%not_in_next) = map {\\n\\t\\t\\t/^([0-9a-f]+) /;\\n\\t\\t\\t($1 => 1);\\n\\t\\t} split(/\\\\n/, $ARGV[1]);\\n\\t\\tfor my $elem (map {\\n\\t\\t\\t\\t/^([0-9a-f]+) (.*)$/;\\n\\t\\t\\t\\t[$1 => $2];\\n\\t\\t\\t} split(/\\\\n/, $ARGV[2])) {\\n\\t\\t\\tif (!exists $not_in_next{$elem->[0]}) {\\n\\t\\t\\t\\tif ($msg) {\\n\\t\\t\\t\\t\\tprint STDERR $msg;\\n\\t\\t\\t\\t\\tundef $msg;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tprint STDERR \" $elem->[1]\\\\n\";\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\' \"$topic\" \"$not_in_next\" \"$not_in_master\"\\n\\texit 1\\nfi\\n\\n<<\\\\DOC_END\\n\\nThis sample hook safeguards topic branches that have been\\npublished from being rewound.\\n\\nThe workflow assumed here is:\\n\\n * Once a topic branch forks from \"master\", \"master\" is never\\n   merged into it again (either directly or indirectly).\\n\\n * Once a topic branch is fully cooked and merged into \"master\",\\n   it is deleted.  If you need to build on top of it to correct\\n   earlier mistakes, a new topic branch is created by forking at\\n   the tip of the \"master\".  This is not strictly necessary, but\\n   it makes it easier to keep your history simple.\\n\\n * Whenever you need to test or publish your changes to topic\\n   branches, merge them into \"next\" branch.\\n\\nThe script, being an example, hardcodes the publish branch name\\nto be \"next\", but it is trivial to make it configurable via\\n$GIT_DIR/config mechanism.\\n\\nWith this workflow, you would want to know:\\n\\n(1) ... if a topic branch has ever been merged to \"next\".  Young\\n    topic branches can have stupid mistakes you would rather\\n    clean up before publishing, and things that have not been\\n    merged into other branches can be easily rebased without\\n    affecting other people.  But once it is published, you would\\n    not want to rewind it.\\n\\n(2) ... if a topic branch has been fully merged to \"master\".\\n    Then you can delete it.  More importantly, you should not\\n    build on top of it -- other people may already want to\\n    change things related to the topic as patches against your\\n    \"master\", so if you need further changes, it is better to\\n    fork the topic (perhaps with the same name) afresh from the\\n    tip of \"master\".\\n\\nLet\\'s look at this example:\\n\\n\\t\\t   o---o---o---o---o---o---o---o---o---o \"next\"\\n\\t\\t  /       /           /           /\\n\\t\\t /   a---a---b A     /           /\\n\\t\\t/   /               /           /\\n\\t       /   /   c---c---c---c B         /\\n\\t      /   /   /             \\\\         /\\n\\t     /   /   /   b---b C     \\\\       /\\n\\t    /   /   /   /             \\\\     /\\n    ---o---o---o---o---o---o---o---o---o---o---o \"master\"\\n\\n\\nA, B and C are topic branches.\\n\\n * A has one fix since it was merged up to \"next\".\\n\\n * B has finished.  It has been fully merged up to \"master\" and \"next\",\\n   and is ready to be deleted.\\n\\n * C has not merged to \"next\" at all.\\n\\nWe would want to allow C to be rebased, refuse A, and encourage\\nB to be deleted.\\n\\nTo compute (1):\\n\\n\\tgit rev-list ^master ^topic next\\n\\tgit rev-list ^master        next\\n\\n\\tif these match, topic has not merged in next at all.\\n\\nTo compute (2):\\n\\n\\tgit rev-list master..topic\\n\\n\\tif this is empty, it is fully merged to \"master\".\\n\\nDOC_END\\n\\n\\nFile Name: post-update.sample\\nContents of Filepost-update.sample: #!/bin/sh\\n#\\n# An example hook script to prepare a packed repository for use over\\n# dumb transports.\\n#\\n# To enable this hook, rename this file to \"post-update\".\\n\\nexec git update-server-info\\n\\n\\nFile Name: pre-receive.sample\\nContents of Filepre-receive.sample: #!/bin/sh\\n#\\n# An example hook script to make use of push options.\\n# The example simply echoes all push options that start with \\'echoback=\\'\\n# and rejects all pushes when the \"reject\" push option is used.\\n#\\n# To enable this hook, rename this file to \"pre-receive\".\\n\\nif test -n \"$GIT_PUSH_OPTION_COUNT\"\\nthen\\n\\ti=0\\n\\twhile test \"$i\" -lt \"$GIT_PUSH_OPTION_COUNT\"\\n\\tdo\\n\\t\\teval \"value=\\\\$GIT_PUSH_OPTION_$i\"\\n\\t\\tcase \"$value\" in\\n\\t\\techoback=*)\\n\\t\\t\\techo \"echo from the pre-receive-hook: ${value#*=}\" >&2\\n\\t\\t\\t;;\\n\\t\\treject)\\n\\t\\t\\texit 1\\n\\t\\tesac\\n\\t\\ti=$((i + 1))\\n\\tdone\\nfi\\n\\n\\nFile Name: applypatch-msg.sample\\nContents of Fileapplypatch-msg.sample: #!/bin/sh\\n#\\n# An example hook script to check the commit log message taken by\\n# applypatch from an e-mail message.\\n#\\n# The hook should exit with non-zero status after issuing an\\n# appropriate message if it wants to stop the commit.  The hook is\\n# allowed to edit the commit message file.\\n#\\n# To enable this hook, rename this file to \"applypatch-msg\".\\n\\n. git-sh-setup\\ncommitmsg=\"$(git rev-parse --git-path hooks/commit-msg)\"\\ntest -x \"$commitmsg\" && exec \"$commitmsg\" ${1+\"$@\"}\\n:\\n\\n\\nFile Name: push-to-checkout.sample\\nContents of Filepush-to-checkout.sample: #!/bin/sh\\n\\n# An example hook script to update a checked-out tree on a git push.\\n#\\n# This hook is invoked by git-receive-pack(1) when it reacts to git\\n# push and updates reference(s) in its repository, and when the push\\n# tries to update the branch that is currently checked out and the\\n# receive.denyCurrentBranch configuration variable is set to\\n# updateInstead.\\n#\\n# By default, such a push is refused if the working tree and the index\\n# of the remote repository has any difference from the currently\\n# checked out commit; when both the working tree and the index match\\n# the current commit, they are updated to match the newly pushed tip\\n# of the branch. This hook is to be used to override the default\\n# behaviour; however the code below reimplements the default behaviour\\n# as a starting point for convenient modification.\\n#\\n# The hook receives the commit with which the tip of the current\\n# branch is going to be updated:\\ncommit=$1\\n\\n# It can exit with a non-zero status to refuse the push (when it does\\n# so, it must not modify the index or the working tree).\\ndie () {\\n\\techo >&2 \"$*\"\\n\\texit 1\\n}\\n\\n# Or it can make any necessary changes to the working tree and to the\\n# index to bring them to the desired state when the tip of the current\\n# branch is updated to the new commit, and exit with a zero status.\\n#\\n# For example, the hook can simply run git read-tree -u -m HEAD \"$1\"\\n# in order to emulate git fetch that is run in the reverse direction\\n# with git push, as the two-tree form of git read-tree -u -m is\\n# essentially the same as git switch or git checkout that switches\\n# branches while keeping the local changes in the working tree that do\\n# not interfere with the difference between the branches.\\n\\n# The below is a more-or-less exact translation to shell of the C code\\n# for the default behaviour for git\\'s push-to-checkout hook defined in\\n# the push_to_deploy() function in builtin/receive-pack.c.\\n#\\n# Note that the hook will be executed from the repository directory,\\n# not from the working tree, so if you want to perform operations on\\n# the working tree, you will have to adapt your code accordingly, e.g.\\n# by adding \"cd ..\" or using relative paths.\\n\\nif ! git update-index -q --ignore-submodules --refresh\\nthen\\n\\tdie \"Up-to-date check failed\"\\nfi\\n\\nif ! git diff-files --quiet --ignore-submodules --\\nthen\\n\\tdie \"Working directory has unstaged changes\"\\nfi\\n\\n# This is a rough translation of:\\n#\\n#   head_has_history() ? \"HEAD\" : EMPTY_TREE_SHA1_HEX\\nif git cat-file -e HEAD 2>/dev/null\\nthen\\n\\thead=HEAD\\nelse\\n\\thead=$(git hash-object -t tree --stdin </dev/null)\\nfi\\n\\nif ! git diff-index --quiet --cached --ignore-submodules $head --\\nthen\\n\\tdie \"Working directory has staged changes\"\\nfi\\n\\nif ! git read-tree -u -m \"$commit\"\\nthen\\n\\tdie \"Could not update working tree to new HEAD\"\\nfi\\n\\n\\nFile Name: update.sample\\nContents of Fileupdate.sample: #!/bin/sh\\n#\\n# An example hook script to block unannotated tags from entering.\\n# Called by \"git receive-pack\" with arguments: refname sha1-old sha1-new\\n#\\n# To enable this hook, rename this file to \"update\".\\n#\\n# Config\\n# ------\\n# hooks.allowunannotated\\n#   This boolean sets whether unannotated tags will be allowed into the\\n#   repository.  By default they won\\'t be.\\n# hooks.allowdeletetag\\n#   This boolean sets whether deleting tags will be allowed in the\\n#   repository.  By default they won\\'t be.\\n# hooks.allowmodifytag\\n#   This boolean sets whether a tag may be modified after creation. By default\\n#   it won\\'t be.\\n# hooks.allowdeletebranch\\n#   This boolean sets whether deleting branches will be allowed in the\\n#   repository.  By default they won\\'t be.\\n# hooks.denycreatebranch\\n#   This boolean sets whether remotely creating branches will be denied\\n#   in the repository.  By default this is allowed.\\n#\\n\\n# --- Command line\\nrefname=\"$1\"\\noldrev=\"$2\"\\nnewrev=\"$3\"\\n\\n# --- Safety check\\nif [ -z \"$GIT_DIR\" ]; then\\n\\techo \"Don\\'t run this script from the command line.\" >&2\\n\\techo \" (if you want, you could supply GIT_DIR then run\" >&2\\n\\techo \"  $0 <ref> <oldrev> <newrev>)\" >&2\\n\\texit 1\\nfi\\n\\nif [ -z \"$refname\" -o -z \"$oldrev\" -o -z \"$newrev\" ]; then\\n\\techo \"usage: $0 <ref> <oldrev> <newrev>\" >&2\\n\\texit 1\\nfi\\n\\n# --- Config\\nallowunannotated=$(git config --type=bool hooks.allowunannotated)\\nallowdeletebranch=$(git config --type=bool hooks.allowdeletebranch)\\ndenycreatebranch=$(git config --type=bool hooks.denycreatebranch)\\nallowdeletetag=$(git config --type=bool hooks.allowdeletetag)\\nallowmodifytag=$(git config --type=bool hooks.allowmodifytag)\\n\\n# check for no description\\nprojectdesc=$(sed -e \\'1q\\' \"$GIT_DIR/description\")\\ncase \"$projectdesc\" in\\n\"Unnamed repository\"* | \"\")\\n\\techo \"*** Project description file hasn\\'t been set\" >&2\\n\\texit 1\\n\\t;;\\nesac\\n\\n# --- Check types\\n# if $newrev is 0000...0000, it\\'s a commit to delete a ref.\\nzero=$(git hash-object --stdin </dev/null | tr \\'[0-9a-f]\\' \\'0\\')\\nif [ \"$newrev\" = \"$zero\" ]; then\\n\\tnewrev_type=delete\\nelse\\n\\tnewrev_type=$(git cat-file -t $newrev)\\nfi\\n\\ncase \"$refname\",\"$newrev_type\" in\\n\\trefs/tags/*,commit)\\n\\t\\t# un-annotated tag\\n\\t\\tshort_refname=${refname##refs/tags/}\\n\\t\\tif [ \"$allowunannotated\" != \"true\" ]; then\\n\\t\\t\\techo \"*** The un-annotated tag, $short_refname, is not allowed in this repository\" >&2\\n\\t\\t\\techo \"*** Use \\'git tag [ -a | -s ]\\' for tags you want to propagate.\" >&2\\n\\t\\t\\texit 1\\n\\t\\tfi\\n\\t\\t;;\\n\\trefs/tags/*,delete)\\n\\t\\t# delete tag\\n\\t\\tif [ \"$allowdeletetag\" != \"true\" ]; then\\n\\t\\t\\techo \"*** Deleting a tag is not allowed in this repository\" >&2\\n\\t\\t\\texit 1\\n\\t\\tfi\\n\\t\\t;;\\n\\trefs/tags/*,tag)\\n\\t\\t# annotated tag\\n\\t\\tif [ \"$allowmodifytag\" != \"true\" ] && git rev-parse $refname > /dev/null 2>&1\\n\\t\\tthen\\n\\t\\t\\techo \"*** Tag \\'$refname\\' already exists.\" >&2\\n\\t\\t\\techo \"*** Modifying a tag is not allowed in this repository.\" >&2\\n\\t\\t\\texit 1\\n\\t\\tfi\\n\\t\\t;;\\n\\trefs/heads/*,commit)\\n\\t\\t# branch\\n\\t\\tif [ \"$oldrev\" = \"$zero\" -a \"$denycreatebranch\" = \"true\" ]; then\\n\\t\\t\\techo \"*** Creating a branch is not allowed in this repository\" >&2\\n\\t\\t\\texit 1\\n\\t\\tfi\\n\\t\\t;;\\n\\trefs/heads/*,delete)\\n\\t\\t# delete branch\\n\\t\\tif [ \"$allowdeletebranch\" != \"true\" ]; then\\n\\t\\t\\techo \"*** Deleting a branch is not allowed in this repository\" >&2\\n\\t\\t\\texit 1\\n\\t\\tfi\\n\\t\\t;;\\n\\trefs/remotes/*,commit)\\n\\t\\t# tracking branch\\n\\t\\t;;\\n\\trefs/remotes/*,delete)\\n\\t\\t# delete tracking branch\\n\\t\\tif [ \"$allowdeletebranch\" != \"true\" ]; then\\n\\t\\t\\techo \"*** Deleting a tracking branch is not allowed in this repository\" >&2\\n\\t\\t\\texit 1\\n\\t\\tfi\\n\\t\\t;;\\n\\t*)\\n\\t\\t# Anything else (is there anything else?)\\n\\t\\techo \"*** Update hook: unknown type of update to ref $refname of type $newrev_type\" >&2\\n\\t\\texit 1\\n\\t\\t;;\\nesac\\n\\n# --- Finished\\nexit 0\\n\\n\\nFile Name: pre-applypatch.sample\\nContents of Filepre-applypatch.sample: #!/bin/sh\\n#\\n# An example hook script to verify what is about to be committed\\n# by applypatch from an e-mail message.\\n#\\n# The hook should exit with non-zero status after issuing an\\n# appropriate message if it wants to stop the commit.\\n#\\n# To enable this hook, rename this file to \"pre-applypatch\".\\n\\n. git-sh-setup\\nprecommit=\"$(git rev-parse --git-path hooks/pre-commit)\"\\ntest -x \"$precommit\" && exec \"$precommit\" ${1+\"$@\"}\\n:\\n\\n\\nFile Name: pre-merge-commit.sample\\nContents of Filepre-merge-commit.sample: #!/bin/sh\\n#\\n# An example hook script to verify what is about to be committed.\\n# Called by \"git merge\" with no arguments.  The hook should\\n# exit with non-zero status after issuing an appropriate message to\\n# stderr if it wants to stop the merge commit.\\n#\\n# To enable this hook, rename this file to \"pre-merge-commit\".\\n\\n. git-sh-setup\\ntest -x \"$GIT_DIR/hooks/pre-commit\" &&\\n        exec \"$GIT_DIR/hooks/pre-commit\"\\n:\\n\\n\\nFile Name: commit-msg.sample\\nContents of Filecommit-msg.sample: #!/bin/sh\\n#\\n# An example hook script to check the commit log message.\\n# Called by \"git commit\" with one argument, the name of the file\\n# that has the commit message.  The hook should exit with non-zero\\n# status after issuing an appropriate message if it wants to stop the\\n# commit.  The hook is allowed to edit the commit message file.\\n#\\n# To enable this hook, rename this file to \"commit-msg\".\\n\\n# Uncomment the below to add a Signed-off-by line to the message.\\n# Doing this in a hook is a bad idea in general, but the prepare-commit-msg\\n# hook is more suited to it.\\n#\\n# SOB=$(git var GIT_AUTHOR_IDENT | sed -n \\'s/^\\\\(.*>\\\\).*$/Signed-off-by: \\\\1/p\\')\\n# grep -qs \"^$SOB\" \"$1\" || echo \"$SOB\" >> \"$1\"\\n\\n# This example catches duplicate Signed-off-by lines.\\n\\ntest \"\" = \"$(grep \\'^Signed-off-by: \\' \"$1\" |\\n\\t sort | uniq -c | sed -e \\'/^[ \\t]*1[ \\t]/d\\')\" || {\\n\\techo >&2 Duplicate Signed-off-by lines.\\n\\texit 1\\n}\\n\\n\\nFile Name: pre-commit.sample\\nContents of Filepre-commit.sample: #!/bin/sh\\n#\\n# An example hook script to verify what is about to be committed.\\n# Called by \"git commit\" with no arguments.  The hook should\\n# exit with non-zero status after issuing an appropriate message if\\n# it wants to stop the commit.\\n#\\n# To enable this hook, rename this file to \"pre-commit\".\\n\\nif git rev-parse --verify HEAD >/dev/null 2>&1\\nthen\\n\\tagainst=HEAD\\nelse\\n\\t# Initial commit: diff against an empty tree object\\n\\tagainst=$(git hash-object -t tree /dev/null)\\nfi\\n\\n# If you want to allow non-ASCII filenames set this variable to true.\\nallownonascii=$(git config --type=bool hooks.allownonascii)\\n\\n# Redirect output to stderr.\\nexec 1>&2\\n\\n# Cross platform projects tend to avoid non-ASCII filenames; prevent\\n# them from being added to the repository. We exploit the fact that the\\n# printable range starts at the space character and ends with tilde.\\nif [ \"$allownonascii\" != \"true\" ] &&\\n\\t# Note that the use of brackets around a tr range is ok here, (it\\'s\\n\\t# even required, for portability to Solaris 10\\'s /usr/bin/tr), since\\n\\t# the square bracket bytes happen to fall in the designated range.\\n\\ttest $(git diff --cached --name-only --diff-filter=A -z $against |\\n\\t  LC_ALL=C tr -d \\'[ -~]\\\\0\\' | wc -c) != 0\\nthen\\n\\tcat <<\\\\EOF\\nError: Attempt to add a non-ASCII file name.\\n\\nThis can cause problems if you want to work with people on other platforms.\\n\\nTo be portable it is advisable to rename the file.\\n\\nIf you know what you are doing you can disable this check using:\\n\\n  git config hooks.allownonascii true\\nEOF\\n\\texit 1\\nfi\\n\\n# If there are whitespace errors, print the offending file names and fail.\\nexec git diff-index --check --cached $against --\\n\\n\\nFile Name: prepare-commit-msg.sample\\nContents of Fileprepare-commit-msg.sample: #!/bin/sh\\n#\\n# An example hook script to prepare the commit log message.\\n# Called by \"git commit\" with the name of the file that has the\\n# commit message, followed by the description of the commit\\n# message\\'s source.  The hook\\'s purpose is to edit the commit\\n# message file.  If the hook fails with a non-zero status,\\n# the commit is aborted.\\n#\\n# To enable this hook, rename this file to \"prepare-commit-msg\".\\n\\n# This hook includes three examples. The first one removes the\\n# \"# Please enter the commit message...\" help message.\\n#\\n# The second includes the output of \"git diff --name-status -r\"\\n# into the message, just before the \"git status\" output.  It is\\n# commented because it doesn\\'t cope with --amend or with squashed\\n# commits.\\n#\\n# The third example adds a Signed-off-by line to the message, that can\\n# still be edited.  This is rarely a good idea.\\n\\nCOMMIT_MSG_FILE=$1\\nCOMMIT_SOURCE=$2\\nSHA1=$3\\n\\n/usr/bin/perl -i.bak -ne \\'print unless(m/^. Please enter the commit message/..m/^#$/)\\' \"$COMMIT_MSG_FILE\"\\n\\n# case \"$COMMIT_SOURCE,$SHA1\" in\\n#  ,|template,)\\n#    /usr/bin/perl -i.bak -pe \\'\\n#       print \"\\\\n\" . `git diff --cached --name-status -r`\\n# \\t if /^#/ && $first++ == 0\\' \"$COMMIT_MSG_FILE\" ;;\\n#  *) ;;\\n# esac\\n\\n# SOB=$(git var GIT_COMMITTER_IDENT | sed -n \\'s/^\\\\(.*>\\\\).*$/Signed-off-by: \\\\1/p\\')\\n# git interpret-trailers --in-place --trailer \"$SOB\" \"$COMMIT_MSG_FILE\"\\n# if test -z \"$COMMIT_SOURCE\"\\n# then\\n#   /usr/bin/perl -i.bak -pe \\'print \"\\\\n\" if !$first_line++\\' \"$COMMIT_MSG_FILE\"\\n# fi\\n\\n\\nFile Name: pre-push.sample\\nContents of Filepre-push.sample: #!/bin/sh\\n\\n# An example hook script to verify what is about to be pushed.  Called by \"git\\n# push\" after it has checked the remote status, but before anything has been\\n# pushed.  If this script exits with a non-zero status nothing will be pushed.\\n#\\n# This hook is called with the following parameters:\\n#\\n# $1 -- Name of the remote to which the push is being done\\n# $2 -- URL to which the push is being done\\n#\\n# If pushing without using a named remote those arguments will be equal.\\n#\\n# Information about the commits which are being pushed is supplied as lines to\\n# the standard input in the form:\\n#\\n#   <local ref> <local oid> <remote ref> <remote oid>\\n#\\n# This sample shows how to prevent push of commits where the log message starts\\n# with \"WIP\" (work in progress).\\n\\nremote=\"$1\"\\nurl=\"$2\"\\n\\nzero=$(git hash-object --stdin </dev/null | tr \\'[0-9a-f]\\' \\'0\\')\\n\\nwhile read local_ref local_oid remote_ref remote_oid\\ndo\\n\\tif test \"$local_oid\" = \"$zero\"\\n\\tthen\\n\\t\\t# Handle delete\\n\\t\\t:\\n\\telse\\n\\t\\tif test \"$remote_oid\" = \"$zero\"\\n\\t\\tthen\\n\\t\\t\\t# New branch, examine all commits\\n\\t\\t\\trange=\"$local_oid\"\\n\\t\\telse\\n\\t\\t\\t# Update to existing branch, examine new commits\\n\\t\\t\\trange=\"$remote_oid..$local_oid\"\\n\\t\\tfi\\n\\n\\t\\t# Check for WIP commit\\n\\t\\tcommit=$(git rev-list -n 1 --grep \\'^WIP\\' \"$range\")\\n\\t\\tif test -n \"$commit\"\\n\\t\\tthen\\n\\t\\t\\techo >&2 \"Found WIP commit in $local_ref, not pushing\"\\n\\t\\t\\texit 1\\n\\t\\tfi\\n\\tfi\\ndone\\n\\nexit 0\\n\\n\\n***Folder: info\\nFile Name: exclude\\nContents of Fileexclude: # git ls-files --others --exclude-from=.git/info/exclude\\n# Lines that start with \\'#\\' are comments.\\n# For a project mostly in C, the following would be a good set of\\n# exclude patterns (uncomment them if you want to use them):\\n# *.[oa]\\n# *~\\n\\n\\n***Folder: utils\\nFile Name: jax-only-sampled-distribution.py\\nContents of Filejax-only-sampled-distribution.py: from os import listdir, path, rename\\n\\nimport json\\nimport numpy as np\\nimport pdal\\nimport random\\n\\nrootdir = \\'/home/chambbj/data/ml-datasets/US3D/oma-only\\'\\ncounts = {0:0,2:0,5:0,6:0,7:0,9:0,17:0}\\nfor filename in listdir(path.join(rootdir, \"validation\")):\\n    p = pdal.Pipeline(json.dumps([path.join(rootdir, \"input_2.000\", filename)]))\\n    p.execute()\\n    pc = p.arrays[0]\\n\\n    classes = pc[\\'Classification\\']\\n    unique_classes = np.unique(classes)\\n    bins = np.bincount(classes)\\n\\n    for c in unique_classes:\\n        counts[c] += bins[c]\\n\\n    print(counts)\\n\\nFile Name: trainer.py\\nContents of Filetrainer.py: #\\n#\\n#      0=================================0\\n#      |    Kernel Point Convolutions    |\\n#      0=================================0\\n#\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Class handling the training of any model\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Hugues THOMAS - 11/06/2018\\n#\\n\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Imports and global variables\\n#       \\\\**********************************/\\n#\\n\\n\\n# Basic libs\\nimport torch\\nimport torch.nn as nn\\nimport numpy as np\\nimport pickle\\nimport os\\nfrom os import makedirs, remove\\nfrom os.path import exists, join\\nimport time\\nimport sys\\n\\n# LAS reader\\nfrom utils.las import write_las\\n\\n# Metrics\\nfrom utils.metrics import IoU_from_confusions, fast_confusion, metrics\\nfrom utils.config import Config\\nfrom sklearn.neighbors import KDTree\\n\\n# from torch.utils.tensorboard import SummaryWriter\\n\\nfrom models.blocks import KPConv\\n\\n\\nbest_acc = 0.0\\nbest_miou = 0.0\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Trainer Class\\n#       \\\\*******************/\\n#\\n\\n\\nclass ModelTrainer:\\n\\n    # Initialization methods\\n    # ------------------------------------------------------------------------------------------------------------------\\n\\n    def __init__(self, net, config, chkp_path=None, finetune=False, on_gpu=True):\\n        \"\"\"\\n        Initialize training parameters and reload previous model for restore/finetune\\n        :param net: network object\\n        :param config: configuration object\\n        :param chkp_path: path to the checkpoint that needs to be loaded (None for new training)\\n        :param finetune: finetune from checkpoint (True) or restore training from checkpoint (False)\\n        :param on_gpu: Train on GPU or CPU\\n        \"\"\"\\n\\n        ############\\n        # Parameters\\n        ############\\n\\n        # Epoch index\\n        self.epoch = 0\\n        self.step = 0\\n\\n        # Optimizer with specific learning rate for deformable KPConv\\n        deform_params = [v for k, v in net.named_parameters() if \\'offset\\' in k]\\n        other_params = [v for k, v in net.named_parameters() if \\'offset\\' not in k]\\n        deform_lr = config.learning_rate * config.deform_lr_factor\\n        self.optimizer = torch.optim.SGD([{\\'params\\': other_params},\\n                                          {\\'params\\': deform_params, \\'lr\\': deform_lr}],\\n                                         lr=config.learning_rate,\\n                                         momentum=config.momentum,\\n                                         weight_decay=config.weight_decay)\\n\\n        # Choose to train on CPU or GPU\\n        if on_gpu and torch.cuda.is_available():\\n            self.device = torch.device(\"cuda:0\")\\n        else:\\n            self.device = torch.device(\"cpu\")\\n        net.to(self.device)\\n\\n        ##########################\\n        # Load previous checkpoint\\n        ##########################\\n\\n        if (chkp_path is not None):\\n            if finetune:\\n                checkpoint = torch.load(chkp_path)\\n                net.load_state_dict(checkpoint[\\'model_state_dict\\'])\\n                net.train()\\n                print(\"Model restored and ready for finetuning.\")\\n            else:\\n                checkpoint = torch.load(chkp_path)\\n                net.load_state_dict(checkpoint[\\'model_state_dict\\'])\\n                self.optimizer.load_state_dict(checkpoint[\\'optimizer_state_dict\\'])\\n                self.epoch = checkpoint[\\'epoch\\']\\n                net.train()\\n                print(\"Model and training state restored.\")\\n\\n        # Path of the result folder\\n        if config.saving:\\n            if config.saving_path is None:\\n                config.saving_path = time.strftime(\\'results/Log_%Y-%m-%d_%H-%M-%S\\', time.gmtime())\\n            if not exists(config.saving_path):\\n                makedirs(config.saving_path)\\n            config.save()\\n\\n        return\\n\\n    # Training main method\\n    # ------------------------------------------------------------------------------------------------------------------\\n\\n    def train(self, net, training_loader, val_loader, config):\\n        \"\"\"\\n        Train the model on a particular dataset.\\n        \"\"\"\\n\\n        ################\\n        # Initialization\\n        ################\\n\\n        # writer = SummaryWriter(\\'runs/LPSVE-2m-none-ignored\\')\\n\\n        # writer.add_graph(net)\\n\\n        if config.saving:\\n            # Training log file\\n            with open(join(config.saving_path, \\'training.txt\\'), \"w\") as file:\\n                file.write(\\'epochs steps out_loss offset_loss train_accuracy time\\\\n\\')\\n\\n            # Killing file (simply delete this file when you want to stop the training)\\n            PID_file = join(config.saving_path, \\'running_PID.txt\\')\\n            if not exists(PID_file):\\n                with open(PID_file, \"w\") as file:\\n                    file.write(\\'Launched with PyCharm\\')\\n\\n            # Checkpoints directory\\n            checkpoint_directory = join(config.saving_path, \\'checkpoints\\')\\n            if not exists(checkpoint_directory):\\n                makedirs(checkpoint_directory)\\n        else:\\n            checkpoint_directory = None\\n            PID_file = None\\n\\n        # Loop variables\\n        t0 = time.time()\\n        t = [time.time()]\\n        last_display = time.time()\\n        mean_dt = np.zeros(1)\\n\\n        # Start training loop\\n        for epoch in range(config.max_epoch):\\n\\n            # Remove File for kill signal\\n            if epoch == config.max_epoch - 1 and exists(PID_file):\\n                remove(PID_file)\\n\\n            self.step = 0\\n            for batch in training_loader:\\n\\n                # Check kill signal (running_PID.txt deleted)\\n                if config.saving and not exists(PID_file):\\n                    continue\\n\\n                ##################\\n                # Processing batch\\n                ##################\\n\\n                # New time\\n                t = t[-1:]\\n                t += [time.time()]\\n\\n                if \\'cuda\\' in self.device.type:\\n                    batch.to(self.device)\\n\\n                # zero the parameter gradients\\n                self.optimizer.zero_grad()\\n\\n                # Forward pass\\n                outputs = net(batch, config)\\n                loss = net.loss(outputs, batch.labels)\\n                acc = net.accuracy(outputs, batch.labels)\\n\\n                t += [time.time()]\\n\\n                # Backward + optimize\\n                loss.backward()\\n\\n                if config.grad_clip_norm > 0:\\n                    #torch.nn.utils.clip_grad_norm_(net.parameters(), config.grad_clip_norm)\\n                    torch.nn.utils.clip_grad_value_(net.parameters(), config.grad_clip_norm)\\n                self.optimizer.step()\\n                torch.cuda.synchronize(self.device)\\n\\n                t += [time.time()]\\n\\n                # Average timing\\n                if self.step < 2:\\n                    mean_dt = np.array(t[1:]) - np.array(t[:-1])\\n                else:\\n                    mean_dt = 0.9 * mean_dt + 0.1 * (np.array(t[1:]) - np.array(t[:-1]))\\n\\n                # Console display (only one per second)\\n                if (t[-1] - last_display) > 1.0:\\n                    last_display = t[-1]\\n                    message = \\'e{:03d}-i{:04d} => L={:.3f} acc={:3.0f}% / t(ms): {:5.1f} {:5.1f} {:5.1f})\\'\\n                    print(message.format(self.epoch, self.step,\\n                                         loss.item(),\\n                                         100*acc,\\n                                         1000 * mean_dt[0],\\n                                         1000 * mean_dt[1],\\n                                         1000 * mean_dt[2]))\\n\\n                # Log file\\n                if config.saving:\\n                    with open(join(config.saving_path, \\'training.txt\\'), \"a\") as file:\\n                        message = \\'{:d} {:d} {:.3f} {:.3f} {:.3f} {:.3f}\\\\n\\'\\n                        file.write(message.format(self.epoch,\\n                                                  self.step,\\n                                                  net.output_loss,\\n                                                  net.reg_loss,\\n                                                  acc,\\n                                                  t[-1] - t0))\\n\\n                # Write to tensorboard here...\\n                if (self.epoch * len(training_loader) + self.step) % 9 == 0:\\n                    config.writer.add_scalar(\\'loss/training\\', loss.item(), (self.epoch * len(training_loader) + self.step))\\n                    config.writer.add_scalar(\\'output_loss/training\\', net.output_loss, (self.epoch * len(training_loader) + self.step))\\n                    config.writer.add_scalar(\\'reg_loss/training\\', net.reg_loss, (self.epoch * len(training_loader) + self.step))\\n                    config.writer.add_scalar(\\'acc/training\\', acc, (self.epoch * len(training_loader) + self.step))\\n\\n                self.step += 1\\n\\n            ##############\\n            # End of epoch\\n            ##############\\n\\n            # Check kill signal (running_PID.txt deleted)\\n            if config.saving and not exists(PID_file):\\n                break\\n\\n            # Update learning rate\\n            if self.epoch in config.lr_decays:\\n                for param_group in self.optimizer.param_groups:\\n                    param_group[\\'lr\\'] *= config.lr_decays[self.epoch]\\n\\n            # Update epoch\\n            self.epoch += 1\\n\\n            # Saving\\n            if config.saving:\\n                # Get current state dict\\n                save_dict = {\\'epoch\\': self.epoch,\\n                             \\'model_state_dict\\': net.state_dict(),\\n                             \\'optimizer_state_dict\\': self.optimizer.state_dict(),\\n                             \\'saving_path\\': config.saving_path}\\n\\n                # Save current state of the network (for restoring purposes)\\n                checkpoint_path = join(checkpoint_directory, \\'current_chkp.tar\\')\\n                torch.save(save_dict, checkpoint_path)\\n\\n                # Save checkpoints occasionally\\n                if (self.epoch + 1) % config.checkpoint_gap == 0:\\n                    checkpoint_path = join(checkpoint_directory, \\'chkp_{:04d}.tar\\'.format(self.epoch + 1))\\n                    torch.save(save_dict, checkpoint_path)\\n\\n            # Validation\\n            net.eval()\\n            self.validation(net, val_loader, config)\\n            net.train()\\n\\n        print(\\'Finished Training\\')\\n        config.writer.close()\\n        return\\n\\n    # Validation methods\\n    # ------------------------------------------------------------------------------------------------------------------\\n\\n    def validation(self, net, val_loader, config: Config):\\n\\n        if config.dataset_task == \\'classification\\':\\n            self.object_classification_validation(net, val_loader, config)\\n        elif config.dataset_task == \\'segmentation\\':\\n            self.object_segmentation_validation(net, val_loader, config)\\n        elif config.dataset_task == \\'cloud_segmentation\\':\\n            self.cloud_segmentation_validation(net, val_loader, config)\\n        elif config.dataset_task == \\'slam_segmentation\\':\\n            self.slam_segmentation_validation(net, val_loader, config)\\n        else:\\n            raise ValueError(\\'No validation method implemented for this network type\\')\\n\\n    def object_classification_validation(self, net, val_loader, config):\\n        \"\"\"\\n        Perform a round of validation and show/save results\\n        :param net: network object\\n        :param val_loader: data loader for validation set\\n        :param config: configuration object\\n        \"\"\"\\n\\n        ############\\n        # Initialize\\n        ############\\n\\n        # Choose validation smoothing parameter (0 for no smothing, 0.99 for big smoothing)\\n        val_smooth = 0.95\\n\\n        # Number of classes predicted by the model\\n        nc_model = config.num_classes\\n        softmax = torch.nn.Softmax(1)\\n\\n        # Initialize global prediction over all models\\n        if not hasattr(self, \\'val_probs\\'):\\n            self.val_probs = np.zeros((val_loader.dataset.num_models, nc_model))\\n\\n        #####################\\n        # Network predictions\\n        #####################\\n\\n        probs = []\\n        targets = []\\n        obj_inds = []\\n\\n        t = [time.time()]\\n        last_display = time.time()\\n        mean_dt = np.zeros(1)\\n\\n        # Start validation loop\\n        for batch in val_loader:\\n\\n            # New time\\n            t = t[-1:]\\n            t += [time.time()]\\n\\n            if \\'cuda\\' in self.device.type:\\n                batch.to(self.device)\\n\\n            # Forward pass\\n            outputs = net(batch, config)\\n\\n            # Get probs and labels\\n            probs += [softmax(outputs).cpu().detach().numpy()]\\n            targets += [batch.labels.cpu().numpy()]\\n            obj_inds += [batch.model_inds.cpu().numpy()]\\n            torch.cuda.synchronize(self.device)\\n\\n            # Average timing\\n            t += [time.time()]\\n            mean_dt = 0.95 * mean_dt + 0.05 * (np.array(t[1:]) - np.array(t[:-1]))\\n\\n            # Display\\n            if (t[-1] - last_display) > 1.0:\\n                last_display = t[-1]\\n                message = \\'Validation : {:.1f}% (timings : {:4.2f} {:4.2f})\\'\\n                print(message.format(100 * len(obj_inds) / config.validation_size,\\n                                     1000 * (mean_dt[0]),\\n                                     1000 * (mean_dt[1])))\\n\\n        # Stack all validation predictions\\n        probs = np.vstack(probs)\\n        targets = np.hstack(targets)\\n        obj_inds = np.hstack(obj_inds)\\n\\n        ###################\\n        # Voting validation\\n        ###################\\n\\n        self.val_probs[obj_inds] = val_smooth * self.val_probs[obj_inds] + (1-val_smooth) * probs\\n\\n        ############\\n        # Confusions\\n        ############\\n\\n        validation_labels = np.array(val_loader.dataset.label_values)\\n\\n        # Compute classification results\\n        C1 = fast_confusion(targets,\\n                            np.argmax(probs, axis=1),\\n                            validation_labels)\\n\\n        # Compute votes confusion\\n        C2 = fast_confusion(val_loader.dataset.input_labels,\\n                            np.argmax(self.val_probs, axis=1),\\n                            validation_labels)\\n\\n\\n        # Saving (optionnal)\\n        if config.saving:\\n            print(\"Save confusions\")\\n            conf_list = [C1, C2]\\n            file_list = [\\'val_confs.txt\\', \\'vote_confs.txt\\']\\n            for conf, conf_file in zip(conf_list, file_list):\\n                test_file = join(config.saving_path, conf_file)\\n                if exists(test_file):\\n                    with open(test_file, \"a\") as text_file:\\n                        for line in conf:\\n                            for value in line:\\n                                text_file.write(\\'%d \\' % value)\\n                        text_file.write(\\'\\\\n\\')\\n                else:\\n                    with open(test_file, \"w\") as text_file:\\n                        for line in conf:\\n                            for value in line:\\n                                text_file.write(\\'%d \\' % value)\\n                        text_file.write(\\'\\\\n\\')\\n\\n        val_ACC = 100 * np.sum(np.diag(C1)) / (np.sum(C1) + 1e-6)\\n        vote_ACC = 100 * np.sum(np.diag(C2)) / (np.sum(C2) + 1e-6)\\n        print(\\'Accuracies : val = {:.1f}% / vote = {:.1f}%\\'.format(val_ACC, vote_ACC))\\n\\n        return C1\\n\\n    def cloud_segmentation_validation(self, net, val_loader, config, debug=False):\\n        \"\"\"\\n        Validation method for cloud segmentation models\\n        \"\"\"\\n\\n        global best_acc, best_miou\\n\\n        ############\\n        # Initialize\\n        ############\\n\\n        t0 = time.time()\\n\\n        # Choose validation smoothing parameter (0 for no smothing, 0.99 for big smoothing)\\n        val_smooth = 0.95\\n        softmax = torch.nn.Softmax(1)\\n\\n        # Do not validate if dataset has no validation cloud\\n        if val_loader.dataset.validation_split not in val_loader.dataset.all_splits:\\n            return\\n\\n        # Number of classes including ignored labels\\n        nc_tot = val_loader.dataset.num_classes\\n\\n        # Number of classes predicted by the model\\n        nc_model = config.num_classes\\n\\n        #print(nc_tot)\\n        #print(nc_model)\\n\\n        # Initiate global prediction over validation clouds\\n        if not hasattr(self, \\'validation_probs\\'):\\n            self.validation_probs = [np.zeros((l.shape[0], nc_model))\\n                                     for l in val_loader.dataset.input_labels]\\n            self.val_proportions = np.zeros(nc_model, dtype=np.float32)\\n            i = 0\\n            for label_value in val_loader.dataset.label_values:\\n                if label_value not in val_loader.dataset.ignored_labels:\\n                    self.val_proportions[i] = np.sum([np.sum(labels == label_value)\\n                                                      for labels in val_loader.dataset.validation_labels])\\n                    i += 1\\n\\n        #####################\\n        # Network predictions\\n        #####################\\n\\n        predictions = []\\n        targets = []\\n\\n        t = [time.time()]\\n        last_display = time.time()\\n        mean_dt = np.zeros(1)\\n\\n\\n        t1 = time.time()\\n\\n        # Start validation loop\\n        for i, batch in enumerate(val_loader):\\n\\n            # New time\\n            t = t[-1:]\\n            t += [time.time()]\\n\\n            if \\'cuda\\' in self.device.type:\\n                batch.to(self.device)\\n\\n            # Forward pass\\n            outputs = net(batch, config)\\n\\n            # Get probs and labels\\n            stacked_probs = softmax(outputs).cpu().detach().numpy()\\n            labels = batch.labels.cpu().numpy()\\n            lengths = batch.lengths[0].cpu().numpy()\\n            in_inds = batch.input_inds.cpu().numpy()\\n            cloud_inds = batch.cloud_inds.cpu().numpy()\\n            torch.cuda.synchronize(self.device)\\n\\n            # Get predictions and labels per instance\\n            # ***************************************\\n\\n            i0 = 0\\n            for b_i, length in enumerate(lengths):\\n\\n                # Get prediction\\n                target = labels[i0:i0 + length]\\n                probs = stacked_probs[i0:i0 + length]\\n                inds = in_inds[i0:i0 + length]\\n                c_i = cloud_inds[b_i]\\n\\n                # Update current probs in whole cloud\\n                self.validation_probs[c_i][inds] = val_smooth * self.validation_probs[c_i][inds] \\\\\\n                                                   + (1 - val_smooth) * probs\\n\\n                # Stack all prediction for this epoch\\n                predictions.append(probs)\\n                targets.append(target)\\n                i0 += length\\n\\n            # Average timing\\n            t += [time.time()]\\n            mean_dt = 0.95 * mean_dt + 0.05 * (np.array(t[1:]) - np.array(t[:-1]))\\n\\n            # Display\\n            if (t[-1] - last_display) > 1.0:\\n                last_display = t[-1]\\n                message = \\'Validation : {:.1f}% (timings : {:4.2f} {:4.2f})\\'\\n                print(message.format(100 * i / config.validation_size,\\n                                     1000 * (mean_dt[0]),\\n                                     1000 * (mean_dt[1])))\\n\\n        t2 = time.time()\\n\\n        # Confusions for our subparts of validation set\\n        Confs = np.zeros((len(predictions), nc_tot, nc_tot), dtype=np.int32)\\n        for i, (probs, truth) in enumerate(zip(predictions, targets)):\\n\\n            # Insert false columns for ignored labels\\n            for l_ind, label_value in enumerate(val_loader.dataset.label_values):\\n                if label_value in val_loader.dataset.ignored_labels:\\n                    probs = np.insert(probs, l_ind, 0, axis=1)\\n\\n            # Predicted labels\\n            preds = np.array([val_loader.dataset.label_to_idx[l] for l in val_loader.dataset.label_values[np.argmax(probs, axis=1)]])\\n            # preds = val_loader.dataset.label_values[np.argmax(probs, axis=1)]\\n\\n            # Confusions\\n            Confs[i, :, :] = fast_confusion(truth, preds, val_loader.dataset.label_values).astype(np.int32)\\n            # Confs[i, :, :] = fast_confusion(truth, preds, np.array([0,1,2,3,4,5])).astype(np.int32)\\n\\n            print(Confs[i,:,:])\\n\\n\\n        t3 = time.time()\\n\\n        # Sum all confusions\\n        C = np.sum(Confs, axis=0).astype(np.float32)\\n\\n        # Remove ignored labels from confusions\\n        for l_ind, label_value in reversed(list(enumerate(val_loader.dataset.label_values))):\\n            if label_value in val_loader.dataset.ignored_labels:\\n                C = np.delete(C, l_ind, axis=0)\\n                C = np.delete(C, l_ind, axis=1)\\n\\n        # Balance with real validation proportions\\n        C *= np.expand_dims(self.val_proportions / (np.sum(C, axis=1) + 1e-6), 1)\\n\\n\\n        t4 = time.time()\\n\\n        # Objects IoU\\n        IoUs = IoU_from_confusions(C)\\n\\n        t5 = time.time()\\n\\n        # Saving (optionnal)\\n        if config.saving:\\n\\n            # Name of saving file\\n            test_file = join(config.saving_path, \\'val_IoUs.txt\\')\\n\\n            # Line to write:\\n            line = \\'\\'\\n            for IoU in IoUs:\\n                line += \\'{:.3f} \\'.format(IoU)\\n            line = line + \\'\\\\n\\'\\n\\n            # Write in file\\n            if exists(test_file):\\n                with open(test_file, \"a\") as text_file:\\n                    text_file.write(line)\\n            else:\\n                with open(test_file, \"w\") as text_file:\\n                    text_file.write(line)\\n\\n            # Save potentials\\n            if val_loader.dataset.use_potentials:\\n                pot_path = join(config.saving_path, \\'potentials\\')\\n                if not exists(pot_path):\\n                    makedirs(pot_path)\\n                files = val_loader.dataset.files\\n                for i, file_path in enumerate(files):\\n                    pot_points = np.array(val_loader.dataset.pot_trees[i].data, copy=False)\\n                    cloud_name = file_path.split(\\'/\\')[-1]\\n                    pot_name = join(pot_path, cloud_name)\\n                    pots = val_loader.dataset.potentials[i].numpy().astype(np.float32)\\n                    dimnames = \\'X,Y,Z,Potentials\\'\\n                    dimformats = \\'f8,f8,f8,f8\\'\\n                    foo = np.core.records.fromarrays(np.vstack((pot_points.T,pots.T)),names=dimnames,formats=dimformats)\\n                    write_las(pot_name, foo)\\n\\n        t6 = time.time()\\n\\n        # Print instance mean\\n        mIoU = 100 * np.mean(IoUs)\\n        print(\\'{:s} mean IoU = {:.1f}%\\'.format(config.dataset, mIoU))\\n\\n        config.writer.add_scalar(\\'miou\\', np.mean(IoUs), self.epoch)\\n        # for iou_idx, iou in enumerate(IoUs):\\n        #     config.writer.add_scalar(\\'validation_iou_{}\\'.format(iou_idx), iou, self.epoch)\\n\\n        PRE, REC, F1, IoU, ACC = metrics(C)\\n        for idx, pre in enumerate(PRE):\\n            config.writer.add_scalar(\\'precision/{}\\'.format(val_loader.dataset.label_to_names[val_loader.dataset.valid_labels[idx]]), pre, self.epoch)\\n        for idx, rec in enumerate(REC):\\n            config.writer.add_scalar(\\'recall/{}\\'.format(val_loader.dataset.label_to_names[val_loader.dataset.valid_labels[idx]]), rec, self.epoch)\\n        for idx, f1 in enumerate(F1):\\n            config.writer.add_scalar(\\'f1/{}\\'.format(val_loader.dataset.label_to_names[val_loader.dataset.valid_labels[idx]]), f1, self.epoch)\\n        for idx, iou in enumerate(IoU):\\n            config.writer.add_scalar(\\'iou/{}\\'.format(val_loader.dataset.label_to_names[val_loader.dataset.valid_labels[idx]]), iou, self.epoch)\\n        config.writer.add_scalar(\\'acc/validation\\', ACC, self.epoch)\\n\\n        print(ACC, best_acc)\\n        if ACC > best_acc:\\n            best_acc = ACC\\n            checkpoint_directory = join(config.saving_path, \\'checkpoints\\')\\n            if not exists(checkpoint_directory):\\n                makedirs(checkpoint_directory)\\n\\n            # Get current state dict\\n            save_dict = {\\'epoch\\': self.epoch,\\n                        \\'model_state_dict\\': net.state_dict(),\\n                        \\'optimizer_state_dict\\': self.optimizer.state_dict(),\\n                        \\'saving_path\\': config.saving_path}\\n\\n            # Save current state of the network (for restoring purposes)\\n            checkpoint_path = join(checkpoint_directory, \\'best_acc_chkp.tar\\')\\n            torch.save(save_dict, checkpoint_path)\\n\\n        print(np.mean(IoUs), best_miou)\\n        if np.mean(IoUs) > best_miou:\\n            best_miou = np.mean(IoUs)\\n            checkpoint_directory = join(config.saving_path, \\'checkpoints\\')\\n            if not exists(checkpoint_directory):\\n                makedirs(checkpoint_directory)\\n\\n            # Get current state dict\\n            save_dict = {\\'epoch\\': self.epoch,\\n                        \\'model_state_dict\\': net.state_dict(),\\n                        \\'optimizer_state_dict\\': self.optimizer.state_dict(),\\n                        \\'saving_path\\': config.saving_path}\\n\\n            # Save current state of the network (for restoring purposes)\\n            checkpoint_path = join(checkpoint_directory, \\'best_miou_chkp.tar\\')\\n            torch.save(save_dict, checkpoint_path)\\n\\n        # Save predicted cloud occasionally\\n        if config.saving and (self.epoch + 1) % config.checkpoint_gap == 0:\\n            val_path = join(config.saving_path, \\'val_preds_{:d}\\'.format(self.epoch + 1))\\n            if not exists(val_path):\\n                makedirs(val_path)\\n            files = val_loader.dataset.files\\n            for i, file_path in enumerate(files):\\n\\n                # Get points\\n                points = val_loader.dataset.load_evaluation_points(file_path)\\n\\n                # Get probs on our own ply points\\n                sub_probs = self.validation_probs[i]\\n\\n                # Insert false columns for ignored labels\\n                for l_ind, label_value in enumerate(val_loader.dataset.label_values):\\n                    if label_value in val_loader.dataset.ignored_labels:\\n                        sub_probs = np.insert(sub_probs, l_ind, 0, axis=1)\\n\\n                # Get the predicted labels\\n                sub_preds = val_loader.dataset.label_values[np.argmax(sub_probs, axis=1).astype(np.int32)]\\n\\n                # Reproject preds on the evaluations points\\n                preds = (sub_preds[val_loader.dataset.test_proj[i]]).astype(np.int32)\\n\\n                x0 = (sub_probs[:,0][val_loader.dataset.test_proj[i]]).astype(np.float32)\\n                x1 = (sub_probs[:,1][val_loader.dataset.test_proj[i]]).astype(np.float32)\\n                x2 = (sub_probs[:,2][val_loader.dataset.test_proj[i]]).astype(np.float32)\\n                x3 = (sub_probs[:,3][val_loader.dataset.test_proj[i]]).astype(np.float32)\\n                x4 = (sub_probs[:,4][val_loader.dataset.test_proj[i]]).astype(np.float32)\\n                # maxprobs = np.max(sub_probs, axis=1).astype(np.float32)\\n                # probs = (maxprobs[val_loader.dataset.test_proj[i]]).astype(np.float32)\\n\\n                # Path of saved validation file\\n                cloud_name = file_path.split(\\'/\\')[-1]\\n                val_name = join(val_path, cloud_name)\\n\\n                # Save file\\n                labels = val_loader.dataset.validation_labels[i].astype(np.int32)\\n                # write_ply(val_name,\\n                #           [points, preds, labels],\\n                #           [\\'x\\', \\'y\\', \\'z\\', \\'preds\\', \\'class\\'])\\n                dimnames = \\'X,Y,Z,Unlabeled,Ground,Bridge,Vegetation,Building,Prediction,Classification\\'\\n                dimformats = \\'f8,f8,f8,f8,f8,f8,f8,f8,u1,u1\\'\\n                foo = np.core.records.fromarrays(np.vstack((points.T,x0.T,x1.T,x2.T,x3.T,x4.T,preds.T,labels.T)),names=dimnames,formats=dimformats)\\n                write_las(val_name, foo)\\n\\n        # Display timings\\n        t7 = time.time()\\n        if debug:\\n            print(\\'\\\\n************************\\\\n\\')\\n            print(\\'Validation timings:\\')\\n            print(\\'Init ...... {:.1f}s\\'.format(t1 - t0))\\n            print(\\'Loop ...... {:.1f}s\\'.format(t2 - t1))\\n            print(\\'Confs ..... {:.1f}s\\'.format(t3 - t2))\\n            print(\\'Confs bis . {:.1f}s\\'.format(t4 - t3))\\n            print(\\'IoU ....... {:.1f}s\\'.format(t5 - t4))\\n            print(\\'Save1 ..... {:.1f}s\\'.format(t6 - t5))\\n            print(\\'Save2 ..... {:.1f}s\\'.format(t7 - t6))\\n            print(\\'\\\\n************************\\\\n\\')\\n\\n        return\\n\\n    def slam_segmentation_validation(self, net, val_loader, config, debug=True):\\n        \"\"\"\\n        Validation method for slam segmentation models\\n        \"\"\"\\n\\n        ############\\n        # Initialize\\n        ############\\n\\n        t0 = time.time()\\n\\n        # Do not validate if dataset has no validation cloud\\n        if val_loader is None:\\n            return\\n\\n        # Choose validation smoothing parameter (0 for no smothing, 0.99 for big smoothing)\\n        val_smooth = 0.95\\n        softmax = torch.nn.Softmax(1)\\n\\n        # Create folder for validation predictions\\n        if not exists (join(config.saving_path, \\'val_preds\\')):\\n            makedirs(join(config.saving_path, \\'val_preds\\'))\\n\\n        # initiate the dataset validation containers\\n        val_loader.dataset.val_points = []\\n        val_loader.dataset.val_labels = []\\n\\n        # Number of classes including ignored labels\\n        nc_tot = val_loader.dataset.num_classes\\n\\n        #####################\\n        # Network predictions\\n        #####################\\n\\n        predictions = []\\n        targets = []\\n        inds = []\\n        val_i = 0\\n\\n        t = [time.time()]\\n        last_display = time.time()\\n        mean_dt = np.zeros(1)\\n\\n\\n        t1 = time.time()\\n\\n        # Start validation loop\\n        for i, batch in enumerate(val_loader):\\n\\n            # New time\\n            t = t[-1:]\\n            t += [time.time()]\\n\\n            if \\'cuda\\' in self.device.type:\\n                batch.to(self.device)\\n\\n            # Forward pass\\n            outputs = net(batch, config)\\n\\n            # Get probs and labels\\n            stk_probs = softmax(outputs).cpu().detach().numpy()\\n            lengths = batch.lengths[0].cpu().numpy()\\n            f_inds = batch.frame_inds.cpu().numpy()\\n            r_inds_list = batch.reproj_inds\\n            r_mask_list = batch.reproj_masks\\n            labels_list = batch.val_labels\\n            torch.cuda.synchronize(self.device)\\n\\n            # Get predictions and labels per instance\\n            # ***************************************\\n\\n            i0 = 0\\n            for b_i, length in enumerate(lengths):\\n\\n                # Get prediction\\n                probs = stk_probs[i0:i0 + length]\\n                proj_inds = r_inds_list[b_i]\\n                proj_mask = r_mask_list[b_i]\\n                frame_labels = labels_list[b_i]\\n                s_ind = f_inds[b_i, 0]\\n                f_ind = f_inds[b_i, 1]\\n\\n                # Project predictions on the frame points\\n                proj_probs = probs[proj_inds]\\n\\n                # Safe check if only one point:\\n                if proj_probs.ndim < 2:\\n                    proj_probs = np.expand_dims(proj_probs, 0)\\n\\n                # Insert false columns for ignored labels\\n                for l_ind, label_value in enumerate(val_loader.dataset.label_values):\\n                    if label_value in val_loader.dataset.ignored_labels:\\n                        proj_probs = np.insert(proj_probs, l_ind, 0, axis=1)\\n\\n                # Predicted labels\\n                preds = val_loader.dataset.label_values[np.argmax(proj_probs, axis=1)]\\n\\n                # Save predictions in a binary file\\n                filename = \\'{:s}_{:07d}.npy\\'.format(val_loader.dataset.sequences[s_ind], f_ind)\\n                filepath = join(config.saving_path, \\'val_preds\\', filename)\\n                if exists(filepath):\\n                    frame_preds = np.load(filepath)\\n                else:\\n                    frame_preds = np.zeros(frame_labels.shape, dtype=np.uint8)\\n                frame_preds[proj_mask] = preds.astype(np.uint8)\\n                np.save(filepath, frame_preds)\\n\\n                # Save some of the frame pots\\n                if f_ind % 20 == 0:\\n                    seq_path = join(val_loader.dataset.path, \\'sequences\\', val_loader.dataset.sequences[s_ind])\\n                    velo_file = join(seq_path, \\'velodyne\\', val_loader.dataset.frames[s_ind][f_ind] + \\'.bin\\')\\n                    frame_points = np.fromfile(velo_file, dtype=np.float32)\\n                    frame_points = frame_points.reshape((-1, 4))\\n                    write_ply(filepath[:-4] + \\'_pots.ply\\',\\n                              [frame_points[:, :3], frame_labels, frame_preds],\\n                              [\\'x\\', \\'y\\', \\'z\\', \\'gt\\', \\'pre\\'])\\n\\n                # Update validation confusions\\n                frame_C = fast_confusion(frame_labels,\\n                                         frame_preds.astype(np.int32),\\n                                         val_loader.dataset.label_values)\\n                val_loader.dataset.val_confs[s_ind][f_ind, :, :] = frame_C\\n\\n                # Stack all prediction for this epoch\\n                predictions += [preds]\\n                targets += [frame_labels[proj_mask]]\\n                inds += [f_inds[b_i, :]]\\n                val_i += 1\\n                i0 += length\\n\\n            # Average timing\\n            t += [time.time()]\\n            mean_dt = 0.95 * mean_dt + 0.05 * (np.array(t[1:]) - np.array(t[:-1]))\\n\\n            # Display\\n            if (t[-1] - last_display) > 1.0:\\n                last_display = t[-1]\\n                message = \\'Validation : {:.1f}% (timings : {:4.2f} {:4.2f})\\'\\n                print(message.format(100 * i / config.validation_size,\\n                                     1000 * (mean_dt[0]),\\n                                     1000 * (mean_dt[1])))\\n\\n        t2 = time.time()\\n\\n        # Confusions for our subparts of validation set\\n        Confs = np.zeros((len(predictions), nc_tot, nc_tot), dtype=np.int32)\\n        for i, (preds, truth) in enumerate(zip(predictions, targets)):\\n\\n            # Confusions\\n            Confs[i, :, :] = fast_confusion(truth, preds, val_loader.dataset.label_values).astype(np.int32)\\n\\n        t3 = time.time()\\n\\n        #######################################\\n        # Results on this subpart of validation\\n        #######################################\\n\\n        # Sum all confusions\\n        C = np.sum(Confs, axis=0).astype(np.float32)\\n\\n        # Balance with real validation proportions\\n        C *= np.expand_dims(val_loader.dataset.class_proportions / (np.sum(C, axis=1) + 1e-6), 1)\\n\\n        # Remove ignored labels from confusions\\n        for l_ind, label_value in reversed(list(enumerate(val_loader.dataset.label_values))):\\n            if label_value in val_loader.dataset.ignored_labels:\\n                C = np.delete(C, l_ind, axis=0)\\n                C = np.delete(C, l_ind, axis=1)\\n\\n        # Objects IoU\\n        IoUs = IoU_from_confusions(C)\\n\\n        #####################################\\n        # Results on the whole validation set\\n        #####################################\\n\\n        t4 = time.time()\\n\\n        # Sum all validation confusions\\n        C_tot = [np.sum(seq_C, axis=0) for seq_C in val_loader.dataset.val_confs if len(seq_C) > 0]\\n        C_tot = np.sum(np.stack(C_tot, axis=0), axis=0)\\n\\n        if debug:\\n            s = \\'\\\\n\\'\\n            for cc in C_tot:\\n                for c in cc:\\n                    s += \\'{:8.1f} \\'.format(c)\\n                s += \\'\\\\n\\'\\n            print(s)\\n\\n        # Remove ignored labels from confusions\\n        for l_ind, label_value in reversed(list(enumerate(val_loader.dataset.label_values))):\\n            if label_value in val_loader.dataset.ignored_labels:\\n                C_tot = np.delete(C_tot, l_ind, axis=0)\\n                C_tot = np.delete(C_tot, l_ind, axis=1)\\n\\n        # Objects IoU\\n        val_IoUs = IoU_from_confusions(C_tot)\\n\\n        t5 = time.time()\\n\\n        # Saving (optionnal)\\n        if config.saving:\\n\\n            IoU_list = [IoUs, val_IoUs]\\n            file_list = [\\'subpart_IoUs.txt\\', \\'val_IoUs.txt\\']\\n            for IoUs_to_save, IoU_file in zip(IoU_list, file_list):\\n\\n                # Name of saving file\\n                test_file = join(config.saving_path, IoU_file)\\n\\n                # Line to write:\\n                line = \\'\\'\\n                for IoU in IoUs_to_save:\\n                    line += \\'{:.3f} \\'.format(IoU)\\n                line = line + \\'\\\\n\\'\\n\\n                # Write in file\\n                if exists(test_file):\\n                    with open(test_file, \"a\") as text_file:\\n                        text_file.write(line)\\n                else:\\n                    with open(test_file, \"w\") as text_file:\\n                        text_file.write(line)\\n\\n        # Print instance mean\\n        mIoU = 100 * np.mean(IoUs)\\n        print(\\'{:s} : subpart mIoU = {:.1f} %\\'.format(config.dataset, mIoU))\\n        mIoU = 100 * np.mean(val_IoUs)\\n        print(\\'{:s} :     val mIoU = {:.1f} %\\'.format(config.dataset, mIoU))\\n\\n        t6 = time.time()\\n\\n        # Display timings\\n        if debug:\\n            print(\\'\\\\n************************\\\\n\\')\\n            print(\\'Validation timings:\\')\\n            print(\\'Init ...... {:.1f}s\\'.format(t1 - t0))\\n            print(\\'Loop ...... {:.1f}s\\'.format(t2 - t1))\\n            print(\\'Confs ..... {:.1f}s\\'.format(t3 - t2))\\n            print(\\'IoU1 ...... {:.1f}s\\'.format(t4 - t3))\\n            print(\\'IoU2 ...... {:.1f}s\\'.format(t5 - t4))\\n            print(\\'Save ...... {:.1f}s\\'.format(t6 - t5))\\n            print(\\'\\\\n************************\\\\n\\')\\n\\n        return\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFile Name: tester.py\\nContents of Filetester.py: #\\n#\\n#      0=================================0\\n#      |    Kernel Point Convolutions    |\\n#      0=================================0\\n#\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Class handling the test of any model\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Hugues THOMAS - 11/06/2018\\n#\\n\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Imports and global variables\\n#       \\\\**********************************/\\n#\\n\\n\\n# Basic libs\\nimport torch\\nimport torch.nn as nn\\nimport numpy as np\\nfrom os import makedirs, listdir\\nfrom os.path import exists, join\\nimport time\\nimport json\\nfrom sklearn.neighbors import KDTree\\n\\n# PLY reader\\n# from utils.ply import read_ply, write_ply\\nfrom utils.las import write_las\\n\\n# Metrics\\nfrom utils.metrics import IoU_from_confusions, fast_confusion\\nfrom sklearn.metrics import confusion_matrix\\n\\n#from utils.visualizer import show_ModelNet_models\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Tester Class\\n#       \\\\******************/\\n#\\n\\n\\nclass ModelTester:\\n\\n    # Initialization methods\\n    # ------------------------------------------------------------------------------------------------------------------\\n\\n    def __init__(self, net, chkp_path=None, on_gpu=True):\\n\\n        ############\\n        # Parameters\\n        ############\\n\\n        # Choose to train on CPU or GPU\\n        if on_gpu and torch.cuda.is_available():\\n            self.device = torch.device(\"cuda:0\")\\n        else:\\n            self.device = torch.device(\"cpu\")\\n        net.to(self.device)\\n\\n        ##########################\\n        # Load previous checkpoint\\n        ##########################\\n\\n        if on_gpu and torch.cuda.is_available():\\n            checkpoint = torch.load(chkp_path)\\n        else:\\n            checkpoint = torch.load(chkp_path, map_location=torch.device(\\'cpu\\'))\\n        net.load_state_dict(checkpoint[\\'model_state_dict\\'])\\n        self.epoch = checkpoint[\\'epoch\\']\\n        net.eval()\\n        print(\"Model and training state restored.\")\\n\\n        return\\n\\n    # Test main methods\\n    # ------------------------------------------------------------------------------------------------------------------\\n\\n    def classification_test(self, net, test_loader, config, num_votes=100, debug=False):\\n\\n        ############\\n        # Initialize\\n        ############\\n\\n        # Choose test smoothing parameter (0 for no smothing, 0.99 for big smoothing)\\n        softmax = torch.nn.Softmax(1)\\n\\n        # Number of classes including ignored labels\\n        nc_tot = test_loader.dataset.num_classes\\n\\n        # Number of classes predicted by the model\\n        nc_model = config.num_classes\\n\\n        # Initiate global prediction over test clouds\\n        self.test_probs = np.zeros((test_loader.dataset.num_models, nc_model))\\n        self.test_counts = np.zeros((test_loader.dataset.num_models, nc_model))\\n\\n        t = [time.time()]\\n        mean_dt = np.zeros(1)\\n        last_display = time.time()\\n        while np.min(self.test_counts) < num_votes:\\n\\n            # Run model on all test examples\\n            # ******************************\\n\\n            # Initiate result containers\\n            probs = []\\n            targets = []\\n            obj_inds = []\\n\\n            # Start validation loop\\n            for batch in test_loader:\\n\\n                # New time\\n                t = t[-1:]\\n                t += [time.time()]\\n\\n                if \\'cuda\\' in self.device.type:\\n                    batch.to(self.device)\\n\\n                # Forward pass\\n                outputs = net(batch, config)\\n\\n                # Get probs and labels\\n                probs += [softmax(outputs).cpu().detach().numpy()]\\n                targets += [batch.labels.cpu().numpy()]\\n                obj_inds += [batch.model_inds.cpu().numpy()]\\n\\n                if \\'cuda\\' in self.device.type:\\n                    torch.cuda.synchronize(self.device)\\n\\n                # Average timing\\n                t += [time.time()]\\n                mean_dt = 0.95 * mean_dt + 0.05 * (np.array(t[1:]) - np.array(t[:-1]))\\n\\n                # Display\\n                if (t[-1] - last_display) > 1.0:\\n                    last_display = t[-1]\\n                    message = \\'Test vote {:.0f} : {:.1f}% (timings : {:4.2f} {:4.2f})\\'\\n                    print(message.format(np.min(self.test_counts),\\n                                         100 * len(obj_inds) / config.validation_size,\\n                                         1000 * (mean_dt[0]),\\n                                         1000 * (mean_dt[1])))\\n            # Stack all validation predictions\\n            probs = np.vstack(probs)\\n            targets = np.hstack(targets)\\n            obj_inds = np.hstack(obj_inds)\\n\\n            if np.any(test_loader.dataset.input_labels[obj_inds] != targets):\\n                raise ValueError(\\'wrong object indices\\')\\n\\n            # Compute incremental average (predictions are always ordered)\\n            self.test_counts[obj_inds] += 1\\n            self.test_probs[obj_inds] += (probs - self.test_probs[obj_inds]) / (self.test_counts[obj_inds])\\n\\n            # Save/Display temporary results\\n            # ******************************\\n\\n            test_labels = np.array(test_loader.dataset.label_values)\\n\\n            # Compute classification results\\n            C1 = fast_confusion(test_loader.dataset.input_labels,\\n                                np.argmax(self.test_probs, axis=1),\\n                                test_labels)\\n\\n            ACC = 100 * np.sum(np.diag(C1)) / (np.sum(C1) + 1e-6)\\n            print(\\'Test Accuracy = {:.1f}%\\'.format(ACC))\\n\\n        return\\n\\n    def cloud_segmentation_test(self, net, test_loader, config, num_votes=30, debug=False):\\n        \"\"\"\\n        Test method for cloud segmentation models\\n        \"\"\"\\n\\n        ############\\n        # Initialize\\n        ############\\n\\n        # Choose test smoothing parameter (0 for no smothing, 0.99 for big smoothing)\\n        test_smooth = 0.95\\n        test_radius_ratio = 0.7\\n        softmax = torch.nn.Softmax(1)\\n\\n        # Number of classes including ignored labels\\n        nc_tot = test_loader.dataset.num_classes\\n\\n        # Number of classes predicted by the model\\n        nc_model = config.num_classes\\n\\n        print(\"Expected class #\\'s \", nc_tot, nc_model)\\n        print(test_loader.dataset.input_labels)\\n\\n        # Initiate global prediction over test clouds\\n        self.test_probs = [np.zeros((l.shape[0], nc_model)) for l in test_loader.dataset.input_labels]\\n\\n        # Test saving path\\n        if config.saving:\\n            test_path = join(\\'test\\', config.saving_path.split(\\'/\\')[-1])\\n            if not exists(test_path):\\n                makedirs(test_path)\\n            if not exists(join(test_path, \\'predictions\\')):\\n                makedirs(join(test_path, \\'predictions\\'))\\n            if not exists(join(test_path, \\'probs\\')):\\n                makedirs(join(test_path, \\'probs\\'))\\n            if not exists(join(test_path, \\'potentials\\')):\\n                makedirs(join(test_path, \\'potentials\\'))\\n        else:\\n            test_path = None\\n\\n        # If on validation directly compute score\\n        if test_loader.dataset.set == \\'validation\\':\\n            val_proportions = np.zeros(nc_model, dtype=np.float32)\\n            i = 0\\n            for label_value in test_loader.dataset.label_values:\\n                if label_value not in test_loader.dataset.ignored_labels:\\n                    val_proportions[i] = np.sum([np.sum(labels == label_value)\\n                                                 for labels in test_loader.dataset.validation_labels])\\n                    i += 1\\n        else:\\n            val_proportions = None\\n\\n        #####################\\n        # Network predictions\\n        #####################\\n\\n        test_epoch = 0\\n        last_min = -0.5\\n\\n        t = [time.time()]\\n        last_display = time.time()\\n        mean_dt = np.zeros(1)\\n\\n        # Start test loop\\n        while True:\\n            print(\\'Initialize workers\\')\\n            for i, batch in enumerate(test_loader):\\n\\n                # New time\\n                t = t[-1:]\\n                t += [time.time()]\\n\\n                if i == 0:\\n                    print(\\'Done in {:.1f}s\\'.format(t[1] - t[0]))\\n\\n                if \\'cuda\\' in self.device.type:\\n                    batch.to(self.device)\\n\\n                # Forward pass\\n                outputs = net(batch, config)\\n\\n                t += [time.time()]\\n\\n                # Get probs and labels\\n                stacked_probs = softmax(outputs).cpu().detach().numpy()\\n                s_points = batch.points[0].cpu().numpy()\\n                lengths = batch.lengths[0].cpu().numpy()\\n                in_inds = batch.input_inds.cpu().numpy()\\n                cloud_inds = batch.cloud_inds.cpu().numpy()\\n                if \\'cuda\\' in self.device.type:\\n                    torch.cuda.synchronize(self.device)\\n\\n                # Get predictions and labels per instance\\n                # ***************************************\\n\\n                i0 = 0\\n                for b_i, length in enumerate(lengths):\\n\\n                    # Get prediction\\n                    points = s_points[i0:i0 + length]\\n                    probs = stacked_probs[i0:i0 + length]\\n                    inds = in_inds[i0:i0 + length]\\n                    c_i = cloud_inds[b_i]\\n\\n                    if 0 < test_radius_ratio < 1:\\n                        mask = np.sum(points ** 2, axis=1) < (test_radius_ratio * config.in_radius) ** 2\\n                        inds = inds[mask]\\n                        probs = probs[mask]\\n\\n                    # Update current probs in whole cloud\\n                    self.test_probs[c_i][inds] = test_smooth * self.test_probs[c_i][inds] + (1 - test_smooth) * probs\\n                    i0 += length\\n\\n                # Average timing\\n                t += [time.time()]\\n                if i < 2:\\n                    mean_dt = np.array(t[1:]) - np.array(t[:-1])\\n                else:\\n                    mean_dt = 0.9 * mean_dt + 0.1 * (np.array(t[1:]) - np.array(t[:-1]))\\n\\n                # Display\\n                if (t[-1] - last_display) > 1.0:\\n                    last_display = t[-1]\\n                    message = \\'e{:03d}-i{:04d} => {:.1f}% (timings : {:4.2f} {:4.2f} {:4.2f})\\'\\n                    print(message.format(test_epoch, i,\\n                                         100 * i / config.validation_size,\\n                                         1000 * (mean_dt[0]),\\n                                         1000 * (mean_dt[1]),\\n                                         1000 * (mean_dt[2])))\\n\\n            # Update minimum od potentials\\n            new_min = torch.min(test_loader.dataset.min_potentials)\\n            print(\\'Test epoch {:d}, end. Min potential = {:.1f}\\'.format(test_epoch, new_min))\\n            #print([np.mean(pots) for pots in test_loader.dataset.potentials])\\n\\n            # Save predicted cloud\\n            if last_min + 1 < new_min:\\n\\n                # Update last_min\\n                last_min += 1\\n\\n                # Show vote results (On subcloud so it is not the good values here)\\n                if test_loader.dataset.set == \\'validation\\':\\n                    print(\\'\\\\nConfusion on sub clouds\\')\\n                    Confs = []\\n                    for i, file_path in enumerate(test_loader.dataset.files):\\n\\n                        # Insert false columns for ignored labels\\n                        probs = np.array(self.test_probs[i], copy=True)\\n                        for l_ind, label_value in enumerate(test_loader.dataset.label_values):\\n                            if label_value in test_loader.dataset.ignored_labels:\\n                                probs = np.insert(probs, l_ind, 0, axis=1)\\n                        print(probs.shape)\\n                        print(probs[0])\\n                        print(np.argmax(probs,axis=1)[0])\\n                        print(test_loader.dataset.label_values)\\n\\n                        # Predicted labels\\n                        preds = test_loader.dataset.label_values[np.argmax(probs, axis=1)].astype(np.int32)\\n\\n                        # Targets\\n                        targets = test_loader.dataset.input_labels[i].astype(np.int32)\\n\\n                        print(np.unique(preds))\\n                        print(preds[0])\\n                        print(np.unique(targets))\\n                        print(targets)\\n\\n                        # Confs\\n                        Confs += [fast_confusion(targets, preds, test_loader.dataset.label_values)]\\n\\n                    # Regroup confusions\\n                    C = np.sum(np.stack(Confs), axis=0).astype(np.float32)\\n\\n                    # Remove ignored labels from confusions\\n                    for l_ind, label_value in reversed(list(enumerate(test_loader.dataset.label_values))):\\n                        if label_value in test_loader.dataset.ignored_labels:\\n                            C = np.delete(C, l_ind, axis=0)\\n                            C = np.delete(C, l_ind, axis=1)\\n\\n                    # Rescale with the right number of point per class\\n                    C *= np.expand_dims(val_proportions / (np.sum(C, axis=1) + 1e-6), 1)\\n\\n                    # Compute IoUs\\n                    IoUs = IoU_from_confusions(C)\\n                    mIoU = np.mean(IoUs)\\n                    s = \\'{:5.2f} | \\'.format(100 * mIoU)\\n                    for IoU in IoUs:\\n                        s += \\'{:5.2f} \\'.format(100 * IoU)\\n                    print(s + \\'\\\\n\\')\\n\\n                # Save real IoU once in a while\\n                if int(np.ceil(new_min)) % 10 == 0:\\n\\n                    # Project predictions\\n                    print(\\'\\\\nReproject Vote #{:d}\\'.format(int(np.floor(new_min))))\\n                    t1 = time.time()\\n                    proj_probs = []\\n                    for i, file_path in enumerate(test_loader.dataset.files):\\n\\n                        print(i, file_path, test_loader.dataset.test_proj[i].shape, self.test_probs[i].shape)\\n\\n                        print(test_loader.dataset.test_proj[i].dtype, np.max(test_loader.dataset.test_proj[i]))\\n                        print(test_loader.dataset.test_proj[i][:5])\\n\\n                        # Reproject probs on the evaluations points\\n                        probs = self.test_probs[i][test_loader.dataset.test_proj[i], :]\\n                        proj_probs += [probs]\\n\\n                    t2 = time.time()\\n                    print(\\'Done in {:.1f} s\\\\n\\'.format(t2 - t1))\\n\\n                    # Show vote results\\n                    if test_loader.dataset.set == \\'validation\\':\\n                        print(\\'Confusion on full clouds\\')\\n                        t1 = time.time()\\n                        Confs = []\\n                        for i, file_path in enumerate(test_loader.dataset.files):\\n\\n                            # Insert false columns for ignored labels\\n                            for l_ind, label_value in enumerate(test_loader.dataset.label_values):\\n                                if label_value in test_loader.dataset.ignored_labels:\\n                                    proj_probs[i] = np.insert(proj_probs[i], l_ind, 0, axis=1)\\n\\n                            # Get the predicted labels\\n                            preds = test_loader.dataset.label_values[np.argmax(proj_probs[i], axis=1)].astype(np.int32)\\n\\n                            # Confusion\\n                            targets = test_loader.dataset.validation_labels[i].astype(np.int32)\\n                            Confs += [fast_confusion(targets, preds, test_loader.dataset.label_values)]\\n\\n                        t2 = time.time()\\n                        print(\\'Done in {:.1f} s\\\\n\\'.format(t2 - t1))\\n\\n                        # Regroup confusions\\n                        C = np.sum(np.stack(Confs), axis=0)\\n\\n                        # Remove ignored labels from confusions\\n                        for l_ind, label_value in reversed(list(enumerate(test_loader.dataset.label_values))):\\n                            if label_value in test_loader.dataset.ignored_labels:\\n                                C = np.delete(C, l_ind, axis=0)\\n                                C = np.delete(C, l_ind, axis=1)\\n\\n                        IoUs = IoU_from_confusions(C)\\n                        mIoU = np.mean(IoUs)\\n                        s = \\'{:5.2f} | \\'.format(100 * mIoU)\\n                        for IoU in IoUs:\\n                            s += \\'{:5.2f} \\'.format(100 * IoU)\\n                        print(\\'-\\' * len(s))\\n                        print(s)\\n                        print(\\'-\\' * len(s) + \\'\\\\n\\')\\n\\n                    # Save predictions\\n                    print(\\'Saving clouds\\')\\n                    t1 = time.time()\\n                    for i, file_path in enumerate(test_loader.dataset.files):\\n\\n                        # Get file\\n                        points = test_loader.dataset.load_evaluation_points(file_path)\\n\\n                        # Get the predicted labels\\n                        # valid_labels = np.array([label for label in test_loader.dataset.label_values if label not in test_loader.dataset.ignored_labels])\\n                        preds = test_loader.dataset.valid_labels[np.argmax(proj_probs[i], axis=1)].astype(np.int32)\\n\\n                        # Save plys\\n                        cloud_name = file_path.split(\\'/\\')[-1]\\n                        test_name = join(test_path, \\'predictions\\', cloud_name)\\n                        # write_ply(test_name,\\n                                #   [points, preds],\\n                                #   [\\'x\\', \\'y\\', \\'z\\', \\'preds\\'])\\n                        dimnames = \\'X,Y,Z,Classification\\'\\n                        dimformats = \\'f8,f8,f8,u1\\'\\n                        foo = np.core.records.fromarrays(np.vstack((points.T,preds.T)),names=dimnames,formats=dimformats)\\n                        write_las(test_name, foo)\\n                        # test_name2 = join(test_path, \\'probs\\', cloud_name)\\n                        # prob_names = [\\'_\\'.join(test_loader.dataset.label_to_names[label].split())\\n                        #               for label in test_loader.dataset.label_values]\\n                        # write_ply(test_name2,\\n                        #           [points, proj_probs[i]],\\n                        #           [\\'x\\', \\'y\\', \\'z\\'] + prob_names)\\n\\n                        # Save potentials\\n                        pot_points = np.array(test_loader.dataset.pot_trees[i].data, copy=False)\\n                        pot_name = join(test_path, \\'potentials\\', cloud_name)\\n                        pots = test_loader.dataset.potentials[i].numpy().astype(np.float32)\\n                        dimnames = \\'X,Y,Z,Potentials\\'\\n                        dimformats = \\'f8,f8,f8,f8\\'\\n                        foo = np.core.records.fromarrays(np.vstack((pot_points.T,pots.T)),names=dimnames,formats=dimformats)\\n                        write_las(pot_name, foo)\\n                        # write_ply(pot_name,\\n                        #           [pot_points.astype(np.float32), pots],\\n                        #           [\\'x\\', \\'y\\', \\'z\\', \\'pots\\'])\\n\\n                        # Save ascii preds\\n                        if test_loader.dataset.set == \\'test\\':\\n                            if test_loader.dataset.name.startswith(\\'Semantic3D\\'):\\n                                ascii_name = join(test_path, \\'predictions\\', test_loader.dataset.ascii_files[cloud_name])\\n                            else:\\n                                ascii_name = join(test_path, \\'predictions\\', cloud_name[:-4] + \\'.txt\\')\\n                            np.savetxt(ascii_name, preds, fmt=\\'%d\\')\\n\\n                    t2 = time.time()\\n                    print(\\'Done in {:.1f} s\\\\n\\'.format(t2 - t1))\\n\\n            test_epoch += 1\\n\\n            # Break when reaching number of desired votes\\n            if last_min > num_votes:\\n                break\\n\\n        return\\n\\n    def slam_segmentation_test(self, net, test_loader, config, num_votes=100, debug=True):\\n        \"\"\"\\n        Test method for slam segmentation models\\n        \"\"\"\\n\\n        ############\\n        # Initialize\\n        ############\\n\\n        # Choose validation smoothing parameter (0 for no smothing, 0.99 for big smoothing)\\n        test_smooth = 0.5\\n        last_min = -0.5\\n        softmax = torch.nn.Softmax(1)\\n\\n        # Number of classes including ignored labels\\n        nc_tot = test_loader.dataset.num_classes\\n        nc_model = net.C\\n\\n        # Test saving path\\n        test_path = None\\n        report_path = None\\n        if config.saving:\\n            test_path = join(\\'test\\', config.saving_path.split(\\'/\\')[-1])\\n            if not exists(test_path):\\n                makedirs(test_path)\\n            report_path = join(test_path, \\'reports\\')\\n            if not exists(report_path):\\n                makedirs(report_path)\\n\\n        if test_loader.dataset.set == \\'validation\\':\\n            for folder in [\\'val_predictions\\', \\'val_probs\\']:\\n                if not exists(join(test_path, folder)):\\n                    makedirs(join(test_path, folder))\\n        else:\\n            for folder in [\\'predictions\\', \\'probs\\']:\\n                if not exists(join(test_path, folder)):\\n                    makedirs(join(test_path, folder))\\n\\n        # Init validation container\\n        all_f_preds = []\\n        all_f_labels = []\\n        if test_loader.dataset.set == \\'validation\\':\\n            for i, seq_frames in enumerate(test_loader.dataset.frames):\\n                all_f_preds.append([np.zeros((0,), dtype=np.int32) for _ in seq_frames])\\n                all_f_labels.append([np.zeros((0,), dtype=np.int32) for _ in seq_frames])\\n\\n        #####################\\n        # Network predictions\\n        #####################\\n\\n        predictions = []\\n        targets = []\\n        test_epoch = 0\\n\\n        t = [time.time()]\\n        last_display = time.time()\\n        mean_dt = np.zeros(1)\\n\\n        # Start test loop\\n        while True:\\n            print(\\'Initialize workers\\')\\n            for i, batch in enumerate(test_loader):\\n\\n                # New time\\n                t = t[-1:]\\n                t += [time.time()]\\n\\n                if i == 0:\\n                    print(\\'Done in {:.1f}s\\'.format(t[1] - t[0]))\\n\\n                if \\'cuda\\' in self.device.type:\\n                    batch.to(self.device)\\n\\n                # Forward pass\\n                outputs = net(batch, config)\\n\\n                # Get probs and labels\\n                stk_probs = softmax(outputs).cpu().detach().numpy()\\n                lengths = batch.lengths[0].cpu().numpy()\\n                f_inds = batch.frame_inds.cpu().numpy()\\n                r_inds_list = batch.reproj_inds\\n                r_mask_list = batch.reproj_masks\\n                labels_list = batch.val_labels\\n                if \\'cuda\\' in self.device.type:\\n                    torch.cuda.synchronize(self.device)\\n\\n                t += [time.time()]\\n\\n                # Get predictions and labels per instance\\n                # ***************************************\\n\\n                i0 = 0\\n                for b_i, length in enumerate(lengths):\\n\\n                    # Get prediction\\n                    probs = stk_probs[i0:i0 + length]\\n                    proj_inds = r_inds_list[b_i]\\n                    proj_mask = r_mask_list[b_i]\\n                    frame_labels = labels_list[b_i]\\n                    s_ind = f_inds[b_i, 0]\\n                    f_ind = f_inds[b_i, 1]\\n\\n                    # Project predictions on the frame points\\n                    proj_probs = probs[proj_inds]\\n\\n                    # Safe check if only one point:\\n                    if proj_probs.ndim < 2:\\n                        proj_probs = np.expand_dims(proj_probs, 0)\\n\\n                    # Save probs in a binary file (uint8 format for lighter weight)\\n                    seq_name = test_loader.dataset.sequences[s_ind]\\n                    if test_loader.dataset.set == \\'validation\\':\\n                        folder = \\'val_probs\\'\\n                        pred_folder = \\'val_predictions\\'\\n                    else:\\n                        folder = \\'probs\\'\\n                        pred_folder = \\'predictions\\'\\n                    filename = \\'{:s}_{:07d}.npy\\'.format(seq_name, f_ind)\\n                    filepath = join(test_path, folder, filename)\\n                    if exists(filepath):\\n                        frame_probs_uint8 = np.load(filepath)\\n                    else:\\n                        frame_probs_uint8 = np.zeros((proj_mask.shape[0], nc_model), dtype=np.uint8)\\n                    frame_probs = frame_probs_uint8[proj_mask, :].astype(np.float32) / 255\\n                    frame_probs = test_smooth * frame_probs + (1 - test_smooth) * proj_probs\\n                    frame_probs_uint8[proj_mask, :] = (frame_probs * 255).astype(np.uint8)\\n                    np.save(filepath, frame_probs_uint8)\\n\\n                    # Save some prediction in ply format for visual\\n                    if test_loader.dataset.set == \\'validation\\':\\n\\n                        # Insert false columns for ignored labels\\n                        frame_probs_uint8_bis = frame_probs_uint8.copy()\\n                        for l_ind, label_value in enumerate(test_loader.dataset.label_values):\\n                            if label_value in test_loader.dataset.ignored_labels:\\n                                frame_probs_uint8_bis = np.insert(frame_probs_uint8_bis, l_ind, 0, axis=1)\\n\\n                        # Predicted labels\\n                        frame_preds = test_loader.dataset.label_values[np.argmax(frame_probs_uint8_bis,\\n                                                                                 axis=1)].astype(np.int32)\\n\\n                        # Save some of the frame pots\\n                        if f_ind % 20 == 0:\\n                            seq_path = join(test_loader.dataset.path, \\'sequences\\', test_loader.dataset.sequences[s_ind])\\n                            velo_file = join(seq_path, \\'velodyne\\', test_loader.dataset.frames[s_ind][f_ind] + \\'.bin\\')\\n                            frame_points = np.fromfile(velo_file, dtype=np.float32)\\n                            frame_points = frame_points.reshape((-1, 4))\\n                            predpath = join(test_path, pred_folder, filename[:-4] + \\'.ply\\')\\n                            #pots = test_loader.dataset.f_potentials[s_ind][f_ind]\\n                            pots = np.zeros((0,))\\n                            if pots.shape[0] > 0:\\n                                write_ply(predpath,\\n                                          [frame_points[:, :3], frame_labels, frame_preds, pots],\\n                                          [\\'x\\', \\'y\\', \\'z\\', \\'gt\\', \\'pre\\', \\'pots\\'])\\n                            else:\\n                                write_ply(predpath,\\n                                          [frame_points[:, :3], frame_labels, frame_preds],\\n                                          [\\'x\\', \\'y\\', \\'z\\', \\'gt\\', \\'pre\\'])\\n\\n                            # Also Save lbl probabilities\\n                            probpath = join(test_path, folder, filename[:-4] + \\'_probs.ply\\')\\n                            lbl_names = [test_loader.dataset.label_to_names[l]\\n                                         for l in test_loader.dataset.label_values\\n                                         if l not in test_loader.dataset.ignored_labels]\\n                            write_ply(probpath,\\n                                      [frame_points[:, :3], frame_probs_uint8],\\n                                      [\\'x\\', \\'y\\', \\'z\\'] + lbl_names)\\n\\n                        # keep frame preds in memory\\n                        all_f_preds[s_ind][f_ind] = frame_preds\\n                        all_f_labels[s_ind][f_ind] = frame_labels\\n\\n                    else:\\n\\n                        # Save some of the frame preds\\n                        if f_inds[b_i, 1] % 100 == 0:\\n\\n                            # Insert false columns for ignored labels\\n                            for l_ind, label_value in enumerate(test_loader.dataset.label_values):\\n                                if label_value in test_loader.dataset.ignored_labels:\\n                                    frame_probs_uint8 = np.insert(frame_probs_uint8, l_ind, 0, axis=1)\\n\\n                            # Predicted labels\\n                            frame_preds = test_loader.dataset.label_values[np.argmax(frame_probs_uint8,\\n                                                                                     axis=1)].astype(np.int32)\\n\\n                            # Load points\\n                            seq_path = join(test_loader.dataset.path, \\'sequences\\', test_loader.dataset.sequences[s_ind])\\n                            velo_file = join(seq_path, \\'velodyne\\', test_loader.dataset.frames[s_ind][f_ind] + \\'.bin\\')\\n                            frame_points = np.fromfile(velo_file, dtype=np.float32)\\n                            frame_points = frame_points.reshape((-1, 4))\\n                            predpath = join(test_path, pred_folder, filename[:-4] + \\'.ply\\')\\n                            #pots = test_loader.dataset.f_potentials[s_ind][f_ind]\\n                            pots = np.zeros((0,))\\n                            if pots.shape[0] > 0:\\n                                write_ply(predpath,\\n                                          [frame_points[:, :3], frame_preds, pots],\\n                                          [\\'x\\', \\'y\\', \\'z\\', \\'pre\\', \\'pots\\'])\\n                            else:\\n                                write_ply(predpath,\\n                                          [frame_points[:, :3], frame_preds],\\n                                          [\\'x\\', \\'y\\', \\'z\\', \\'pre\\'])\\n\\n                    # Stack all prediction for this epoch\\n                    i0 += length\\n\\n                # Average timing\\n                t += [time.time()]\\n                mean_dt = 0.95 * mean_dt + 0.05 * (np.array(t[1:]) - np.array(t[:-1]))\\n\\n                # Display\\n                if (t[-1] - last_display) > 1.0:\\n                    last_display = t[-1]\\n                    message = \\'e{:03d}-i{:04d} => {:.1f}% (timings : {:4.2f} {:4.2f} {:4.2f}) / pots {:d} => {:.1f}%\\'\\n                    min_pot = int(torch.floor(torch.min(test_loader.dataset.potentials)))\\n                    pot_num = torch.sum(test_loader.dataset.potentials > min_pot + 0.5).type(torch.int32).item()\\n                    current_num = pot_num + (i + 1 - config.validation_size) * config.val_batch_num\\n                    print(message.format(test_epoch, i,\\n                                         100 * i / config.validation_size,\\n                                         1000 * (mean_dt[0]),\\n                                         1000 * (mean_dt[1]),\\n                                         1000 * (mean_dt[2]),\\n                                         min_pot,\\n                                         100.0 * current_num / len(test_loader.dataset.potentials)))\\n\\n\\n            # Update minimum od potentials\\n            new_min = torch.min(test_loader.dataset.potentials)\\n            print(\\'Test epoch {:d}, end. Min potential = {:.1f}\\'.format(test_epoch, new_min))\\n\\n            if last_min + 1 < new_min:\\n\\n                # Update last_min\\n                last_min += 1\\n\\n                if test_loader.dataset.set == \\'validation\\' and last_min % 1 == 0:\\n\\n                    #####################################\\n                    # Results on the whole validation set\\n                    #####################################\\n\\n                    # Confusions for our subparts of validation set\\n                    Confs = np.zeros((len(predictions), nc_tot, nc_tot), dtype=np.int32)\\n                    for i, (preds, truth) in enumerate(zip(predictions, targets)):\\n\\n                        # Confusions\\n                        Confs[i, :, :] = fast_confusion(truth, preds, test_loader.dataset.label_values).astype(np.int32)\\n\\n\\n                    # Show vote results\\n                    print(\\'\\\\nCompute confusion\\')\\n\\n                    val_preds = []\\n                    val_labels = []\\n                    t1 = time.time()\\n                    for i, seq_frames in enumerate(test_loader.dataset.frames):\\n                        val_preds += [np.hstack(all_f_preds[i])]\\n                        val_labels += [np.hstack(all_f_labels[i])]\\n                    val_preds = np.hstack(val_preds)\\n                    val_labels = np.hstack(val_labels)\\n                    t2 = time.time()\\n                    C_tot = fast_confusion(val_labels, val_preds, test_loader.dataset.label_values)\\n                    t3 = time.time()\\n                    print(\\' Stacking time : {:.1f}s\\'.format(t2 - t1))\\n                    print(\\'Confusion time : {:.1f}s\\'.format(t3 - t2))\\n\\n                    s1 = \\'\\\\n\\'\\n                    for cc in C_tot:\\n                        for c in cc:\\n                            s1 += \\'{:7.0f} \\'.format(c)\\n                        s1 += \\'\\\\n\\'\\n                    if debug:\\n                        print(s1)\\n\\n                    # Remove ignored labels from confusions\\n                    for l_ind, label_value in reversed(list(enumerate(test_loader.dataset.label_values))):\\n                        if label_value in test_loader.dataset.ignored_labels:\\n                            C_tot = np.delete(C_tot, l_ind, axis=0)\\n                            C_tot = np.delete(C_tot, l_ind, axis=1)\\n\\n                    # Objects IoU\\n                    val_IoUs = IoU_from_confusions(C_tot)\\n\\n                    # Compute IoUs\\n                    mIoU = np.mean(val_IoUs)\\n                    s2 = \\'{:5.2f} | \\'.format(100 * mIoU)\\n                    for IoU in val_IoUs:\\n                        s2 += \\'{:5.2f} \\'.format(100 * IoU)\\n                    print(s2 + \\'\\\\n\\')\\n\\n                    # Save a report\\n                    report_file = join(report_path, \\'report_{:04d}.txt\\'.format(int(np.floor(last_min))))\\n                    str = \\'Report of the confusion and metrics\\\\n\\'\\n                    str += \\'***********************************\\\\n\\\\n\\\\n\\'\\n                    str += \\'Confusion matrix:\\\\n\\\\n\\'\\n                    str += s1\\n                    str += \\'\\\\nIoU values:\\\\n\\\\n\\'\\n                    str += s2\\n                    str += \\'\\\\n\\\\n\\'\\n                    with open(report_file, \\'w\\') as f:\\n                        f.write(str)\\n\\n            test_epoch += 1\\n\\n            # Break when reaching number of desired votes\\n            if last_min > num_votes:\\n                break\\n\\n        return\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFile Name: config.py\\nContents of Fileconfig.py: #\\n#\\n#      0=================================0\\n#      |    Kernel Point Convolutions    |\\n#      0=================================0\\n#\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Configuration class\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Hugues THOMAS - 11/06/2018\\n#\\n\\n\\nfrom os.path import join\\nimport numpy as np\\n\\nfrom torch.utils.tensorboard import SummaryWriter\\n\\n\\n# Colors for printing\\nclass bcolors:\\n    HEADER = \\'\\\\033[95m\\'\\n    OKBLUE = \\'\\\\033[94m\\'\\n    OKGREEN = \\'\\\\033[92m\\'\\n    WARNING = \\'\\\\033[93m\\'\\n    FAIL = \\'\\\\033[91m\\'\\n    ENDC = \\'\\\\033[0m\\'\\n    BOLD = \\'\\\\033[1m\\'\\n    UNDERLINE = \\'\\\\033[4m\\'\\n\\n\\nclass Config:\\n    \"\"\"\\n    Class containing the parameters you want to modify for this dataset\\n    \"\"\"\\n\\n    ##################\\n    # Input parameters\\n    ##################\\n\\n    # Dataset name\\n    dataset = \\'\\'\\n\\n    # Type of network model\\n    dataset_task = \\'\\'\\n\\n    # Number of classes in the dataset\\n    num_classes = 0\\n\\n    # Dimension of input points\\n    in_points_dim = 3\\n\\n    # Dimension of input features\\n    in_features_dim = 1\\n\\n    # Radius of the input sphere (ignored for models, only used for point clouds)\\n    in_radius = 1.0\\n\\n    # Number of CPU threads for the input pipeline\\n    input_threads = 8\\n\\n    ##################\\n    # Model parameters\\n    ##################\\n\\n    # Architecture definition. List of blocks\\n    architecture = []\\n\\n    # Decide the mode of equivariance and invariance\\n    equivar_mode = \\'\\'\\n    invar_mode = \\'\\'\\n\\n    # Dimension of the first feature maps\\n    first_features_dim = 64\\n\\n    # Batch normalization parameters\\n    use_batch_norm = True\\n    batch_norm_momentum = 0.99\\n\\n    # For segmentation models : ratio between the segmented area and the input area\\n    segmentation_ratio = 1.0\\n\\n    ###################\\n    # KPConv parameters\\n    ###################\\n\\n    # Number of kernel points\\n    num_kernel_points = 15\\n\\n    # Size of the first subsampling grid in meter\\n    first_subsampling_dl = 0.02\\n\\n    # Radius of convolution in \"number grid cell\". (2.5 is the standard value)\\n    conv_radius = 2.5\\n\\n    # Radius of deformable convolution in \"number grid cell\". Larger so that deformed kernel can spread out\\n    deform_radius = 5.0\\n\\n    # Kernel point influence radius\\n    KP_extent = 1.0\\n\\n    # Influence function when d < KP_extent. (\\'constant\\', \\'linear\\', \\'gaussian\\') When d > KP_extent, always zero\\n    KP_influence = \\'linear\\'\\n\\n    # Aggregation function of KPConv in (\\'closest\\', \\'sum\\')\\n    # Decide if you sum all kernel point influences, or if you only take the influence of the closest KP\\n    aggregation_mode = \\'sum\\'\\n\\n    # Fixed points in the kernel : \\'none\\', \\'center\\' or \\'verticals\\'\\n    fixed_kernel_points = \\'center\\'\\n\\n    # Use modulateion in deformable convolutions\\n    modulated = False\\n\\n    # For SLAM datasets like SemanticKitti number of frames used (minimum one)\\n    n_frames = 1\\n\\n    # For SLAM datasets like SemanticKitti max number of point in input cloud + validation\\n    max_in_points = 0\\n    val_radius = 51.0\\n    max_val_points = 50000\\n\\n    #####################\\n    # Training parameters\\n    #####################\\n\\n    # Network optimizer parameters (learning rate and momentum)\\n    learning_rate = 1e-3\\n    momentum = 0.9\\n\\n    # Learning rate decays. Dictionary of all decay values with their epoch {epoch: decay}.\\n    lr_decays = {200: 0.2, 300: 0.2}\\n\\n    # Gradient clipping value (negative means no clipping)\\n    grad_clip_norm = 100.0\\n\\n    # Augmentation parameters\\n    augment_scale_anisotropic = True\\n    augment_scale_min = 0.9\\n    augment_scale_max = 1.1\\n    augment_symmetries = [False, False, False]\\n    augment_rotation = \\'vertical\\'\\n    augment_noise = 0.005\\n    augment_color = 0.7\\n\\n    # Augment with occlusions (not implemented yet)\\n    augment_occlusion = \\'none\\'\\n    augment_occlusion_ratio = 0.2\\n    augment_occlusion_num = 1\\n\\n    # Regularization loss importance\\n    weight_decay = 1e-3\\n\\n    # The way we balance segmentation loss DEPRECATED\\n    segloss_balance = \\'none\\'\\n\\n    # Choose weights for class (used in segmentation loss). Empty list for no weights\\n    class_w = []\\n\\n    # Deformable offset loss\\n    # \\'point2point\\' fitting geometry by penalizing distance from deform point to input points\\n    # \\'point2plane\\' fitting geometry by penalizing distance from deform point to input point triplet (not implemented)\\n    deform_fitting_mode = \\'point2point\\'\\n    deform_fitting_power = 1.0              # Multiplier for the fitting/repulsive loss\\n    deform_lr_factor = 0.1                  # Multiplier for learning rate applied to the deformations\\n    repulse_extent = 1.0                    # Distance of repulsion for deformed kernel points\\n\\n    # Number of batch\\n    batch_num = 10\\n    val_batch_num = 10\\n\\n    # Maximal number of epochs\\n    max_epoch = 1000\\n\\n    # Number of steps per epochs\\n    epoch_steps = 1000\\n\\n    # Number of validation examples per epoch\\n    validation_size = 100\\n\\n    # Number of epoch between each checkpoint\\n    checkpoint_gap = 50\\n\\n    # Do we nee to save convergence\\n    saving = True\\n    saving_path = None\\n\\n    path = \\'/path/to/data/root\\'\\n    writer = SummaryWriter(\\'runs/foo\\')\\n\\n    def __init__(self):\\n        \"\"\"\\n        Class Initialyser\\n        \"\"\"\\n\\n        # Number of layers\\n        self.num_layers = len([block for block in self.architecture if \\'pool\\' in block or \\'strided\\' in block]) + 1\\n\\n        ###################\\n        # Deform layer list\\n        ###################\\n        #\\n        # List of boolean indicating which layer has a deformable convolution\\n        #\\n\\n        layer_blocks = []\\n        self.deform_layers = []\\n        arch = self.architecture\\n        for block_i, block in enumerate(arch):\\n\\n            # Get all blocks of the layer\\n            if not (\\'pool\\' in block or \\'strided\\' in block or \\'global\\' in block or \\'upsample\\' in block):\\n                layer_blocks += [block]\\n                continue\\n\\n            # Convolution neighbors indices\\n            # *****************************\\n\\n            deform_layer = False\\n            if layer_blocks:\\n                if np.any([\\'deformable\\' in blck for blck in layer_blocks]):\\n                    deform_layer = True\\n\\n            if \\'pool\\' in block or \\'strided\\' in block:\\n                if \\'deformable\\' in block:\\n                    deform_layer = True\\n\\n            self.deform_layers += [deform_layer]\\n            layer_blocks = []\\n\\n            # Stop when meeting a global pooling or upsampling\\n            if \\'global\\' in block or \\'upsample\\' in block:\\n                break\\n\\n    def load(self, path):\\n\\n        filename = join(path, \\'parameters.txt\\')\\n        with open(filename, \\'r\\') as f:\\n            lines = f.readlines()\\n\\n        # Class variable dictionary\\n        for line in lines:\\n            line_info = line.split()\\n            if len(line_info) > 2 and line_info[0] != \\'#\\':\\n\\n                if line_info[2] == \\'None\\':\\n                    setattr(self, line_info[0], None)\\n\\n                elif line_info[0] == \\'lr_decay_epochs\\':\\n                    self.lr_decays = {int(b.split(\\':\\')[0]): float(b.split(\\':\\')[1]) for b in line_info[2:]}\\n\\n                elif line_info[0] == \\'architecture\\':\\n                    self.architecture = [b for b in line_info[2:]]\\n\\n                elif line_info[0] == \\'augment_symmetries\\':\\n                    self.augment_symmetries = [bool(int(b)) for b in line_info[2:]]\\n\\n                elif line_info[0] == \\'num_classes\\':\\n                    if len(line_info) > 3:\\n                        self.num_classes = [int(c) for c in line_info[2:]]\\n                    else:\\n                        self.num_classes = int(line_info[2])\\n\\n                elif line_info[0] == \\'class_w\\':\\n                    self.class_w = [float(w) for w in line_info[2:]]\\n\\n                elif hasattr(self, line_info[0]):\\n                    attr_type = type(getattr(self, line_info[0]))\\n                    if attr_type == bool:\\n                        setattr(self, line_info[0], attr_type(int(line_info[2])))\\n                    else:\\n                        setattr(self, line_info[0], attr_type(line_info[2]))\\n\\n        self.saving = True\\n        self.saving_path = path\\n        self.__init__()\\n\\n    def save(self):\\n\\n        with open(join(self.saving_path, \\'parameters.txt\\'), \"w\") as text_file:\\n\\n            text_file.write(\\'# -----------------------------------#\\\\n\\')\\n            text_file.write(\\'# Parameters of the training session #\\\\n\\')\\n            text_file.write(\\'# -----------------------------------#\\\\n\\\\n\\')\\n\\n            # Input parameters\\n            text_file.write(\\'# Input parameters\\\\n\\')\\n            text_file.write(\\'# ****************\\\\n\\\\n\\')\\n            text_file.write(\\'dataset = {:s}\\\\n\\'.format(self.dataset))\\n            text_file.write(\\'dataset_task = {:s}\\\\n\\'.format(self.dataset_task))\\n            text_file.write(\\'path = {:s}\\\\n\\'.format(self.path))\\n            text_file.write(\\'tensorboard = {:s}\\\\n\\'.format(self.writer.log_dir))\\n            if type(self.num_classes) is list:\\n                text_file.write(\\'num_classes =\\')\\n                for n in self.num_classes:\\n                    text_file.write(\\' {:d}\\'.format(n))\\n                text_file.write(\\'\\\\n\\')\\n            else:\\n                text_file.write(\\'num_classes = {:d}\\\\n\\'.format(self.num_classes))\\n            text_file.write(\\'in_points_dim = {:d}\\\\n\\'.format(self.in_points_dim))\\n            text_file.write(\\'in_features_dim = {:d}\\\\n\\'.format(self.in_features_dim))\\n            text_file.write(\\'in_radius = {:.6f}\\\\n\\'.format(self.in_radius))\\n            text_file.write(\\'input_threads = {:d}\\\\n\\\\n\\'.format(self.input_threads))\\n\\n            # Model parameters\\n            text_file.write(\\'# Model parameters\\\\n\\')\\n            text_file.write(\\'# ****************\\\\n\\\\n\\')\\n\\n            text_file.write(\\'architecture =\\')\\n            for a in self.architecture:\\n                text_file.write(\\' {:s}\\'.format(a))\\n            text_file.write(\\'\\\\n\\')\\n            text_file.write(\\'equivar_mode = {:s}\\\\n\\'.format(self.equivar_mode))\\n            text_file.write(\\'invar_mode = {:s}\\\\n\\'.format(self.invar_mode))\\n            text_file.write(\\'num_layers = {:d}\\\\n\\'.format(self.num_layers))\\n            text_file.write(\\'first_features_dim = {:d}\\\\n\\'.format(self.first_features_dim))\\n            text_file.write(\\'use_batch_norm = {:d}\\\\n\\'.format(int(self.use_batch_norm)))\\n            text_file.write(\\'batch_norm_momentum = {:.6f}\\\\n\\\\n\\'.format(self.batch_norm_momentum))\\n            text_file.write(\\'segmentation_ratio = {:.6f}\\\\n\\\\n\\'.format(self.segmentation_ratio))\\n\\n            # KPConv parameters\\n            text_file.write(\\'# KPConv parameters\\\\n\\')\\n            text_file.write(\\'# *****************\\\\n\\\\n\\')\\n\\n            text_file.write(\\'first_subsampling_dl = {:.6f}\\\\n\\'.format(self.first_subsampling_dl))\\n            text_file.write(\\'num_kernel_points = {:d}\\\\n\\'.format(self.num_kernel_points))\\n            text_file.write(\\'conv_radius = {:.6f}\\\\n\\'.format(self.conv_radius))\\n            text_file.write(\\'deform_radius = {:.6f}\\\\n\\'.format(self.deform_radius))\\n            text_file.write(\\'fixed_kernel_points = {:s}\\\\n\\'.format(self.fixed_kernel_points))\\n            text_file.write(\\'KP_extent = {:.6f}\\\\n\\'.format(self.KP_extent))\\n            text_file.write(\\'KP_influence = {:s}\\\\n\\'.format(self.KP_influence))\\n            text_file.write(\\'aggregation_mode = {:s}\\\\n\\'.format(self.aggregation_mode))\\n            text_file.write(\\'modulated = {:d}\\\\n\\'.format(int(self.modulated)))\\n            text_file.write(\\'n_frames = {:d}\\\\n\\'.format(self.n_frames))\\n            text_file.write(\\'max_in_points = {:d}\\\\n\\\\n\\'.format(self.max_in_points))\\n            text_file.write(\\'max_val_points = {:d}\\\\n\\\\n\\'.format(self.max_val_points))\\n            text_file.write(\\'val_radius = {:.6f}\\\\n\\\\n\\'.format(self.val_radius))\\n\\n            # Training parameters\\n            text_file.write(\\'# Training parameters\\\\n\\')\\n            text_file.write(\\'# *******************\\\\n\\\\n\\')\\n\\n            text_file.write(\\'learning_rate = {:f}\\\\n\\'.format(self.learning_rate))\\n            text_file.write(\\'momentum = {:f}\\\\n\\'.format(self.momentum))\\n            text_file.write(\\'lr_decay_epochs =\\')\\n            for e, d in self.lr_decays.items():\\n                text_file.write(\\' {:d}:{:f}\\'.format(e, d))\\n            text_file.write(\\'\\\\n\\')\\n            text_file.write(\\'grad_clip_norm = {:f}\\\\n\\\\n\\'.format(self.grad_clip_norm))\\n\\n\\n            text_file.write(\\'augment_symmetries =\\')\\n            for a in self.augment_symmetries:\\n                text_file.write(\\' {:d}\\'.format(int(a)))\\n            text_file.write(\\'\\\\n\\')\\n            text_file.write(\\'augment_rotation = {:s}\\\\n\\'.format(self.augment_rotation))\\n            text_file.write(\\'augment_noise = {:f}\\\\n\\'.format(self.augment_noise))\\n            text_file.write(\\'augment_occlusion = {:s}\\\\n\\'.format(self.augment_occlusion))\\n            text_file.write(\\'augment_occlusion_ratio = {:.6f}\\\\n\\'.format(self.augment_occlusion_ratio))\\n            text_file.write(\\'augment_occlusion_num = {:d}\\\\n\\'.format(self.augment_occlusion_num))\\n            text_file.write(\\'augment_scale_anisotropic = {:d}\\\\n\\'.format(int(self.augment_scale_anisotropic)))\\n            text_file.write(\\'augment_scale_min = {:.6f}\\\\n\\'.format(self.augment_scale_min))\\n            text_file.write(\\'augment_scale_max = {:.6f}\\\\n\\'.format(self.augment_scale_max))\\n            text_file.write(\\'augment_color = {:.6f}\\\\n\\\\n\\'.format(self.augment_color))\\n\\n            text_file.write(\\'weight_decay = {:f}\\\\n\\'.format(self.weight_decay))\\n            text_file.write(\\'segloss_balance = {:s}\\\\n\\'.format(self.segloss_balance))\\n            text_file.write(\\'class_w =\\')\\n            for a in self.class_w:\\n                text_file.write(\\' {:.6f}\\'.format(a))\\n            text_file.write(\\'\\\\n\\')\\n            text_file.write(\\'deform_fitting_mode = {:s}\\\\n\\'.format(self.deform_fitting_mode))\\n            text_file.write(\\'deform_fitting_power = {:.6f}\\\\n\\'.format(self.deform_fitting_power))\\n            text_file.write(\\'deform_lr_factor = {:.6f}\\\\n\\'.format(self.deform_lr_factor))\\n            text_file.write(\\'repulse_extent = {:.6f}\\\\n\\'.format(self.repulse_extent))\\n            text_file.write(\\'batch_num = {:d}\\\\n\\'.format(self.batch_num))\\n            text_file.write(\\'val_batch_num = {:d}\\\\n\\'.format(self.val_batch_num))\\n            text_file.write(\\'max_epoch = {:d}\\\\n\\'.format(self.max_epoch))\\n            if self.epoch_steps is None:\\n                text_file.write(\\'epoch_steps = None\\\\n\\')\\n            else:\\n                text_file.write(\\'epoch_steps = {:d}\\\\n\\'.format(self.epoch_steps))\\n            text_file.write(\\'validation_size = {:d}\\\\n\\'.format(self.validation_size))\\n            text_file.write(\\'checkpoint_gap = {:d}\\\\n\\'.format(self.checkpoint_gap))\\n\\n\\n\\nFile Name: jax-only-distribution.py\\nContents of Filejax-only-distribution.py: from os import listdir, path, rename\\n\\nimport json\\nimport numpy as np\\nimport pdal\\nimport random\\n\\nrootdir = \\'/home/chambbj/data/ml-datasets/US3D/oma-only\\'\\nfor filename in listdir(path.join(rootdir, \"all\")):\\n    p = pdal.Pipeline(json.dumps([path.join(rootdir, \"all\", filename)]))\\n    p.execute()\\n    pc = p.arrays[0]\\n\\n    classes = pc[\\'Classification\\']\\n    unique_classes = np.unique(classes)\\n\\n    if any(x in [7, 9, 17] for x in unique_classes):\\n        r = random.random()\\n        print(\"minority\", r)\\n        if (r < 0.2):\\n            rename(path.join(rootdir, \"all\", filename), path.join(rootdir, \"validation\", filename))\\n        else:\\n            rename(path.join(rootdir, \"all\", filename), path.join(rootdir, \"train\", filename))\\n    else:\\n        r = random.random()\\n        print(\"majority\", r)\\n        if (r < 0.2):\\n            rename(path.join(rootdir, \"all\", filename), path.join(rootdir, \"validation\", filename))\\n        else:\\n            rename(path.join(rootdir, \"all\", filename), path.join(rootdir, \"train\", filename))\\n\\nFile Name: metrics.py\\nContents of Filemetrics.py: #\\n#\\n#      0=================================0\\n#      |    Kernel Point Convolutions    |\\n#      0=================================0\\n#\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Metric utility functions\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Hugues THOMAS - 11/06/2018\\n#\\n\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Imports and global variables\\n#       \\\\**********************************/\\n#\\n\\n\\n# Basic libs\\nimport numpy as np\\n\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Utilities\\n#       \\\\***************/\\n#\\n\\ndef fast_confusion(true, pred, label_values=None):\\n    \"\"\"\\n    Fast confusion matrix (100x faster than Scikit learn). But only works if labels are la\\n    :param true:\\n    :param false:\\n    :param num_classes:\\n    :return:\\n    \"\"\"\\n\\n    # Ensure data is in the right format\\n    # print(\"Before \", true.shape)\\n    true = np.squeeze(true)\\n    # print(\"After \", true.shape)\\n    pred = np.squeeze(pred)\\n    if len(true.shape) != 1:\\n        raise ValueError(\\'Truth values are stored in a {:d}D array instead of 1D array\\'. format(len(true.shape)))\\n    if len(pred.shape) != 1:\\n        raise ValueError(\\'Prediction values are stored in a {:d}D array instead of 1D array\\'. format(len(pred.shape)))\\n    if true.dtype not in [np.int32, np.int64]:\\n        raise ValueError(\\'Truth values are {:s} instead of int32 or int64\\'.format(true.dtype))\\n    if pred.dtype not in [np.int32, np.int64]:\\n        raise ValueError(\\'Prediction values are {:s} instead of int32 or int64\\'.format(pred.dtype))\\n    true = true.astype(np.int32)\\n    pred = pred.astype(np.int32)\\n\\n    # Get the label values\\n    if label_values is None:\\n        # From data if they are not given\\n        label_values = np.unique(np.hstack((true, pred)))\\n    else:\\n        # Ensure they are good if given\\n        if label_values.dtype not in [np.int32, np.int64]:\\n            raise ValueError(\\'label values are {:s} instead of int32 or int64\\'.format(label_values.dtype))\\n        if len(np.unique(label_values)) < len(label_values):\\n            raise ValueError(\\'Given labels are not unique\\')\\n\\n    # Sort labels\\n    label_values = np.sort(label_values)\\n\\n    # Get the number of classes\\n    num_classes = len(label_values)\\n\\n    print(\"# classes {}\".format(num_classes))\\n    print(label_values)\\n    print(\"max true {}\".format(np.max(true)))\\n    print(\"max pred {}\".format(np.max(pred)))\\n    #print(np.max(true * num_classes + pred))\\n\\n    # Start confusion computations\\n    if label_values[0] == 0 and label_values[-1] == num_classes - 1:\\n\\n        # Vectorized confusion\\n        vec_conf = np.bincount(true * num_classes + pred)\\n\\n        # Add possible missing values due to classes not being in pred or true\\n        #print(vec_conf.shape)\\n        if vec_conf.shape[0] < num_classes ** 2:\\n            vec_conf = np.pad(vec_conf, (0, num_classes ** 2 - vec_conf.shape[0]), \\'constant\\')\\n        #print(vec_conf.shape)\\n\\n        # Reshape confusion in a matrix\\n        return vec_conf.reshape((num_classes, num_classes))\\n\\n\\n    else:\\n\\n        # Ensure no negative classes\\n        if label_values[0] < 0:\\n            raise ValueError(\\'Unsupported negative classes\\')\\n\\n        # Get the data in [0,num_classes[\\n        label_map = np.zeros((label_values[-1] + 1,), dtype=np.int32)\\n        for k, v in enumerate(label_values):\\n            label_map[v] = k\\n\\n        pred = label_map[pred]\\n        true = label_map[true]\\n\\n        print(\"max true {}\".format(np.max(true)))\\n        print(\"max pred {}\".format(np.max(pred)))\\n\\n        # Vectorized confusion\\n        vec_conf = np.bincount(true * num_classes + pred)\\n\\n        # Add possible missing values due to classes not being in pred or true\\n        if vec_conf.shape[0] < num_classes ** 2:\\n            vec_conf = np.pad(vec_conf, (0, num_classes ** 2 - vec_conf.shape[0]), \\'constant\\')\\n\\n        # Reshape confusion in a matrix\\n        return vec_conf.reshape((num_classes, num_classes))\\n\\ndef metrics(confusions, ignore_unclassified=False):\\n    \"\"\"\\n    Computes different metrics from confusion matrices.\\n    :param confusions: ([..., n_c, n_c] np.int32). Can be any dimension, the confusion matrices should be described by\\n    the last axes. n_c = number of classes\\n    :param ignore_unclassified: (bool). True if the the first class should be ignored in the results\\n    :return: ([..., n_c] np.float32) precision, recall, F1 score, IoU score\\n    \"\"\"\\n\\n    # If the first class (often \"unclassified\") should be ignored, erase it from the confusion.\\n    if (ignore_unclassified):\\n        confusions[..., 0, :] = 0\\n        confusions[..., :, 0] = 0\\n\\n    # Compute TP, FP, FN. This assume that the second to last axis counts the truths (like the first axis of a\\n    # confusion matrix), and that the last axis counts the predictions (like the second axis of a confusion matrix)\\n    TP = np.diagonal(confusions, axis1=-2, axis2=-1)\\n    TP_plus_FP = np.sum(confusions, axis=-1)\\n    TP_plus_FN = np.sum(confusions, axis=-2)\\n\\n    # Compute precision and recall. This assume that the second to last axis counts the truths (like the first axis of\\n    # a confusion matrix), and that the last axis counts the predictions (like the second axis of a confusion matrix)\\n    PRE = TP / (TP_plus_FN + 1e-6)\\n    REC = TP / (TP_plus_FP + 1e-6)\\n\\n    # Compute Accuracy\\n    ACC = np.sum(TP, axis=-1) / (np.sum(confusions, axis=(-2, -1)) + 1e-6)\\n\\n    # Compute F1 score\\n    F1 = 2 * TP / (TP_plus_FP + TP_plus_FN + 1e-6)\\n\\n    # Compute IoU\\n    IoU = F1 / (2 - F1)\\n\\n    return PRE, REC, F1, IoU, ACC\\n\\n\\ndef smooth_metrics(confusions, smooth_n=0, ignore_unclassified=False):\\n    \"\"\"\\n    Computes different metrics from confusion matrices. Smoothed over a number of epochs.\\n    :param confusions: ([..., n_c, n_c] np.int32). Can be any dimension, the confusion matrices should be described by\\n    the last axes. n_c = number of classes\\n    :param smooth_n: (int). smooth extent\\n    :param ignore_unclassified: (bool). True if the the first class should be ignored in the results\\n    :return: ([..., n_c] np.float32) precision, recall, F1 score, IoU score\\n    \"\"\"\\n\\n    # If the first class (often \"unclassified\") should be ignored, erase it from the confusion.\\n    if ignore_unclassified:\\n        confusions[..., 0, :] = 0\\n        confusions[..., :, 0] = 0\\n\\n    # Sum successive confusions for smoothing\\n    smoothed_confusions = confusions.copy()\\n    if confusions.ndim > 2 and smooth_n > 0:\\n        for epoch in range(confusions.shape[-3]):\\n            i0 = max(epoch - smooth_n, 0)\\n            i1 = min(epoch + smooth_n + 1, confusions.shape[-3])\\n            smoothed_confusions[..., epoch, :, :] = np.sum(confusions[..., i0:i1, :, :], axis=-3)\\n\\n    # Compute TP, FP, FN. This assume that the second to last axis counts the truths (like the first axis of a\\n    # confusion matrix), and that the last axis counts the predictions (like the second axis of a confusion matrix)\\n    TP = np.diagonal(smoothed_confusions, axis1=-2, axis2=-1)\\n    TP_plus_FP = np.sum(smoothed_confusions, axis=-2)\\n    TP_plus_FN = np.sum(smoothed_confusions, axis=-1)\\n\\n    # Compute precision and recall. This assume that the second to last axis counts the truths (like the first axis of\\n    # a confusion matrix), and that the last axis counts the predictions (like the second axis of a confusion matrix)\\n    PRE = TP / (TP_plus_FN + 1e-6)\\n    REC = TP / (TP_plus_FP + 1e-6)\\n\\n    # Compute Accuracy\\n    ACC = np.sum(TP, axis=-1) / (np.sum(smoothed_confusions, axis=(-2, -1)) + 1e-6)\\n\\n    # Compute F1 score\\n    F1 = 2 * TP / (TP_plus_FP + TP_plus_FN + 1e-6)\\n\\n    # Compute IoU\\n    IoU = F1 / (2 - F1)\\n\\n    return PRE, REC, F1, IoU, ACC\\n\\n\\ndef IoU_from_confusions(confusions):\\n    \"\"\"\\n    Computes IoU from confusion matrices.\\n    :param confusions: ([..., n_c, n_c] np.int32). Can be any dimension, the confusion matrices should be described by\\n    the last axes. n_c = number of classes\\n    :param ignore_unclassified: (bool). True if the the first class should be ignored in the results\\n    :return: ([..., n_c] np.float32) IoU score\\n    \"\"\"\\n\\n    # Compute TP, FP, FN. This assume that the second to last axis counts the truths (like the first axis of a\\n    # confusion matrix), and that the last axis counts the predictions (like the second axis of a confusion matrix)\\n    TP = np.diagonal(confusions, axis1=-2, axis2=-1)\\n    TP_plus_FN = np.sum(confusions, axis=-1)\\n    TP_plus_FP = np.sum(confusions, axis=-2)\\n\\n    # Compute IoU\\n    IoU = TP / (TP_plus_FP + TP_plus_FN - TP + 1e-6)\\n\\n    # # Compute mIoU with only the actual classes\\n    # mask = TP_plus_FN < 1e-3\\n    # counts = np.sum(1 - mask, axis=-1, keepdims=True)\\n    # mIoU = np.sum(IoU, axis=-1, keepdims=True) / (counts + 1e-6)\\n\\n    # # If class is absent, place mIoU in place of 0 IoU to get the actual mean later\\n    # IoU += mask * mIoU\\n\\n    return IoU\\n\\n\\nFile Name: las.py\\nContents of Filelas.py: import json\\nimport numpy as np\\nimport pdal\\n\\ndef read_las_points(filename):\\n    p = pdal.Pipeline(json.dumps([filename]))\\n    p.validate()\\n    p.execute()\\n    data = p.arrays[0]\\n    points = np.vstack((data[\\'X\\'], data[\\'Y\\'], data[\\'Z\\'])).T\\n    return points\\n\\ndef read_processed_las(filename):\\n    p = pdal.Pipeline(json.dumps([filename]))\\n    p.validate()\\n    p.execute()\\n    data = p.arrays[0]\\n    points = np.vstack((data[\\'X\\'], data[\\'Y\\'], data[\\'Z\\'])).T\\n    # intensity = np.expand_dims(data[\\'Intensity\\'], 1).astype(np.float32)\\n    # intensity = np.minimum(intensity, 255.0)/255.0\\n    # features = intensity\\n    # rn = data[\\'ReturnNumber\\'].astype(np.float32)\\n    # nr = data[\\'NumberOfReturns\\'].astype(np.float32)\\n    features = np.vstack((data[\\'Linearity\\'],data[\\'Planarity\\'],data[\\'Scattering\\'],data[\\'Verticality\\'])).T\\n    # features = np.vstack((data[\\'Eigenvalue0\\'],data[\\'Eigenvalue1\\'],data[\\'Eigenvalue2\\'])).T\\n    # features = np.vstack((data[\\'X\\'], data[\\'Y\\'], data[\\'Z\\'])).T\\n    labels = data[\\'Classification\\']\\n    return points, features, labels\\n\\ndef read_raw_las(filename):\\n    p = pdal.Pipeline(json.dumps([\\n        # filename\\n        filename,\\n        {\\n            \"type\":\"filters.range\",\\n            \"limits\":\"Classification(:17]\"\\n        },\\n        # {\\n        #     \"type\":\"filters.assign\",\\n        #     \"assignment\":\"Classification[1:1]=0\"\\n        # },\\n        # {\\n        #     \"type\":\"filters.assign\",\\n        #     \"assignment\":\"Classification[2:2]=1\"\\n        # },\\n        # {\\n        #     \"type\":\"filters.assign\",\\n        #     \"assignment\":\"Classification[7:7]=2\"\\n        # },\\n        # {\\n        #     \"type\":\"filters.assign\",\\n        #     \"assignment\":\"Classification[9:9]=3\"\\n        # },\\n        # {\\n        #     \"type\":\"filters.assign\",\\n        #     \"assignment\":\"Classification[17:17]=4\"\\n        # },\\n        {\\n            \"type\":\"filters.covariancefeatures\"\\n        }\\n        # {\\n        #     \"type\":\"filters.eigenvalues\",\\n        #     \"knn\":10\\n        # }\\n    ]))\\n    p.validate()\\n    p.execute()\\n    data = p.arrays[0]\\n    points = np.vstack((data[\\'X\\'], data[\\'Y\\'], data[\\'Z\\'])).T\\n    # intensity = np.expand_dims(data[\\'Intensity\\'], 1).astype(np.float32)\\n    # intensity = np.minimum(intensity, 255.0)/255.0\\n    # features = intensity\\n    # rn = data[\\'ReturnNumber\\'].astype(np.float32)\\n    # nr = data[\\'NumberOfReturns\\'].astype(np.float32)\\n    features = np.vstack((data[\\'Linearity\\'],data[\\'Planarity\\'],data[\\'Scattering\\'],data[\\'Verticality\\'])).T\\n    # features = np.vstack((data[\\'Eigenvalue0\\'],data[\\'Eigenvalue1\\'],data[\\'Eigenvalue2\\'])).T\\n    # features = np.vstack((data[\\'X\\'], data[\\'Y\\'], data[\\'Z\\'])).T\\n    labels = data[\\'Classification\\']\\n    return points, features, labels\\n\\ndef read_subsampled_las(filename, dl):\\n    p = pdal.Pipeline(json.dumps([\\n        # filename\\n        filename,\\n        {\\n            \"type\":\"filters.range\",\\n            \"limits\":\"Classification(:17]\"\\n        },\\n        # {\\n        #     \"type\":\"filters.assign\",\\n        #     \"assignment\":\"Classification[1:1]=0\"\\n        # },\\n        # {\\n        #     \"type\":\"filters.assign\",\\n        #     \"assignment\":\"Classification[2:2]=1\"\\n        # },\\n        # {\\n        #     \"type\":\"filters.assign\",\\n        #     \"assignment\":\"Classification[7:7]=2\"\\n        # },\\n        # {\\n        #     \"type\":\"filters.assign\",\\n        #     \"assignment\":\"Classification[9:9]=3\"\\n        # },\\n        # {\\n        #     \"type\":\"filters.assign\",\\n        #     \"assignment\":\"Classification[17:17]=4\"\\n        # },\\n        {\\n            \"type\":\"filters.covariancefeatures\"\\n        },\\n        {\\n            \"type\":\"filters.sample\",\\n            \"radius\":dl\\n        }\\n        # {\\n        #     \"type\":\"filters.eigenvalues\",\\n        #     \"knn\":10\\n        # }\\n    ]))\\n    p.validate()\\n    p.execute()\\n    data = p.arrays[0]\\n    points = np.vstack((data[\\'X\\'], data[\\'Y\\'], data[\\'Z\\'])).T\\n    # intensity = np.expand_dims(data[\\'Intensity\\'], 1).astype(np.float32)\\n    # intensity = np.minimum(intensity, 255.0)/255.0\\n    # features = intensity\\n    # rn = data[\\'ReturnNumber\\'].astype(np.float32)\\n    # nr = data[\\'NumberOfReturns\\'].astype(np.float32)\\n    features = np.vstack((data[\\'Linearity\\'],data[\\'Planarity\\'],data[\\'Scattering\\'],data[\\'Verticality\\'])).T\\n    # features = np.vstack((data[\\'Eigenvalue0\\'],data[\\'Eigenvalue1\\'],data[\\'Eigenvalue2\\'])).T\\n    # features = np.vstack((data[\\'X\\'], data[\\'Y\\'], data[\\'Z\\'])).T\\n    labels = data[\\'Classification\\']\\n    return points, features, labels\\n\\ndef write_las(filename, array):\\n    # merge the fields then\\n    p = pdal.Pipeline(json.dumps([{\\n        \"type\":\"writers.las\",\\n        \"filename\":filename,\\n        # \"offset_x\":\"auto\",\\n        # \"offset_y\":\"auto\",\\n        # \"offset_z\":\"auto\",\\n        # \"scale_x\":0.01,\\n        # \"scale_y\":0.01,\\n        # \"scale_z\":0.01\\n        \"forward\":\"all\",\\n        \"minor_version\":4,\\n        \"extra_dims\":\"all\"\\n        }]), [array])\\n    p.validate()\\n    p.execute()\\n    return True\\n\\n***Folder: cpp_wrappers\\nFile Name: compile_wrappers.sh\\nContents of Filecompile_wrappers.sh: #!/bin/bash\\n\\n# Compile cpp subsampling\\ncd cpp_subsampling\\npython3 setup.py build_ext --inplace\\ncd ..\\n\\n# Compile cpp neighbors\\ncd cpp_neighbors\\npython3 setup.py build_ext --inplace\\ncd ..\\n\\n***Folder: cpp_neighbors\\nFile Name: build.bat\\nContents of Filebuild.bat: @echo off\\npy setup.py build_ext --inplace\\n\\n\\npause\\n\\nFile Name: setup.py\\nContents of Filesetup.py: from distutils.core import setup, Extension\\nimport numpy.distutils.misc_util\\n\\n# Adding OpenCV to project\\n# ************************\\n\\n# Adding sources of the project\\n# *****************************\\n\\nSOURCES = [\"../cpp_utils/cloud/cloud.cpp\",\\n             \"neighbors/neighbors.cpp\",\\n             \"wrapper.cpp\"]\\n\\nmodule = Extension(name=\"radius_neighbors\",\\n                    sources=SOURCES,\\n                    extra_compile_args=[\\'-std=c++11\\',\\n                                        \\'-D_GLIBCXX_USE_CXX11_ABI=0\\'])\\n\\n\\nsetup(ext_modules=[module], include_dirs=numpy.distutils.misc_util.get_numpy_include_dirs())\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFile Name: wrapper.cpp\\nContents of Filewrapper.cpp: #include <Python.h>\\n#include <numpy/arrayobject.h>\\n#include \"neighbors/neighbors.h\"\\n#include <string>\\n\\n\\n\\n// docstrings for our module\\n// *************************\\n\\nstatic char module_docstring[] = \"This module provides two methods to compute radius neighbors from pointclouds or batch of pointclouds\";\\n\\nstatic char batch_query_docstring[] = \"Method to get radius neighbors in a batch of stacked pointclouds\";\\n\\n\\n// Declare the functions\\n// *********************\\n\\nstatic PyObject *batch_neighbors(PyObject *self, PyObject *args, PyObject *keywds);\\n\\n\\n// Specify the members of the module\\n// *********************************\\n\\nstatic PyMethodDef module_methods[] = \\n{\\n\\t{ \"batch_query\", (PyCFunction)batch_neighbors, METH_VARARGS | METH_KEYWORDS, batch_query_docstring },\\n\\t{NULL, NULL, 0, NULL}\\n};\\n\\n\\n// Initialize the module\\n// *********************\\n\\nstatic struct PyModuleDef moduledef = \\n{\\n    PyModuleDef_HEAD_INIT,\\n    \"radius_neighbors\",\\t\\t// m_name\\n    module_docstring,       // m_doc\\n    -1,                     // m_size\\n    module_methods,         // m_methods\\n    NULL,                   // m_reload\\n    NULL,                   // m_traverse\\n    NULL,                   // m_clear\\n    NULL,                   // m_free\\n};\\n\\nPyMODINIT_FUNC PyInit_radius_neighbors(void)\\n{\\n    import_array();\\n\\treturn PyModule_Create(&moduledef);\\n}\\n\\n\\n// Definition of the batch_subsample method\\n// **********************************\\n\\nstatic PyObject* batch_neighbors(PyObject* self, PyObject* args, PyObject* keywds)\\n{\\n\\n\\t// Manage inputs\\n\\t// *************\\n\\n\\t// Args containers\\n\\tPyObject* queries_obj = NULL;\\n\\tPyObject* supports_obj = NULL;\\n\\tPyObject* q_batches_obj = NULL;\\n\\tPyObject* s_batches_obj = NULL;\\n\\n\\t// Keywords containers\\n\\tstatic char* kwlist[] = { \"queries\", \"supports\", \"q_batches\", \"s_batches\", \"radius\", NULL };\\n\\tfloat radius = 0.1;\\n\\n\\t// Parse the input  \\n\\tif (!PyArg_ParseTupleAndKeywords(args, keywds, \"OOOO|$f\", kwlist, &queries_obj, &supports_obj, &q_batches_obj, &s_batches_obj, &radius))\\n\\t{\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Error parsing arguments\");\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\n\\t// Interpret the input objects as numpy arrays.\\n\\tPyObject* queries_array = PyArray_FROM_OTF(queries_obj, NPY_FLOAT, NPY_IN_ARRAY);\\n\\tPyObject* supports_array = PyArray_FROM_OTF(supports_obj, NPY_FLOAT, NPY_IN_ARRAY);\\n\\tPyObject* q_batches_array = PyArray_FROM_OTF(q_batches_obj, NPY_INT, NPY_IN_ARRAY);\\n\\tPyObject* s_batches_array = PyArray_FROM_OTF(s_batches_obj, NPY_INT, NPY_IN_ARRAY);\\n\\n\\t// Verify data was load correctly.\\n\\tif (queries_array == NULL)\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Error converting query points to numpy arrays of type float32\");\\n\\t\\treturn NULL;\\n\\t}\\n\\tif (supports_array == NULL)\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Error converting support points to numpy arrays of type float32\");\\n\\t\\treturn NULL;\\n\\t}\\n\\tif (q_batches_array == NULL)\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Error converting query batches to numpy arrays of type int32\");\\n\\t\\treturn NULL;\\n\\t}\\n\\tif (s_batches_array == NULL)\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Error converting support batches to numpy arrays of type int32\");\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\t// Check that the input array respect the dims\\n\\tif ((int)PyArray_NDIM(queries_array) != 2 || (int)PyArray_DIM(queries_array, 1) != 3)\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Wrong dimensions : query.shape is not (N, 3)\");\\n\\t\\treturn NULL;\\n\\t}\\n\\tif ((int)PyArray_NDIM(supports_array) != 2 || (int)PyArray_DIM(supports_array, 1) != 3)\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Wrong dimensions : support.shape is not (N, 3)\");\\n\\t\\treturn NULL;\\n\\t}\\n\\tif ((int)PyArray_NDIM(q_batches_array) > 1)\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Wrong dimensions : queries_batches.shape is not (B,) \");\\n\\t\\treturn NULL;\\n\\t}\\n\\tif ((int)PyArray_NDIM(s_batches_array) > 1)\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Wrong dimensions : supports_batches.shape is not (B,) \");\\n\\t\\treturn NULL;\\n\\t}\\n\\tif ((int)PyArray_DIM(q_batches_array, 0) != (int)PyArray_DIM(s_batches_array, 0))\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Wrong number of batch elements: different for queries and supports \");\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\t// Number of points\\n\\tint Nq = (int)PyArray_DIM(queries_array, 0);\\n\\tint Ns= (int)PyArray_DIM(supports_array, 0);\\n\\n\\t// Number of batches\\n\\tint Nb = (int)PyArray_DIM(q_batches_array, 0);\\n\\n\\t// Call the C++ function\\n\\t// *********************\\n\\n\\t// Convert PyArray to Cloud C++ class\\n\\tvector<PointXYZ> queries;\\n\\tvector<PointXYZ> supports;\\n\\tvector<int> q_batches;\\n\\tvector<int> s_batches;\\n\\tqueries = vector<PointXYZ>((PointXYZ*)PyArray_DATA(queries_array), (PointXYZ*)PyArray_DATA(queries_array) + Nq);\\n\\tsupports = vector<PointXYZ>((PointXYZ*)PyArray_DATA(supports_array), (PointXYZ*)PyArray_DATA(supports_array) + Ns);\\n\\tq_batches = vector<int>((int*)PyArray_DATA(q_batches_array), (int*)PyArray_DATA(q_batches_array) + Nb);\\n\\ts_batches = vector<int>((int*)PyArray_DATA(s_batches_array), (int*)PyArray_DATA(s_batches_array) + Nb);\\n\\n\\t// Create result containers\\n\\tvector<int> neighbors_indices;\\n\\n\\t// Compute results\\n\\t//batch_ordered_neighbors(queries, supports, q_batches, s_batches, neighbors_indices, radius);\\n\\tbatch_nanoflann_neighbors(queries, supports, q_batches, s_batches, neighbors_indices, radius);\\n\\n\\t// Check result\\n\\tif (neighbors_indices.size() < 1)\\n\\t{\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Error\");\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\t// Manage outputs\\n\\t// **************\\n\\n\\t// Maximal number of neighbors\\n\\tint max_neighbors = neighbors_indices.size() / Nq;\\n\\n\\t// Dimension of output containers\\n\\tnpy_intp* neighbors_dims = new npy_intp[2];\\n\\tneighbors_dims[0] = Nq;\\n\\tneighbors_dims[1] = max_neighbors;\\n\\n\\t// Create output array\\n\\tPyObject* res_obj = PyArray_SimpleNew(2, neighbors_dims, NPY_INT);\\n\\tPyObject* ret = NULL;\\n\\n\\t// Fill output array with values\\n\\tsize_t size_in_bytes = Nq * max_neighbors * sizeof(int);\\n\\tmemcpy(PyArray_DATA(res_obj), neighbors_indices.data(), size_in_bytes);\\n\\n\\t// Merge results\\n\\tret = Py_BuildValue(\"N\", res_obj);\\n\\n\\t// Clean up\\n\\t// ********\\n\\n\\tPy_XDECREF(queries_array);\\n\\tPy_XDECREF(supports_array);\\n\\tPy_XDECREF(q_batches_array);\\n\\tPy_XDECREF(s_batches_array);\\n\\n\\treturn ret;\\n}\\n\\n\\n***Folder: neighbors\\nFile Name: neighbors.cpp\\nContents of Fileneighbors.cpp: \\n#include \"neighbors.h\"\\n\\n\\nvoid brute_neighbors(vector<PointXYZ>& queries, vector<PointXYZ>& supports, vector<int>& neighbors_indices, float radius, int verbose)\\n{\\n\\n\\t// Initialize variables\\n\\t// ******************\\n\\n\\t// square radius\\n\\tfloat r2 = radius * radius;\\n\\n\\t// indices\\n\\tint i0 = 0;\\n\\n\\t// Counting vector\\n\\tint max_count = 0;\\n\\tvector<vector<int>> tmp(queries.size());\\n\\n\\t// Search neigbors indices\\n\\t// ***********************\\n\\n\\tfor (auto& p0 : queries)\\n\\t{\\n\\t\\tint i = 0;\\n\\t\\tfor (auto& p : supports)\\n\\t\\t{\\n\\t\\t\\tif ((p0 - p).sq_norm() < r2)\\n\\t\\t\\t{\\n\\t\\t\\t\\ttmp[i0].push_back(i);\\n\\t\\t\\t\\tif (tmp[i0].size() > max_count)\\n\\t\\t\\t\\t\\tmax_count = tmp[i0].size();\\n\\t\\t\\t}\\n\\t\\t\\ti++;\\n\\t\\t}\\n\\t\\ti0++;\\n\\t}\\n\\n\\t// Reserve the memory\\n\\tneighbors_indices.resize(queries.size() * max_count);\\n\\ti0 = 0;\\n\\tfor (auto& inds : tmp)\\n\\t{\\n\\t\\tfor (int j = 0; j < max_count; j++)\\n\\t\\t{\\n\\t\\t\\tif (j < inds.size())\\n\\t\\t\\t\\tneighbors_indices[i0 * max_count + j] = inds[j];\\n\\t\\t\\telse\\n\\t\\t\\t\\tneighbors_indices[i0 * max_count + j] = -1;\\n\\t\\t}\\n\\t\\ti0++;\\n\\t}\\n\\n\\treturn;\\n}\\n\\nvoid ordered_neighbors(vector<PointXYZ>& queries,\\n                        vector<PointXYZ>& supports,\\n                        vector<int>& neighbors_indices,\\n                        float radius)\\n{\\n\\n\\t// Initialize variables\\n\\t// ******************\\n\\n\\t// square radius\\n\\tfloat r2 = radius * radius;\\n\\n\\t// indices\\n\\tint i0 = 0;\\n\\n\\t// Counting vector\\n\\tint max_count = 0;\\n\\tfloat d2;\\n\\tvector<vector<int>> tmp(queries.size());\\n\\tvector<vector<float>> dists(queries.size());\\n\\n\\t// Search neigbors indices\\n\\t// ***********************\\n\\n\\tfor (auto& p0 : queries)\\n\\t{\\n\\t\\tint i = 0;\\n\\t\\tfor (auto& p : supports)\\n\\t\\t{\\n\\t\\t    d2 = (p0 - p).sq_norm();\\n\\t\\t\\tif (d2 < r2)\\n\\t\\t\\t{\\n\\t\\t\\t    // Find order of the new point\\n\\t\\t\\t    auto it = std::upper_bound(dists[i0].begin(), dists[i0].end(), d2);\\n\\t\\t\\t    int index = std::distance(dists[i0].begin(), it);\\n\\n\\t\\t\\t    // Insert element\\n                dists[i0].insert(it, d2);\\n                tmp[i0].insert(tmp[i0].begin() + index, i);\\n\\n\\t\\t\\t    // Update max count\\n\\t\\t\\t\\tif (tmp[i0].size() > max_count)\\n\\t\\t\\t\\t\\tmax_count = tmp[i0].size();\\n\\t\\t\\t}\\n\\t\\t\\ti++;\\n\\t\\t}\\n\\t\\ti0++;\\n\\t}\\n\\n\\t// Reserve the memory\\n\\tneighbors_indices.resize(queries.size() * max_count);\\n\\ti0 = 0;\\n\\tfor (auto& inds : tmp)\\n\\t{\\n\\t\\tfor (int j = 0; j < max_count; j++)\\n\\t\\t{\\n\\t\\t\\tif (j < inds.size())\\n\\t\\t\\t\\tneighbors_indices[i0 * max_count + j] = inds[j];\\n\\t\\t\\telse\\n\\t\\t\\t\\tneighbors_indices[i0 * max_count + j] = -1;\\n\\t\\t}\\n\\t\\ti0++;\\n\\t}\\n\\n\\treturn;\\n}\\n\\nvoid batch_ordered_neighbors(vector<PointXYZ>& queries,\\n                                vector<PointXYZ>& supports,\\n                                vector<int>& q_batches,\\n                                vector<int>& s_batches,\\n                                vector<int>& neighbors_indices,\\n                                float radius)\\n{\\n\\n\\t// Initialize variables\\n\\t// ******************\\n\\n\\t// square radius\\n\\tfloat r2 = radius * radius;\\n\\n\\t// indices\\n\\tint i0 = 0;\\n\\n\\t// Counting vector\\n\\tint max_count = 0;\\n\\tfloat d2;\\n\\tvector<vector<int>> tmp(queries.size());\\n\\tvector<vector<float>> dists(queries.size());\\n\\n\\t// batch index\\n\\tint b = 0;\\n\\tint sum_qb = 0;\\n\\tint sum_sb = 0;\\n\\n\\n\\t// Search neigbors indices\\n\\t// ***********************\\n\\n\\tfor (auto& p0 : queries)\\n\\t{\\n\\t    // Check if we changed batch\\n\\t    if (i0 == sum_qb + q_batches[b])\\n\\t    {\\n\\t        sum_qb += q_batches[b];\\n\\t        sum_sb += s_batches[b];\\n\\t        b++;\\n\\t    }\\n\\n\\t    // Loop only over the supports of current batch\\n\\t    vector<PointXYZ>::iterator p_it;\\n\\t\\tint i = 0;\\n        for(p_it = supports.begin() + sum_sb; p_it < supports.begin() + sum_sb + s_batches[b]; p_it++ )\\n        {\\n\\t\\t    d2 = (p0 - *p_it).sq_norm();\\n\\t\\t\\tif (d2 < r2)\\n\\t\\t\\t{\\n\\t\\t\\t    // Find order of the new point\\n\\t\\t\\t    auto it = std::upper_bound(dists[i0].begin(), dists[i0].end(), d2);\\n\\t\\t\\t    int index = std::distance(dists[i0].begin(), it);\\n\\n\\t\\t\\t    // Insert element\\n                dists[i0].insert(it, d2);\\n                tmp[i0].insert(tmp[i0].begin() + index, sum_sb + i);\\n\\n\\t\\t\\t    // Update max count\\n\\t\\t\\t\\tif (tmp[i0].size() > max_count)\\n\\t\\t\\t\\t\\tmax_count = tmp[i0].size();\\n\\t\\t\\t}\\n\\t\\t\\ti++;\\n\\t\\t}\\n\\t\\ti0++;\\n\\t}\\n\\n\\t// Reserve the memory\\n\\tneighbors_indices.resize(queries.size() * max_count);\\n\\ti0 = 0;\\n\\tfor (auto& inds : tmp)\\n\\t{\\n\\t\\tfor (int j = 0; j < max_count; j++)\\n\\t\\t{\\n\\t\\t\\tif (j < inds.size())\\n\\t\\t\\t\\tneighbors_indices[i0 * max_count + j] = inds[j];\\n\\t\\t\\telse\\n\\t\\t\\t\\tneighbors_indices[i0 * max_count + j] = supports.size();\\n\\t\\t}\\n\\t\\ti0++;\\n\\t}\\n\\n\\treturn;\\n}\\n\\n\\nvoid batch_nanoflann_neighbors(vector<PointXYZ>& queries,\\n                                vector<PointXYZ>& supports,\\n                                vector<int>& q_batches,\\n                                vector<int>& s_batches,\\n                                vector<int>& neighbors_indices,\\n                                float radius)\\n{\\n\\n\\t// Initialize variables\\n\\t// ******************\\n\\n\\t// indices\\n\\tint i0 = 0;\\n\\n\\t// Square radius\\n\\tfloat r2 = radius * radius;\\n\\n\\t// Counting vector\\n\\tint max_count = 0;\\n\\tfloat d2;\\n\\tvector<vector<pair<size_t, float>>> all_inds_dists(queries.size());\\n\\n\\t// batch index\\n\\tint b = 0;\\n\\tint sum_qb = 0;\\n\\tint sum_sb = 0;\\n\\n\\t// Nanoflann related variables\\n\\t// ***************************\\n\\n\\t// CLoud variable\\n\\tPointCloud current_cloud;\\n\\n\\t// Tree parameters\\n\\tnanoflann::KDTreeSingleIndexAdaptorParams tree_params(10 /* max leaf */);\\n\\n\\t// KDTree type definition\\n    typedef nanoflann::KDTreeSingleIndexAdaptor< nanoflann::L2_Simple_Adaptor<float, PointCloud > ,\\n                                                        PointCloud,\\n                                                        3 > my_kd_tree_t;\\n\\n    // Pointer to trees\\n    my_kd_tree_t* index;\\n\\n    // Build KDTree for the first batch element\\n    current_cloud.pts = vector<PointXYZ>(supports.begin() + sum_sb, supports.begin() + sum_sb + s_batches[b]);\\n    index = new my_kd_tree_t(3, current_cloud, tree_params);\\n    index->buildIndex();\\n\\n\\n\\t// Search neigbors indices\\n\\t// ***********************\\n\\n    // Search params\\n    nanoflann::SearchParams search_params;\\n    search_params.sorted = true;\\n\\n\\tfor (auto& p0 : queries)\\n\\t{\\n\\n\\t    // Check if we changed batch\\n\\t    if (i0 == sum_qb + q_batches[b])\\n\\t    {\\n\\t        sum_qb += q_batches[b];\\n\\t        sum_sb += s_batches[b];\\n\\t        b++;\\n\\n\\t        // Change the points\\n\\t        current_cloud.pts.clear();\\n            current_cloud.pts = vector<PointXYZ>(supports.begin() + sum_sb, supports.begin() + sum_sb + s_batches[b]);\\n\\n\\t        // Build KDTree of the current element of the batch\\n            delete index;\\n            index = new my_kd_tree_t(3, current_cloud, tree_params);\\n            index->buildIndex();\\n\\t    }\\n\\n\\t    // Initial guess of neighbors size\\n        all_inds_dists[i0].reserve(max_count);\\n\\n\\t    // Find neighbors\\n\\t    float query_pt[3] = { p0.x, p0.y, p0.z};\\n\\t\\tsize_t nMatches = index->radiusSearch(query_pt, r2, all_inds_dists[i0], search_params);\\n\\n        // Update max count\\n        if (nMatches > max_count)\\n            max_count = nMatches;\\n\\n        // Increment query idx\\n\\t\\ti0++;\\n\\t}\\n\\n\\t// Reserve the memory\\n\\tneighbors_indices.resize(queries.size() * max_count);\\n\\ti0 = 0;\\n\\tsum_sb = 0;\\n\\tsum_qb = 0;\\n\\tb = 0;\\n\\tfor (auto& inds_dists : all_inds_dists)\\n\\t{\\n\\t    // Check if we changed batch\\n\\t    if (i0 == sum_qb + q_batches[b])\\n\\t    {\\n\\t        sum_qb += q_batches[b];\\n\\t        sum_sb += s_batches[b];\\n\\t        b++;\\n\\t    }\\n\\n\\t\\tfor (int j = 0; j < max_count; j++)\\n\\t\\t{\\n\\t\\t\\tif (j < inds_dists.size())\\n\\t\\t\\t\\tneighbors_indices[i0 * max_count + j] = inds_dists[j].first + sum_sb;\\n\\t\\t\\telse\\n\\t\\t\\t\\tneighbors_indices[i0 * max_count + j] = supports.size();\\n\\t\\t}\\n\\t\\ti0++;\\n\\t}\\n\\n\\tdelete index;\\n\\n\\treturn;\\n}\\n\\n\\n\\nFile Name: neighbors.h\\nContents of Fileneighbors.h: \\n\\n#include \"../../cpp_utils/cloud/cloud.h\"\\n#include \"../../cpp_utils/nanoflann/nanoflann.hpp\"\\n\\n#include <set>\\n#include <cstdint>\\n\\nusing namespace std;\\n\\n\\nvoid ordered_neighbors(vector<PointXYZ>& queries,\\n                        vector<PointXYZ>& supports,\\n                        vector<int>& neighbors_indices,\\n                        float radius);\\n\\nvoid batch_ordered_neighbors(vector<PointXYZ>& queries,\\n                                vector<PointXYZ>& supports,\\n                                vector<int>& q_batches,\\n                                vector<int>& s_batches,\\n                                vector<int>& neighbors_indices,\\n                                float radius);\\n\\nvoid batch_nanoflann_neighbors(vector<PointXYZ>& queries,\\n                                vector<PointXYZ>& supports,\\n                                vector<int>& q_batches,\\n                                vector<int>& s_batches,\\n                                vector<int>& neighbors_indices,\\n                                float radius);\\n\\n\\n***Folder: cpp_subsampling\\nFile Name: build.bat\\nContents of Filebuild.bat: @echo off\\npy setup.py build_ext --inplace\\n\\n\\npause\\n\\nFile Name: setup.py\\nContents of Filesetup.py: from distutils.core import setup, Extension\\nimport numpy.distutils.misc_util\\n\\n# Adding OpenCV to project\\n# ************************\\n\\n# Adding sources of the project\\n# *****************************\\n\\nSOURCES = [\"../cpp_utils/cloud/cloud.cpp\",\\n             \"neighbors/neighbors.cpp\",\\n             \"wrapper.cpp\"]\\n\\nmodule = Extension(name=\"radius_neighbors\",\\n                    sources=SOURCES,\\n                    extra_compile_args=[\\'-std=c++11\\',\\n                                        \\'-D_GLIBCXX_USE_CXX11_ABI=0\\'])\\n\\n\\nsetup(ext_modules=[module], include_dirs=numpy.distutils.misc_util.get_numpy_include_dirs())\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFile Name: wrapper.cpp\\nContents of Filewrapper.cpp: #include <Python.h>\\n#include <numpy/arrayobject.h>\\n#include \"neighbors/neighbors.h\"\\n#include <string>\\n\\n\\n\\n// docstrings for our module\\n// *************************\\n\\nstatic char module_docstring[] = \"This module provides two methods to compute radius neighbors from pointclouds or batch of pointclouds\";\\n\\nstatic char batch_query_docstring[] = \"Method to get radius neighbors in a batch of stacked pointclouds\";\\n\\n\\n// Declare the functions\\n// *********************\\n\\nstatic PyObject *batch_neighbors(PyObject *self, PyObject *args, PyObject *keywds);\\n\\n\\n// Specify the members of the module\\n// *********************************\\n\\nstatic PyMethodDef module_methods[] = \\n{\\n\\t{ \"batch_query\", (PyCFunction)batch_neighbors, METH_VARARGS | METH_KEYWORDS, batch_query_docstring },\\n\\t{NULL, NULL, 0, NULL}\\n};\\n\\n\\n// Initialize the module\\n// *********************\\n\\nstatic struct PyModuleDef moduledef = \\n{\\n    PyModuleDef_HEAD_INIT,\\n    \"radius_neighbors\",\\t\\t// m_name\\n    module_docstring,       // m_doc\\n    -1,                     // m_size\\n    module_methods,         // m_methods\\n    NULL,                   // m_reload\\n    NULL,                   // m_traverse\\n    NULL,                   // m_clear\\n    NULL,                   // m_free\\n};\\n\\nPyMODINIT_FUNC PyInit_radius_neighbors(void)\\n{\\n    import_array();\\n\\treturn PyModule_Create(&moduledef);\\n}\\n\\n\\n// Definition of the batch_subsample method\\n// **********************************\\n\\nstatic PyObject* batch_neighbors(PyObject* self, PyObject* args, PyObject* keywds)\\n{\\n\\n\\t// Manage inputs\\n\\t// *************\\n\\n\\t// Args containers\\n\\tPyObject* queries_obj = NULL;\\n\\tPyObject* supports_obj = NULL;\\n\\tPyObject* q_batches_obj = NULL;\\n\\tPyObject* s_batches_obj = NULL;\\n\\n\\t// Keywords containers\\n\\tstatic char* kwlist[] = { \"queries\", \"supports\", \"q_batches\", \"s_batches\", \"radius\", NULL };\\n\\tfloat radius = 0.1;\\n\\n\\t// Parse the input  \\n\\tif (!PyArg_ParseTupleAndKeywords(args, keywds, \"OOOO|$f\", kwlist, &queries_obj, &supports_obj, &q_batches_obj, &s_batches_obj, &radius))\\n\\t{\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Error parsing arguments\");\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\n\\t// Interpret the input objects as numpy arrays.\\n\\tPyObject* queries_array = PyArray_FROM_OTF(queries_obj, NPY_FLOAT, NPY_IN_ARRAY);\\n\\tPyObject* supports_array = PyArray_FROM_OTF(supports_obj, NPY_FLOAT, NPY_IN_ARRAY);\\n\\tPyObject* q_batches_array = PyArray_FROM_OTF(q_batches_obj, NPY_INT, NPY_IN_ARRAY);\\n\\tPyObject* s_batches_array = PyArray_FROM_OTF(s_batches_obj, NPY_INT, NPY_IN_ARRAY);\\n\\n\\t// Verify data was load correctly.\\n\\tif (queries_array == NULL)\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Error converting query points to numpy arrays of type float32\");\\n\\t\\treturn NULL;\\n\\t}\\n\\tif (supports_array == NULL)\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Error converting support points to numpy arrays of type float32\");\\n\\t\\treturn NULL;\\n\\t}\\n\\tif (q_batches_array == NULL)\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Error converting query batches to numpy arrays of type int32\");\\n\\t\\treturn NULL;\\n\\t}\\n\\tif (s_batches_array == NULL)\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Error converting support batches to numpy arrays of type int32\");\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\t// Check that the input array respect the dims\\n\\tif ((int)PyArray_NDIM(queries_array) != 2 || (int)PyArray_DIM(queries_array, 1) != 3)\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Wrong dimensions : query.shape is not (N, 3)\");\\n\\t\\treturn NULL;\\n\\t}\\n\\tif ((int)PyArray_NDIM(supports_array) != 2 || (int)PyArray_DIM(supports_array, 1) != 3)\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Wrong dimensions : support.shape is not (N, 3)\");\\n\\t\\treturn NULL;\\n\\t}\\n\\tif ((int)PyArray_NDIM(q_batches_array) > 1)\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Wrong dimensions : queries_batches.shape is not (B,) \");\\n\\t\\treturn NULL;\\n\\t}\\n\\tif ((int)PyArray_NDIM(s_batches_array) > 1)\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Wrong dimensions : supports_batches.shape is not (B,) \");\\n\\t\\treturn NULL;\\n\\t}\\n\\tif ((int)PyArray_DIM(q_batches_array, 0) != (int)PyArray_DIM(s_batches_array, 0))\\n\\t{\\n\\t\\tPy_XDECREF(queries_array);\\n\\t\\tPy_XDECREF(supports_array);\\n\\t\\tPy_XDECREF(q_batches_array);\\n\\t\\tPy_XDECREF(s_batches_array);\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Wrong number of batch elements: different for queries and supports \");\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\t// Number of points\\n\\tint Nq = (int)PyArray_DIM(queries_array, 0);\\n\\tint Ns= (int)PyArray_DIM(supports_array, 0);\\n\\n\\t// Number of batches\\n\\tint Nb = (int)PyArray_DIM(q_batches_array, 0);\\n\\n\\t// Call the C++ function\\n\\t// *********************\\n\\n\\t// Convert PyArray to Cloud C++ class\\n\\tvector<PointXYZ> queries;\\n\\tvector<PointXYZ> supports;\\n\\tvector<int> q_batches;\\n\\tvector<int> s_batches;\\n\\tqueries = vector<PointXYZ>((PointXYZ*)PyArray_DATA(queries_array), (PointXYZ*)PyArray_DATA(queries_array) + Nq);\\n\\tsupports = vector<PointXYZ>((PointXYZ*)PyArray_DATA(supports_array), (PointXYZ*)PyArray_DATA(supports_array) + Ns);\\n\\tq_batches = vector<int>((int*)PyArray_DATA(q_batches_array), (int*)PyArray_DATA(q_batches_array) + Nb);\\n\\ts_batches = vector<int>((int*)PyArray_DATA(s_batches_array), (int*)PyArray_DATA(s_batches_array) + Nb);\\n\\n\\t// Create result containers\\n\\tvector<int> neighbors_indices;\\n\\n\\t// Compute results\\n\\t//batch_ordered_neighbors(queries, supports, q_batches, s_batches, neighbors_indices, radius);\\n\\tbatch_nanoflann_neighbors(queries, supports, q_batches, s_batches, neighbors_indices, radius);\\n\\n\\t// Check result\\n\\tif (neighbors_indices.size() < 1)\\n\\t{\\n\\t\\tPyErr_SetString(PyExc_RuntimeError, \"Error\");\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\t// Manage outputs\\n\\t// **************\\n\\n\\t// Maximal number of neighbors\\n\\tint max_neighbors = neighbors_indices.size() / Nq;\\n\\n\\t// Dimension of output containers\\n\\tnpy_intp* neighbors_dims = new npy_intp[2];\\n\\tneighbors_dims[0] = Nq;\\n\\tneighbors_dims[1] = max_neighbors;\\n\\n\\t// Create output array\\n\\tPyObject* res_obj = PyArray_SimpleNew(2, neighbors_dims, NPY_INT);\\n\\tPyObject* ret = NULL;\\n\\n\\t// Fill output array with values\\n\\tsize_t size_in_bytes = Nq * max_neighbors * sizeof(int);\\n\\tmemcpy(PyArray_DATA(res_obj), neighbors_indices.data(), size_in_bytes);\\n\\n\\t// Merge results\\n\\tret = Py_BuildValue(\"N\", res_obj);\\n\\n\\t// Clean up\\n\\t// ********\\n\\n\\tPy_XDECREF(queries_array);\\n\\tPy_XDECREF(supports_array);\\n\\tPy_XDECREF(q_batches_array);\\n\\tPy_XDECREF(s_batches_array);\\n\\n\\treturn ret;\\n}\\n\\n\\n***Folder: grid_subsampling\\nFile Name: grid_subsampling.h\\nContents of Filegrid_subsampling.h: \\n\\n#include \"../../cpp_utils/cloud/cloud.h\"\\n\\n#include <set>\\n#include <cstdint>\\n\\nusing namespace std;\\n\\nclass SampledData\\n{\\npublic:\\n\\n\\t// Elements\\n\\t// ********\\n\\n\\tint count;\\n\\tPointXYZ point;\\n\\tvector<float> features;\\n\\tvector<unordered_map<int, int>> labels;\\n\\n\\n\\t// Methods\\n\\t// *******\\n\\n\\t// Constructor\\n\\tSampledData() \\n\\t{ \\n\\t\\tcount = 0; \\n\\t\\tpoint = PointXYZ();\\n\\t}\\n\\n\\tSampledData(const size_t fdim, const size_t ldim)\\n\\t{\\n\\t\\tcount = 0;\\n\\t\\tpoint = PointXYZ();\\n\\t    features = vector<float>(fdim);\\n\\t    labels = vector<unordered_map<int, int>>(ldim);\\n\\t}\\n\\n\\t// Method Update\\n\\tvoid update_all(const PointXYZ p, vector<float>::iterator f_begin, vector<int>::iterator l_begin)\\n\\t{\\n\\t\\tcount += 1;\\n\\t\\tpoint += p;\\n\\t\\ttransform (features.begin(), features.end(), f_begin, features.begin(), plus<float>());\\n\\t\\tint i = 0;\\n\\t\\tfor(vector<int>::iterator it = l_begin; it != l_begin + labels.size(); ++it)\\n\\t\\t{\\n\\t\\t    labels[i][*it] += 1;\\n\\t\\t    i++;\\n\\t\\t}\\n\\t\\treturn;\\n\\t}\\n\\tvoid update_features(const PointXYZ p, vector<float>::iterator f_begin)\\n\\t{\\n\\t\\tcount += 1;\\n\\t\\tpoint += p;\\n\\t\\ttransform (features.begin(), features.end(), f_begin, features.begin(), plus<float>());\\n\\t\\treturn;\\n\\t}\\n\\tvoid update_classes(const PointXYZ p, vector<int>::iterator l_begin)\\n\\t{\\n\\t\\tcount += 1;\\n\\t\\tpoint += p;\\n\\t\\tint i = 0;\\n\\t\\tfor(vector<int>::iterator it = l_begin; it != l_begin + labels.size(); ++it)\\n\\t\\t{\\n\\t\\t    labels[i][*it] += 1;\\n\\t\\t    i++;\\n\\t\\t}\\n\\t\\treturn;\\n\\t}\\n\\tvoid update_points(const PointXYZ p)\\n\\t{\\n\\t\\tcount += 1;\\n\\t\\tpoint += p;\\n\\t\\treturn;\\n\\t}\\n};\\n\\nvoid grid_subsampling(vector<PointXYZ>& original_points,\\n                      vector<PointXYZ>& subsampled_points,\\n                      vector<float>& original_features,\\n                      vector<float>& subsampled_features,\\n                      vector<int>& original_classes,\\n                      vector<int>& subsampled_classes,\\n                      float sampleDl,\\n                      int verbose);\\n\\nvoid batch_grid_subsampling(vector<PointXYZ>& original_points,\\n                            vector<PointXYZ>& subsampled_points,\\n                            vector<float>& original_features,\\n                            vector<float>& subsampled_features,\\n                            vector<int>& original_classes,\\n                            vector<int>& subsampled_classes,\\n                            vector<int>& original_batches,\\n                            vector<int>& subsampled_batches,\\n                            float sampleDl,\\n                            int max_p);\\n\\n\\n\\nFile Name: grid_subsampling.cpp\\nContents of Filegrid_subsampling.cpp: \\n#include \"grid_subsampling.h\"\\n\\n\\nvoid grid_subsampling(vector<PointXYZ>& original_points,\\n                      vector<PointXYZ>& subsampled_points,\\n                      vector<float>& original_features,\\n                      vector<float>& subsampled_features,\\n                      vector<int>& original_classes,\\n                      vector<int>& subsampled_classes,\\n                      float sampleDl,\\n                      int verbose) {\\n\\n\\t// Initialize variables\\n\\t// ******************\\n\\n\\t// Number of points in the cloud\\n\\tsize_t N = original_points.size();\\n\\n\\t// Dimension of the features\\n\\tsize_t fdim = original_features.size() / N;\\n\\tsize_t ldim = original_classes.size() / N;\\n\\n\\t// Limits of the cloud\\n\\tPointXYZ minCorner = min_point(original_points);\\n\\tPointXYZ maxCorner = max_point(original_points);\\n\\tPointXYZ originCorner = floor(minCorner * (1/sampleDl)) * sampleDl;\\n\\n\\t// Dimensions of the grid\\n\\tsize_t sampleNX = (size_t)floor((maxCorner.x - originCorner.x) / sampleDl) + 1;\\n\\tsize_t sampleNY = (size_t)floor((maxCorner.y - originCorner.y) / sampleDl) + 1;\\n\\t//size_t sampleNZ = (size_t)floor((maxCorner.z - originCorner.z) / sampleDl) + 1;\\n\\n\\t// Check if features and classes need to be processed\\n\\tbool use_feature = original_features.size() > 0;\\n\\tbool use_classes = original_classes.size() > 0;\\n\\n\\n\\t// Create the sampled map\\n\\t// **********************\\n\\n\\t// Verbose parameters\\n\\tint i = 0;\\n\\tint nDisp = N / 100;\\n\\n\\t// Initialize variables\\n\\tsize_t iX, iY, iZ, mapIdx;\\n\\tunordered_map<size_t, SampledData> data;\\n\\n\\tfor (auto& p : original_points)\\n\\t{\\n\\t\\t// Position of point in sample map\\n\\t\\tiX = (size_t)floor((p.x - originCorner.x) / sampleDl);\\n\\t\\tiY = (size_t)floor((p.y - originCorner.y) / sampleDl);\\n\\t\\tiZ = (size_t)floor((p.z - originCorner.z) / sampleDl);\\n\\t\\tmapIdx = iX + sampleNX*iY + sampleNX*sampleNY*iZ;\\n\\n\\t\\t// If not already created, create key\\n\\t\\tif (data.count(mapIdx) < 1)\\n\\t\\t\\tdata.emplace(mapIdx, SampledData(fdim, ldim));\\n\\n\\t\\t// Fill the sample map\\n\\t\\tif (use_feature && use_classes)\\n\\t\\t\\tdata[mapIdx].update_all(p, original_features.begin() + i * fdim, original_classes.begin() + i * ldim);\\n\\t\\telse if (use_feature)\\n\\t\\t\\tdata[mapIdx].update_features(p, original_features.begin() + i * fdim);\\n\\t\\telse if (use_classes)\\n\\t\\t\\tdata[mapIdx].update_classes(p, original_classes.begin() + i * ldim);\\n\\t\\telse\\n\\t\\t\\tdata[mapIdx].update_points(p);\\n\\n\\t\\t// Display\\n\\t\\ti++;\\n\\t\\tif (verbose > 1 && i%nDisp == 0)\\n\\t\\t\\tstd::cout << \"\\\\rSampled Map : \" << std::setw(3) << i / nDisp << \"%\";\\n\\n\\t}\\n\\n\\t// Divide for barycentre and transfer to a vector\\n\\tsubsampled_points.reserve(data.size());\\n\\tif (use_feature)\\n\\t\\tsubsampled_features.reserve(data.size() * fdim);\\n\\tif (use_classes)\\n\\t\\tsubsampled_classes.reserve(data.size() * ldim);\\n\\tfor (auto& v : data)\\n\\t{\\n\\t\\tsubsampled_points.push_back(v.second.point * (1.0 / v.second.count));\\n\\t\\tif (use_feature)\\n\\t\\t{\\n\\t\\t    float count = (float)v.second.count;\\n\\t\\t    transform(v.second.features.begin(),\\n                      v.second.features.end(),\\n                      v.second.features.begin(),\\n                      [count](float f) { return f / count;});\\n            subsampled_features.insert(subsampled_features.end(),v.second.features.begin(),v.second.features.end());\\n\\t\\t}\\n\\t\\tif (use_classes)\\n\\t\\t{\\n\\t\\t    for (int i = 0; i < ldim; i++)\\n\\t\\t        subsampled_classes.push_back(max_element(v.second.labels[i].begin(), v.second.labels[i].end(),\\n\\t\\t        [](const pair<int, int>&a, const pair<int, int>&b){return a.second < b.second;})->first);\\n\\t\\t}\\n\\t}\\n\\n\\treturn;\\n}\\n\\n\\nvoid batch_grid_subsampling(vector<PointXYZ>& original_points,\\n                              vector<PointXYZ>& subsampled_points,\\n                              vector<float>& original_features,\\n                              vector<float>& subsampled_features,\\n                              vector<int>& original_classes,\\n                              vector<int>& subsampled_classes,\\n                              vector<int>& original_batches,\\n                              vector<int>& subsampled_batches,\\n                              float sampleDl,\\n                              int max_p)\\n{\\n\\t// Initialize variables\\n\\t// ******************\\n\\n\\tint b = 0;\\n\\tint sum_b = 0;\\n\\n\\t// Number of points in the cloud\\n\\tsize_t N = original_points.size();\\n\\n\\t// Dimension of the features\\n\\tsize_t fdim = original_features.size() / N;\\n\\tsize_t ldim = original_classes.size() / N;\\n\\n\\t// Handle max_p = 0\\n\\tif (max_p < 1)\\n\\t    max_p = N;\\n\\n\\t// Loop over batches\\n\\t// *****************\\n\\n\\tfor (b = 0; b < original_batches.size(); b++)\\n\\t{\\n\\n\\t    // Extract batch points features and labels\\n\\t    vector<PointXYZ> b_o_points = vector<PointXYZ>(original_points.begin () + sum_b,\\n\\t                                                   original_points.begin () + sum_b + original_batches[b]);\\n\\n        vector<float> b_o_features;\\n        if (original_features.size() > 0)\\n        {\\n            b_o_features = vector<float>(original_features.begin () + sum_b * fdim,\\n                                         original_features.begin () + (sum_b + original_batches[b]) * fdim);\\n\\t    }\\n\\n\\t    vector<int> b_o_classes;\\n        if (original_classes.size() > 0)\\n        {\\n            b_o_classes = vector<int>(original_classes.begin () + sum_b * ldim,\\n                                      original_classes.begin () + sum_b + original_batches[b] * ldim);\\n\\t    }\\n\\n\\n        // Create result containers\\n        vector<PointXYZ> b_s_points;\\n        vector<float> b_s_features;\\n        vector<int> b_s_classes;\\n\\n        // Compute subsampling on current batch\\n        grid_subsampling(b_o_points,\\n                         b_s_points,\\n                         b_o_features,\\n                         b_s_features,\\n                         b_o_classes,\\n                         b_s_classes,\\n                         sampleDl,\\n\\t\\t\\t\\t\\t\\t 0);\\n\\n        // Stack batches points features and labels\\n        // ****************************************\\n\\n        // If too many points remove some\\n        if (b_s_points.size() <= max_p)\\n        {\\n            subsampled_points.insert(subsampled_points.end(), b_s_points.begin(), b_s_points.end());\\n\\n            if (original_features.size() > 0)\\n                subsampled_features.insert(subsampled_features.end(), b_s_features.begin(), b_s_features.end());\\n\\n            if (original_classes.size() > 0)\\n                subsampled_classes.insert(subsampled_classes.end(), b_s_classes.begin(), b_s_classes.end());\\n\\n            subsampled_batches.push_back(b_s_points.size());\\n        }\\n        else\\n        {\\n            subsampled_points.insert(subsampled_points.end(), b_s_points.begin(), b_s_points.begin() + max_p);\\n\\n            if (original_features.size() > 0)\\n                subsampled_features.insert(subsampled_features.end(), b_s_features.begin(), b_s_features.begin() + max_p * fdim);\\n\\n            if (original_classes.size() > 0)\\n                subsampled_classes.insert(subsampled_classes.end(), b_s_classes.begin(), b_s_classes.begin() + max_p * ldim);\\n\\n            subsampled_batches.push_back(max_p);\\n        }\\n\\n        // Stack new batch lengths\\n        sum_b += original_batches[b];\\n\\t}\\n\\n\\treturn;\\n}\\n\\n\\n***Folder: cloud\\nFile Name: cloud.cpp\\nContents of Filecloud.cpp: //\\n//\\n//\\t\\t0==========================0\\n//\\t\\t|    Local feature test    |\\n//\\t\\t0==========================0\\n//\\n//\\t\\tversion 1.0 : \\n//\\t\\t\\t> \\n//\\n//---------------------------------------------------\\n//\\n//\\t\\tCloud source :\\n//\\t\\tDefine usefull Functions/Methods\\n//\\n//----------------------------------------------------\\n//\\n//\\t\\tHugues THOMAS - 10/02/2017\\n//\\n\\n\\n#include \"cloud.h\"\\n\\n\\n// Getters\\n// *******\\n\\nPointXYZ max_point(std::vector<PointXYZ> points)\\n{\\n\\t// Initialize limits\\n\\tPointXYZ maxP(points[0]);\\n\\n\\t// Loop over all points\\n\\tfor (auto p : points)\\n\\t{\\n\\t\\tif (p.x > maxP.x)\\n\\t\\t\\tmaxP.x = p.x;\\n\\n\\t\\tif (p.y > maxP.y)\\n\\t\\t\\tmaxP.y = p.y;\\n\\n\\t\\tif (p.z > maxP.z)\\n\\t\\t\\tmaxP.z = p.z;\\n\\t}\\n\\n\\treturn maxP;\\n}\\n\\nPointXYZ min_point(std::vector<PointXYZ> points)\\n{\\n\\t// Initialize limits\\n\\tPointXYZ minP(points[0]);\\n\\n\\t// Loop over all points\\n\\tfor (auto p : points)\\n\\t{\\n\\t\\tif (p.x < minP.x)\\n\\t\\t\\tminP.x = p.x;\\n\\n\\t\\tif (p.y < minP.y)\\n\\t\\t\\tminP.y = p.y;\\n\\n\\t\\tif (p.z < minP.z)\\n\\t\\t\\tminP.z = p.z;\\n\\t}\\n\\n\\treturn minP;\\n}\\n\\nFile Name: cloud.h\\nContents of Filecloud.h: //\\n//\\n//\\t\\t0==========================0\\n//\\t\\t|    Local feature test    |\\n//\\t\\t0==========================0\\n//\\n//\\t\\tversion 1.0 : \\n//\\t\\t\\t> \\n//\\n//---------------------------------------------------\\n//\\n//\\t\\tCloud header\\n//\\n//----------------------------------------------------\\n//\\n//\\t\\tHugues THOMAS - 10/02/2017\\n//\\n\\n\\n# pragma once\\n\\n#include <vector>\\n#include <unordered_map>\\n#include <map>\\n#include <algorithm>\\n#include <numeric>\\n#include <iostream>\\n#include <iomanip>\\n#include <cmath>\\n\\n#include <time.h>\\n\\n\\n\\n\\n// Point class\\n// ***********\\n\\n\\nclass PointXYZ\\n{\\npublic:\\n\\n\\t// Elements\\n\\t// ********\\n\\n\\tfloat x, y, z;\\n\\n\\n\\t// Methods\\n\\t// *******\\n\\t\\n\\t// Constructor\\n\\tPointXYZ() { x = 0; y = 0; z = 0; }\\n\\tPointXYZ(float x0, float y0, float z0) { x = x0; y = y0; z = z0; }\\n\\t\\n\\t// array type accessor\\n\\tfloat operator [] (int i) const\\n\\t{\\n\\t\\tif (i == 0) return x;\\n\\t\\telse if (i == 1) return y;\\n\\t\\telse return z;\\n\\t}\\n\\n\\t// opperations\\n\\tfloat dot(const PointXYZ P) const\\n\\t{\\n\\t\\treturn x * P.x + y * P.y + z * P.z;\\n\\t}\\n\\n\\tfloat sq_norm()\\n\\t{\\n\\t\\treturn x*x + y*y + z*z;\\n\\t}\\n\\n\\tPointXYZ cross(const PointXYZ P) const\\n\\t{\\n\\t\\treturn PointXYZ(y*P.z - z*P.y, z*P.x - x*P.z, x*P.y - y*P.x);\\n\\t}\\t\\n\\n\\tPointXYZ& operator+=(const PointXYZ& P)\\n\\t{\\n\\t\\tx += P.x;\\n\\t\\ty += P.y;\\n\\t\\tz += P.z;\\n\\t\\treturn *this;\\n\\t}\\n\\n\\tPointXYZ& operator-=(const PointXYZ& P)\\n\\t{\\n\\t\\tx -= P.x;\\n\\t\\ty -= P.y;\\n\\t\\tz -= P.z;\\n\\t\\treturn *this;\\n\\t}\\n\\n\\tPointXYZ& operator*=(const float& a)\\n\\t{\\n\\t\\tx *= a;\\n\\t\\ty *= a;\\n\\t\\tz *= a;\\n\\t\\treturn *this;\\n\\t}\\n};\\n\\n\\n// Point Opperations\\n// *****************\\n\\ninline PointXYZ operator + (const PointXYZ A, const PointXYZ B)\\n{\\n\\treturn PointXYZ(A.x + B.x, A.y + B.y, A.z + B.z);\\n}\\n\\ninline PointXYZ operator - (const PointXYZ A, const PointXYZ B)\\n{\\n\\treturn PointXYZ(A.x - B.x, A.y - B.y, A.z - B.z);\\n}\\n\\ninline PointXYZ operator * (const PointXYZ P, const float a)\\n{\\n\\treturn PointXYZ(P.x * a, P.y * a, P.z * a);\\n}\\n\\ninline PointXYZ operator * (const float a, const PointXYZ P)\\n{\\n\\treturn PointXYZ(P.x * a, P.y * a, P.z * a);\\n}\\n\\ninline std::ostream& operator << (std::ostream& os, const PointXYZ P)\\n{\\n\\treturn os << \"[\" << P.x << \", \" << P.y << \", \" << P.z << \"]\";\\n}\\n\\ninline bool operator == (const PointXYZ A, const PointXYZ B)\\n{\\n\\treturn A.x == B.x && A.y == B.y && A.z == B.z;\\n}\\n\\ninline PointXYZ floor(const PointXYZ P)\\n{\\n\\treturn PointXYZ(std::floor(P.x), std::floor(P.y), std::floor(P.z));\\n}\\n\\n\\nPointXYZ max_point(std::vector<PointXYZ> points);\\nPointXYZ min_point(std::vector<PointXYZ> points);\\n\\n\\nstruct PointCloud\\n{\\n\\n\\tstd::vector<PointXYZ>  pts;\\n\\n\\t// Must return the number of data points\\n\\tinline size_t kdtree_get_point_count() const { return pts.size(); }\\n\\n\\t// Returns the dim\\'th component of the idx\\'th point in the class:\\n\\t// Since this is inlined and the \"dim\" argument is typically an immediate value, the\\n\\t//  \"if/else\\'s\" are actually solved at compile time.\\n\\tinline float kdtree_get_pt(const size_t idx, const size_t dim) const\\n\\t{\\n\\t\\tif (dim == 0) return pts[idx].x;\\n\\t\\telse if (dim == 1) return pts[idx].y;\\n\\t\\telse return pts[idx].z;\\n\\t}\\n\\n\\t// Optional bounding-box computation: return false to default to a standard bbox computation loop.\\n\\t//   Return true if the BBOX was already computed by the class and returned in \"bb\" so it can be avoided to redo it again.\\n\\t//   Look at bb.size() to find out the expected dimensionality (e.g. 2 or 3 for point clouds)\\n\\ttemplate <class BBOX>\\n\\tbool kdtree_get_bbox(BBOX& /* bb */) const { return false; }\\n\\n};\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n***Folder: nanoflann\\nFile Name: nanoflann.hpp\\nContents of Filenanoflann.hpp: /***********************************************************************\\n * Software License Agreement (BSD License)\\n *\\n * Copyright 2008-2009  Marius Muja (mariusm@cs.ubc.ca). All rights reserved.\\n * Copyright 2008-2009  David G. Lowe (lowe@cs.ubc.ca). All rights reserved.\\n * Copyright 2011-2016  Jose Luis Blanco (joseluisblancoc@gmail.com).\\n *   All rights reserved.\\n *\\n * THE BSD LICENSE\\n *\\n * Redistribution and use in source and binary forms, with or without\\n * modification, are permitted provided that the following conditions\\n * are met:\\n *\\n * 1. Redistributions of source code must retain the above copyright\\n *    notice, this list of conditions and the following disclaimer.\\n * 2. Redistributions in binary form must reproduce the above copyright\\n *    notice, this list of conditions and the following disclaimer in the\\n *    documentation and/or other materials provided with the distribution.\\n *\\n * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS\\'\\' AND ANY EXPRESS OR\\n * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES\\n * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\\n * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,\\n * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT\\n * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\\n * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n *************************************************************************/\\n\\n/** \\\\mainpage nanoflann C++ API documentation\\n *  nanoflann is a C++ header-only library for building KD-Trees, mostly\\n *  optimized for 2D or 3D point clouds.\\n *\\n *  nanoflann does not require compiling or installing, just an\\n *  #include <nanoflann.hpp> in your code.\\n *\\n *  See:\\n *   - <a href=\"modules.html\" >C++ API organized by modules</a>\\n *   - <a href=\"https://github.com/jlblancoc/nanoflann\" >Online README</a>\\n *   - <a href=\"http://jlblancoc.github.io/nanoflann/\" >Doxygen\\n * documentation</a>\\n */\\n\\n#ifndef NANOFLANN_HPP_\\n#define NANOFLANN_HPP_\\n\\n#include <algorithm>\\n#include <array>\\n#include <cassert>\\n#include <cmath>   // for abs()\\n#include <cstdio>  // for fwrite()\\n#include <cstdlib> // for abs()\\n#include <functional>\\n#include <limits> // std::reference_wrapper\\n#include <stdexcept>\\n#include <vector>\\n\\n/** Library version: 0xMmP (M=Major,m=minor,P=patch) */\\n#define NANOFLANN_VERSION 0x130\\n\\n// Avoid conflicting declaration of min/max macros in windows headers\\n#if !defined(NOMINMAX) &&                                                      \\\\\\n    (defined(_WIN32) || defined(_WIN32_) || defined(WIN32) || defined(_WIN64))\\n#define NOMINMAX\\n#ifdef max\\n#undef max\\n#undef min\\n#endif\\n#endif\\n\\nnamespace nanoflann {\\n/** @addtogroup nanoflann_grp nanoflann C++ library for ANN\\n *  @{ */\\n\\n/** the PI constant (required to avoid MSVC missing symbols) */\\ntemplate <typename T> T pi_const() {\\n  return static_cast<T>(3.14159265358979323846);\\n}\\n\\n/**\\n * Traits if object is resizable and assignable (typically has a resize | assign\\n * method)\\n */\\ntemplate <typename T, typename = int> struct has_resize : std::false_type {};\\n\\ntemplate <typename T>\\nstruct has_resize<T, decltype((void)std::declval<T>().resize(1), 0)>\\n    : std::true_type {};\\n\\ntemplate <typename T, typename = int> struct has_assign : std::false_type {};\\n\\ntemplate <typename T>\\nstruct has_assign<T, decltype((void)std::declval<T>().assign(1, 0), 0)>\\n    : std::true_type {};\\n\\n/**\\n * Free function to resize a resizable object\\n */\\ntemplate <typename Container>\\ninline typename std::enable_if<has_resize<Container>::value, void>::type\\nresize(Container &c, const size_t nElements) {\\n  c.resize(nElements);\\n}\\n\\n/**\\n * Free function that has no effects on non resizable containers (e.g.\\n * std::array) It raises an exception if the expected size does not match\\n */\\ntemplate <typename Container>\\ninline typename std::enable_if<!has_resize<Container>::value, void>::type\\nresize(Container &c, const size_t nElements) {\\n  if (nElements != c.size())\\n    throw std::logic_error(\"Try to change the size of a std::array.\");\\n}\\n\\n/**\\n * Free function to assign to a container\\n */\\ntemplate <typename Container, typename T>\\ninline typename std::enable_if<has_assign<Container>::value, void>::type\\nassign(Container &c, const size_t nElements, const T &value) {\\n  c.assign(nElements, value);\\n}\\n\\n/**\\n * Free function to assign to a std::array\\n */\\ntemplate <typename Container, typename T>\\ninline typename std::enable_if<!has_assign<Container>::value, void>::type\\nassign(Container &c, const size_t nElements, const T &value) {\\n  for (size_t i = 0; i < nElements; i++)\\n    c[i] = value;\\n}\\n\\n/** @addtogroup result_sets_grp Result set classes\\n *  @{ */\\ntemplate <typename _DistanceType, typename _IndexType = size_t,\\n          typename _CountType = size_t>\\nclass KNNResultSet {\\npublic:\\n  typedef _DistanceType DistanceType;\\n  typedef _IndexType IndexType;\\n  typedef _CountType CountType;\\n\\nprivate:\\n  IndexType *indices;\\n  DistanceType *dists;\\n  CountType capacity;\\n  CountType count;\\n\\npublic:\\n  inline KNNResultSet(CountType capacity_)\\n      : indices(0), dists(0), capacity(capacity_), count(0) {}\\n\\n  inline void init(IndexType *indices_, DistanceType *dists_) {\\n    indices = indices_;\\n    dists = dists_;\\n    count = 0;\\n    if (capacity)\\n      dists[capacity - 1] = (std::numeric_limits<DistanceType>::max)();\\n  }\\n\\n  inline CountType size() const { return count; }\\n\\n  inline bool full() const { return count == capacity; }\\n\\n  /**\\n   * Called during search to add an element matching the criteria.\\n   * @return true if the search should be continued, false if the results are\\n   * sufficient\\n   */\\n  inline bool addPoint(DistanceType dist, IndexType index) {\\n    CountType i;\\n    for (i = count; i > 0; --i) {\\n#ifdef NANOFLANN_FIRST_MATCH // If defined and two points have the same\\n                             // distance, the one with the lowest-index will be\\n                             // returned first.\\n      if ((dists[i - 1] > dist) ||\\n          ((dist == dists[i - 1]) && (indices[i - 1] > index))) {\\n#else\\n      if (dists[i - 1] > dist) {\\n#endif\\n        if (i < capacity) {\\n          dists[i] = dists[i - 1];\\n          indices[i] = indices[i - 1];\\n        }\\n      } else\\n        break;\\n    }\\n    if (i < capacity) {\\n      dists[i] = dist;\\n      indices[i] = index;\\n    }\\n    if (count < capacity)\\n      count++;\\n\\n    // tell caller that the search shall continue\\n    return true;\\n  }\\n\\n  inline DistanceType worstDist() const { return dists[capacity - 1]; }\\n};\\n\\n/** operator \"<\" for std::sort() */\\nstruct IndexDist_Sorter {\\n  /** PairType will be typically: std::pair<IndexType,DistanceType> */\\n  template <typename PairType>\\n  inline bool operator()(const PairType &p1, const PairType &p2) const {\\n    return p1.second < p2.second;\\n  }\\n};\\n\\n/**\\n * A result-set class used when performing a radius based search.\\n */\\ntemplate <typename _DistanceType, typename _IndexType = size_t>\\nclass RadiusResultSet {\\npublic:\\n  typedef _DistanceType DistanceType;\\n  typedef _IndexType IndexType;\\n\\npublic:\\n  const DistanceType radius;\\n\\n  std::vector<std::pair<IndexType, DistanceType>> &m_indices_dists;\\n\\n  inline RadiusResultSet(\\n      DistanceType radius_,\\n      std::vector<std::pair<IndexType, DistanceType>> &indices_dists)\\n      : radius(radius_), m_indices_dists(indices_dists) {\\n    init();\\n  }\\n\\n  inline void init() { clear(); }\\n  inline void clear() { m_indices_dists.clear(); }\\n\\n  inline size_t size() const { return m_indices_dists.size(); }\\n\\n  inline bool full() const { return true; }\\n\\n  /**\\n   * Called during search to add an element matching the criteria.\\n   * @return true if the search should be continued, false if the results are\\n   * sufficient\\n   */\\n  inline bool addPoint(DistanceType dist, IndexType index) {\\n    if (dist < radius)\\n      m_indices_dists.push_back(std::make_pair(index, dist));\\n    return true;\\n  }\\n\\n  inline DistanceType worstDist() const { return radius; }\\n\\n  /**\\n   * Find the worst result (furtherest neighbor) without copying or sorting\\n   * Pre-conditions: size() > 0\\n   */\\n  std::pair<IndexType, DistanceType> worst_item() const {\\n    if (m_indices_dists.empty())\\n      throw std::runtime_error(\"Cannot invoke RadiusResultSet::worst_item() on \"\\n                               \"an empty list of results.\");\\n    typedef\\n        typename std::vector<std::pair<IndexType, DistanceType>>::const_iterator\\n            DistIt;\\n    DistIt it = std::max_element(m_indices_dists.begin(), m_indices_dists.end(),\\n                                 IndexDist_Sorter());\\n    return *it;\\n  }\\n};\\n\\n/** @} */\\n\\n/** @addtogroup loadsave_grp Load/save auxiliary functions\\n * @{ */\\ntemplate <typename T>\\nvoid save_value(FILE *stream, const T &value, size_t count = 1) {\\n  fwrite(&value, sizeof(value), count, stream);\\n}\\n\\ntemplate <typename T>\\nvoid save_value(FILE *stream, const std::vector<T> &value) {\\n  size_t size = value.size();\\n  fwrite(&size, sizeof(size_t), 1, stream);\\n  fwrite(&value[0], sizeof(T), size, stream);\\n}\\n\\ntemplate <typename T>\\nvoid load_value(FILE *stream, T &value, size_t count = 1) {\\n  size_t read_cnt = fread(&value, sizeof(value), count, stream);\\n  if (read_cnt != count) {\\n    throw std::runtime_error(\"Cannot read from file\");\\n  }\\n}\\n\\ntemplate <typename T> void load_value(FILE *stream, std::vector<T> &value) {\\n  size_t size;\\n  size_t read_cnt = fread(&size, sizeof(size_t), 1, stream);\\n  if (read_cnt != 1) {\\n    throw std::runtime_error(\"Cannot read from file\");\\n  }\\n  value.resize(size);\\n  read_cnt = fread(&value[0], sizeof(T), size, stream);\\n  if (read_cnt != size) {\\n    throw std::runtime_error(\"Cannot read from file\");\\n  }\\n}\\n/** @} */\\n\\n/** @addtogroup metric_grp Metric (distance) classes\\n * @{ */\\n\\nstruct Metric {};\\n\\n/** Manhattan distance functor (generic version, optimized for\\n * high-dimensionality data sets). Corresponding distance traits:\\n * nanoflann::metric_L1 \\\\tparam T Type of the elements (e.g. double, float,\\n * uint8_t) \\\\tparam _DistanceType Type of distance variables (must be signed)\\n * (e.g. float, double, int64_t)\\n */\\ntemplate <class T, class DataSource, typename _DistanceType = T>\\nstruct L1_Adaptor {\\n  typedef T ElementType;\\n  typedef _DistanceType DistanceType;\\n\\n  const DataSource &data_source;\\n\\n  L1_Adaptor(const DataSource &_data_source) : data_source(_data_source) {}\\n\\n  inline DistanceType evalMetric(const T *a, const size_t b_idx, size_t size,\\n                                 DistanceType worst_dist = -1) const {\\n    DistanceType result = DistanceType();\\n    const T *last = a + size;\\n    const T *lastgroup = last - 3;\\n    size_t d = 0;\\n\\n    /* Process 4 items with each loop for efficiency. */\\n    while (a < lastgroup) {\\n      const DistanceType diff0 =\\n          std::abs(a[0] - data_source.kdtree_get_pt(b_idx, d++));\\n      const DistanceType diff1 =\\n          std::abs(a[1] - data_source.kdtree_get_pt(b_idx, d++));\\n      const DistanceType diff2 =\\n          std::abs(a[2] - data_source.kdtree_get_pt(b_idx, d++));\\n      const DistanceType diff3 =\\n          std::abs(a[3] - data_source.kdtree_get_pt(b_idx, d++));\\n      result += diff0 + diff1 + diff2 + diff3;\\n      a += 4;\\n      if ((worst_dist > 0) && (result > worst_dist)) {\\n        return result;\\n      }\\n    }\\n    /* Process last 0-3 components.  Not needed for standard vector lengths. */\\n    while (a < last) {\\n      result += std::abs(*a++ - data_source.kdtree_get_pt(b_idx, d++));\\n    }\\n    return result;\\n  }\\n\\n  template <typename U, typename V>\\n  inline DistanceType accum_dist(const U a, const V b, const size_t) const {\\n    return std::abs(a - b);\\n  }\\n};\\n\\n/** Squared Euclidean distance functor (generic version, optimized for\\n * high-dimensionality data sets). Corresponding distance traits:\\n * nanoflann::metric_L2 \\\\tparam T Type of the elements (e.g. double, float,\\n * uint8_t) \\\\tparam _DistanceType Type of distance variables (must be signed)\\n * (e.g. float, double, int64_t)\\n */\\ntemplate <class T, class DataSource, typename _DistanceType = T>\\nstruct L2_Adaptor {\\n  typedef T ElementType;\\n  typedef _DistanceType DistanceType;\\n\\n  const DataSource &data_source;\\n\\n  L2_Adaptor(const DataSource &_data_source) : data_source(_data_source) {}\\n\\n  inline DistanceType evalMetric(const T *a, const size_t b_idx, size_t size,\\n                                 DistanceType worst_dist = -1) const {\\n    DistanceType result = DistanceType();\\n    const T *last = a + size;\\n    const T *lastgroup = last - 3;\\n    size_t d = 0;\\n\\n    /* Process 4 items with each loop for efficiency. */\\n    while (a < lastgroup) {\\n      const DistanceType diff0 = a[0] - data_source.kdtree_get_pt(b_idx, d++);\\n      const DistanceType diff1 = a[1] - data_source.kdtree_get_pt(b_idx, d++);\\n      const DistanceType diff2 = a[2] - data_source.kdtree_get_pt(b_idx, d++);\\n      const DistanceType diff3 = a[3] - data_source.kdtree_get_pt(b_idx, d++);\\n      result += diff0 * diff0 + diff1 * diff1 + diff2 * diff2 + diff3 * diff3;\\n      a += 4;\\n      if ((worst_dist > 0) && (result > worst_dist)) {\\n        return result;\\n      }\\n    }\\n    /* Process last 0-3 components.  Not needed for standard vector lengths. */\\n    while (a < last) {\\n      const DistanceType diff0 = *a++ - data_source.kdtree_get_pt(b_idx, d++);\\n      result += diff0 * diff0;\\n    }\\n    return result;\\n  }\\n\\n  template <typename U, typename V>\\n  inline DistanceType accum_dist(const U a, const V b, const size_t) const {\\n    return (a - b) * (a - b);\\n  }\\n};\\n\\n/** Squared Euclidean (L2) distance functor (suitable for low-dimensionality\\n * datasets, like 2D or 3D point clouds) Corresponding distance traits:\\n * nanoflann::metric_L2_Simple \\\\tparam T Type of the elements (e.g. double,\\n * float, uint8_t) \\\\tparam _DistanceType Type of distance variables (must be\\n * signed) (e.g. float, double, int64_t)\\n */\\ntemplate <class T, class DataSource, typename _DistanceType = T>\\nstruct L2_Simple_Adaptor {\\n  typedef T ElementType;\\n  typedef _DistanceType DistanceType;\\n\\n  const DataSource &data_source;\\n\\n  L2_Simple_Adaptor(const DataSource &_data_source)\\n      : data_source(_data_source) {}\\n\\n  inline DistanceType evalMetric(const T *a, const size_t b_idx,\\n                                 size_t size) const {\\n    DistanceType result = DistanceType();\\n    for (size_t i = 0; i < size; ++i) {\\n      const DistanceType diff = a[i] - data_source.kdtree_get_pt(b_idx, i);\\n      result += diff * diff;\\n    }\\n    return result;\\n  }\\n\\n  template <typename U, typename V>\\n  inline DistanceType accum_dist(const U a, const V b, const size_t) const {\\n    return (a - b) * (a - b);\\n  }\\n};\\n\\n/** SO2 distance functor\\n *  Corresponding distance traits: nanoflann::metric_SO2\\n * \\\\tparam T Type of the elements (e.g. double, float)\\n * \\\\tparam _DistanceType Type of distance variables (must be signed) (e.g.\\n * float, double) orientation is constrained to be in [-pi, pi]\\n */\\ntemplate <class T, class DataSource, typename _DistanceType = T>\\nstruct SO2_Adaptor {\\n  typedef T ElementType;\\n  typedef _DistanceType DistanceType;\\n\\n  const DataSource &data_source;\\n\\n  SO2_Adaptor(const DataSource &_data_source) : data_source(_data_source) {}\\n\\n  inline DistanceType evalMetric(const T *a, const size_t b_idx,\\n                                 size_t size) const {\\n    return accum_dist(a[size - 1], data_source.kdtree_get_pt(b_idx, size - 1),\\n                      size - 1);\\n  }\\n\\n  /** Note: this assumes that input angles are already in the range [-pi,pi] */\\n  template <typename U, typename V>\\n  inline DistanceType accum_dist(const U a, const V b, const size_t) const {\\n    DistanceType result = DistanceType(), PI = pi_const<DistanceType>();\\n    result = b - a;\\n    if (result > PI)\\n      result -= 2 * PI;\\n    else if (result < -PI)\\n      result += 2 * PI;\\n    return result;\\n  }\\n};\\n\\n/** SO3 distance functor (Uses L2_Simple)\\n *  Corresponding distance traits: nanoflann::metric_SO3\\n * \\\\tparam T Type of the elements (e.g. double, float)\\n * \\\\tparam _DistanceType Type of distance variables (must be signed) (e.g.\\n * float, double)\\n */\\ntemplate <class T, class DataSource, typename _DistanceType = T>\\nstruct SO3_Adaptor {\\n  typedef T ElementType;\\n  typedef _DistanceType DistanceType;\\n\\n  L2_Simple_Adaptor<T, DataSource> distance_L2_Simple;\\n\\n  SO3_Adaptor(const DataSource &_data_source)\\n      : distance_L2_Simple(_data_source) {}\\n\\n  inline DistanceType evalMetric(const T *a, const size_t b_idx,\\n                                 size_t size) const {\\n    return distance_L2_Simple.evalMetric(a, b_idx, size);\\n  }\\n\\n  template <typename U, typename V>\\n  inline DistanceType accum_dist(const U a, const V b, const size_t idx) const {\\n    return distance_L2_Simple.accum_dist(a, b, idx);\\n  }\\n};\\n\\n/** Metaprogramming helper traits class for the L1 (Manhattan) metric */\\nstruct metric_L1 : public Metric {\\n  template <class T, class DataSource> struct traits {\\n    typedef L1_Adaptor<T, DataSource> distance_t;\\n  };\\n};\\n/** Metaprogramming helper traits class for the L2 (Euclidean) metric */\\nstruct metric_L2 : public Metric {\\n  template <class T, class DataSource> struct traits {\\n    typedef L2_Adaptor<T, DataSource> distance_t;\\n  };\\n};\\n/** Metaprogramming helper traits class for the L2_simple (Euclidean) metric */\\nstruct metric_L2_Simple : public Metric {\\n  template <class T, class DataSource> struct traits {\\n    typedef L2_Simple_Adaptor<T, DataSource> distance_t;\\n  };\\n};\\n/** Metaprogramming helper traits class for the SO3_InnerProdQuat metric */\\nstruct metric_SO2 : public Metric {\\n  template <class T, class DataSource> struct traits {\\n    typedef SO2_Adaptor<T, DataSource> distance_t;\\n  };\\n};\\n/** Metaprogramming helper traits class for the SO3_InnerProdQuat metric */\\nstruct metric_SO3 : public Metric {\\n  template <class T, class DataSource> struct traits {\\n    typedef SO3_Adaptor<T, DataSource> distance_t;\\n  };\\n};\\n\\n/** @} */\\n\\n/** @addtogroup param_grp Parameter structs\\n * @{ */\\n\\n/**  Parameters (see README.md) */\\nstruct KDTreeSingleIndexAdaptorParams {\\n  KDTreeSingleIndexAdaptorParams(size_t _leaf_max_size = 10)\\n      : leaf_max_size(_leaf_max_size) {}\\n\\n  size_t leaf_max_size;\\n};\\n\\n/** Search options for KDTreeSingleIndexAdaptor::findNeighbors() */\\nstruct SearchParams {\\n  /** Note: The first argument (checks_IGNORED_) is ignored, but kept for\\n   * compatibility with the FLANN interface */\\n  SearchParams(int checks_IGNORED_ = 32, float eps_ = 0, bool sorted_ = true)\\n      : checks(checks_IGNORED_), eps(eps_), sorted(sorted_) {}\\n\\n  int checks;  //!< Ignored parameter (Kept for compatibility with the FLANN\\n               //!< interface).\\n  float eps;   //!< search for eps-approximate neighbours (default: 0)\\n  bool sorted; //!< only for radius search, require neighbours sorted by\\n               //!< distance (default: true)\\n};\\n/** @} */\\n\\n/** @addtogroup memalloc_grp Memory allocation\\n * @{ */\\n\\n/**\\n * Allocates (using C\\'s malloc) a generic type T.\\n *\\n * Params:\\n *     count = number of instances to allocate.\\n * Returns: pointer (of type T*) to memory buffer\\n */\\ntemplate <typename T> inline T *allocate(size_t count = 1) {\\n  T *mem = static_cast<T *>(::malloc(sizeof(T) * count));\\n  return mem;\\n}\\n\\n/**\\n * Pooled storage allocator\\n *\\n * The following routines allow for the efficient allocation of storage in\\n * small chunks from a specified pool.  Rather than allowing each structure\\n * to be freed individually, an entire pool of storage is freed at once.\\n * This method has two advantages over just using malloc() and free().  First,\\n * it is far more efficient for allocating small objects, as there is\\n * no overhead for remembering all the information needed to free each\\n * object or consolidating fragmented memory.  Second, the decision about\\n * how long to keep an object is made at the time of allocation, and there\\n * is no need to track down all the objects to free them.\\n *\\n */\\n\\nconst size_t WORDSIZE = 16;\\nconst size_t BLOCKSIZE = 8192;\\n\\nclass PooledAllocator {\\n  /* We maintain memory alignment to word boundaries by requiring that all\\n      allocations be in multiples of the machine wordsize.  */\\n  /* Size of machine word in bytes.  Must be power of 2. */\\n  /* Minimum number of bytes requested at a time from\\tthe system.  Must be\\n   * multiple of WORDSIZE. */\\n\\n  size_t remaining; /* Number of bytes left in current block of storage. */\\n  void *base;       /* Pointer to base of current block of storage. */\\n  void *loc;        /* Current location in block to next allocate memory. */\\n\\n  void internal_init() {\\n    remaining = 0;\\n    base = NULL;\\n    usedMemory = 0;\\n    wastedMemory = 0;\\n  }\\n\\npublic:\\n  size_t usedMemory;\\n  size_t wastedMemory;\\n\\n  /**\\n      Default constructor. Initializes a new pool.\\n   */\\n  PooledAllocator() { internal_init(); }\\n\\n  /**\\n   * Destructor. Frees all the memory allocated in this pool.\\n   */\\n  ~PooledAllocator() { free_all(); }\\n\\n  /** Frees all allocated memory chunks */\\n  void free_all() {\\n    while (base != NULL) {\\n      void *prev =\\n          *(static_cast<void **>(base)); /* Get pointer to prev block. */\\n      ::free(base);\\n      base = prev;\\n    }\\n    internal_init();\\n  }\\n\\n  /**\\n   * Returns a pointer to a piece of new memory of the given size in bytes\\n   * allocated from the pool.\\n   */\\n  void *malloc(const size_t req_size) {\\n    /* Round size up to a multiple of wordsize.  The following expression\\n        only works for WORDSIZE that is a power of 2, by masking last bits of\\n        incremented size to zero.\\n     */\\n    const size_t size = (req_size + (WORDSIZE - 1)) & ~(WORDSIZE - 1);\\n\\n    /* Check whether a new block must be allocated.  Note that the first word\\n        of a block is reserved for a pointer to the previous block.\\n     */\\n    if (size > remaining) {\\n\\n      wastedMemory += remaining;\\n\\n      /* Allocate new storage. */\\n      const size_t blocksize =\\n          (size + sizeof(void *) + (WORDSIZE - 1) > BLOCKSIZE)\\n              ? size + sizeof(void *) + (WORDSIZE - 1)\\n              : BLOCKSIZE;\\n\\n      // use the standard C malloc to allocate memory\\n      void *m = ::malloc(blocksize);\\n      if (!m) {\\n        fprintf(stderr, \"Failed to allocate memory.\\\\n\");\\n        return NULL;\\n      }\\n\\n      /* Fill first word of new block with pointer to previous block. */\\n      static_cast<void **>(m)[0] = base;\\n      base = m;\\n\\n      size_t shift = 0;\\n      // int size_t = (WORDSIZE - ( (((size_t)m) + sizeof(void*)) &\\n      // (WORDSIZE-1))) & (WORDSIZE-1);\\n\\n      remaining = blocksize - sizeof(void *) - shift;\\n      loc = (static_cast<char *>(m) + sizeof(void *) + shift);\\n    }\\n    void *rloc = loc;\\n    loc = static_cast<char *>(loc) + size;\\n    remaining -= size;\\n\\n    usedMemory += size;\\n\\n    return rloc;\\n  }\\n\\n  /**\\n   * Allocates (using this pool) a generic type T.\\n   *\\n   * Params:\\n   *     count = number of instances to allocate.\\n   * Returns: pointer (of type T*) to memory buffer\\n   */\\n  template <typename T> T *allocate(const size_t count = 1) {\\n    T *mem = static_cast<T *>(this->malloc(sizeof(T) * count));\\n    return mem;\\n  }\\n};\\n/** @} */\\n\\n/** @addtogroup nanoflann_metaprog_grp Auxiliary metaprogramming stuff\\n * @{ */\\n\\n/** Used to declare fixed-size arrays when DIM>0, dynamically-allocated vectors\\n * when DIM=-1. Fixed size version for a generic DIM:\\n */\\ntemplate <int DIM, typename T> struct array_or_vector_selector {\\n  typedef std::array<T, DIM> container_t;\\n};\\n/** Dynamic size version */\\ntemplate <typename T> struct array_or_vector_selector<-1, T> {\\n  typedef std::vector<T> container_t;\\n};\\n\\n/** @} */\\n\\n/** kd-tree base-class\\n *\\n * Contains the member functions common to the classes KDTreeSingleIndexAdaptor\\n * and KDTreeSingleIndexDynamicAdaptor_.\\n *\\n * \\\\tparam Derived The name of the class which inherits this class.\\n * \\\\tparam DatasetAdaptor The user-provided adaptor (see comments above).\\n * \\\\tparam Distance The distance metric to use, these are all classes derived\\n * from nanoflann::Metric \\\\tparam DIM Dimensionality of data points (e.g. 3 for\\n * 3D points) \\\\tparam IndexType Will be typically size_t or int\\n */\\n\\ntemplate <class Derived, typename Distance, class DatasetAdaptor, int DIM = -1,\\n          typename IndexType = size_t>\\nclass KDTreeBaseClass {\\n\\npublic:\\n  /** Frees the previously-built index. Automatically called within\\n   * buildIndex(). */\\n  void freeIndex(Derived &obj) {\\n    obj.pool.free_all();\\n    obj.root_node = NULL;\\n    obj.m_size_at_index_build = 0;\\n  }\\n\\n  typedef typename Distance::ElementType ElementType;\\n  typedef typename Distance::DistanceType DistanceType;\\n\\n  /*--------------------- Internal Data Structures --------------------------*/\\n  struct Node {\\n    /** Union used because a node can be either a LEAF node or a non-leaf node,\\n     * so both data fields are never used simultaneously */\\n    union {\\n      struct leaf {\\n        IndexType left, right; //!< Indices of points in leaf node\\n      } lr;\\n      struct nonleaf {\\n        int divfeat;                  //!< Dimension used for subdivision.\\n        DistanceType divlow, divhigh; //!< The values used for subdivision.\\n      } sub;\\n    } node_type;\\n    Node *child1, *child2; //!< Child nodes (both=NULL mean its a leaf node)\\n  };\\n\\n  typedef Node *NodePtr;\\n\\n  struct Interval {\\n    ElementType low, high;\\n  };\\n\\n  /**\\n   *  Array of indices to vectors in the dataset.\\n   */\\n  std::vector<IndexType> vind;\\n\\n  NodePtr root_node;\\n\\n  size_t m_leaf_max_size;\\n\\n  size_t m_size;                //!< Number of current points in the dataset\\n  size_t m_size_at_index_build; //!< Number of points in the dataset when the\\n                                //!< index was built\\n  int dim;                      //!< Dimensionality of each data point\\n\\n  /** Define \"BoundingBox\" as a fixed-size or variable-size container depending\\n   * on \"DIM\" */\\n  typedef\\n      typename array_or_vector_selector<DIM, Interval>::container_t BoundingBox;\\n\\n  /** Define \"distance_vector_t\" as a fixed-size or variable-size container\\n   * depending on \"DIM\" */\\n  typedef typename array_or_vector_selector<DIM, DistanceType>::container_t\\n      distance_vector_t;\\n\\n  /** The KD-tree used to find neighbours */\\n\\n  BoundingBox root_bbox;\\n\\n  /**\\n   * Pooled memory allocator.\\n   *\\n   * Using a pooled memory allocator is more efficient\\n   * than allocating memory directly when there is a large\\n   * number small of memory allocations.\\n   */\\n  PooledAllocator pool;\\n\\n  /** Returns number of points in dataset  */\\n  size_t size(const Derived &obj) const { return obj.m_size; }\\n\\n  /** Returns the length of each point in the dataset */\\n  size_t veclen(const Derived &obj) {\\n    return static_cast<size_t>(DIM > 0 ? DIM : obj.dim);\\n  }\\n\\n  /// Helper accessor to the dataset points:\\n  inline ElementType dataset_get(const Derived &obj, size_t idx,\\n                                 int component) const {\\n    return obj.dataset.kdtree_get_pt(idx, component);\\n  }\\n\\n  /**\\n   * Computes the inde memory usage\\n   * Returns: memory used by the index\\n   */\\n  size_t usedMemory(Derived &obj) {\\n    return obj.pool.usedMemory + obj.pool.wastedMemory +\\n           obj.dataset.kdtree_get_point_count() *\\n               sizeof(IndexType); // pool memory and vind array memory\\n  }\\n\\n  void computeMinMax(const Derived &obj, IndexType *ind, IndexType count,\\n                     int element, ElementType &min_elem,\\n                     ElementType &max_elem) {\\n    min_elem = dataset_get(obj, ind[0], element);\\n    max_elem = dataset_get(obj, ind[0], element);\\n    for (IndexType i = 1; i < count; ++i) {\\n      ElementType val = dataset_get(obj, ind[i], element);\\n      if (val < min_elem)\\n        min_elem = val;\\n      if (val > max_elem)\\n        max_elem = val;\\n    }\\n  }\\n\\n  /**\\n   * Create a tree node that subdivides the list of vecs from vind[first]\\n   * to vind[last].  The routine is called recursively on each sublist.\\n   *\\n   * @param left index of the first vector\\n   * @param right index of the last vector\\n   */\\n  NodePtr divideTree(Derived &obj, const IndexType left, const IndexType right,\\n                     BoundingBox &bbox) {\\n    NodePtr node = obj.pool.template allocate<Node>(); // allocate memory\\n\\n    /* If too few exemplars remain, then make this a leaf node. */\\n    if ((right - left) <= static_cast<IndexType>(obj.m_leaf_max_size)) {\\n      node->child1 = node->child2 = NULL; /* Mark as leaf node. */\\n      node->node_type.lr.left = left;\\n      node->node_type.lr.right = right;\\n\\n      // compute bounding-box of leaf points\\n      for (int i = 0; i < (DIM > 0 ? DIM : obj.dim); ++i) {\\n        bbox[i].low = dataset_get(obj, obj.vind[left], i);\\n        bbox[i].high = dataset_get(obj, obj.vind[left], i);\\n      }\\n      for (IndexType k = left + 1; k < right; ++k) {\\n        for (int i = 0; i < (DIM > 0 ? DIM : obj.dim); ++i) {\\n          if (bbox[i].low > dataset_get(obj, obj.vind[k], i))\\n            bbox[i].low = dataset_get(obj, obj.vind[k], i);\\n          if (bbox[i].high < dataset_get(obj, obj.vind[k], i))\\n            bbox[i].high = dataset_get(obj, obj.vind[k], i);\\n        }\\n      }\\n    } else {\\n      IndexType idx;\\n      int cutfeat;\\n      DistanceType cutval;\\n      middleSplit_(obj, &obj.vind[0] + left, right - left, idx, cutfeat, cutval,\\n                   bbox);\\n\\n      node->node_type.sub.divfeat = cutfeat;\\n\\n      BoundingBox left_bbox(bbox);\\n      left_bbox[cutfeat].high = cutval;\\n      node->child1 = divideTree(obj, left, left + idx, left_bbox);\\n\\n      BoundingBox right_bbox(bbox);\\n      right_bbox[cutfeat].low = cutval;\\n      node->child2 = divideTree(obj, left + idx, right, right_bbox);\\n\\n      node->node_type.sub.divlow = left_bbox[cutfeat].high;\\n      node->node_type.sub.divhigh = right_bbox[cutfeat].low;\\n\\n      for (int i = 0; i < (DIM > 0 ? DIM : obj.dim); ++i) {\\n        bbox[i].low = std::min(left_bbox[i].low, right_bbox[i].low);\\n        bbox[i].high = std::max(left_bbox[i].high, right_bbox[i].high);\\n      }\\n    }\\n\\n    return node;\\n  }\\n\\n  void middleSplit_(Derived &obj, IndexType *ind, IndexType count,\\n                    IndexType &index, int &cutfeat, DistanceType &cutval,\\n                    const BoundingBox &bbox) {\\n    const DistanceType EPS = static_cast<DistanceType>(0.00001);\\n    ElementType max_span = bbox[0].high - bbox[0].low;\\n    for (int i = 1; i < (DIM > 0 ? DIM : obj.dim); ++i) {\\n      ElementType span = bbox[i].high - bbox[i].low;\\n      if (span > max_span) {\\n        max_span = span;\\n      }\\n    }\\n    ElementType max_spread = -1;\\n    cutfeat = 0;\\n    for (int i = 0; i < (DIM > 0 ? DIM : obj.dim); ++i) {\\n      ElementType span = bbox[i].high - bbox[i].low;\\n      if (span > (1 - EPS) * max_span) {\\n        ElementType min_elem, max_elem;\\n        computeMinMax(obj, ind, count, i, min_elem, max_elem);\\n        ElementType spread = max_elem - min_elem;\\n        ;\\n        if (spread > max_spread) {\\n          cutfeat = i;\\n          max_spread = spread;\\n        }\\n      }\\n    }\\n    // split in the middle\\n    DistanceType split_val = (bbox[cutfeat].low + bbox[cutfeat].high) / 2;\\n    ElementType min_elem, max_elem;\\n    computeMinMax(obj, ind, count, cutfeat, min_elem, max_elem);\\n\\n    if (split_val < min_elem)\\n      cutval = min_elem;\\n    else if (split_val > max_elem)\\n      cutval = max_elem;\\n    else\\n      cutval = split_val;\\n\\n    IndexType lim1, lim2;\\n    planeSplit(obj, ind, count, cutfeat, cutval, lim1, lim2);\\n\\n    if (lim1 > count / 2)\\n      index = lim1;\\n    else if (lim2 < count / 2)\\n      index = lim2;\\n    else\\n      index = count / 2;\\n  }\\n\\n  /**\\n   *  Subdivide the list of points by a plane perpendicular on axe corresponding\\n   *  to the \\'cutfeat\\' dimension at \\'cutval\\' position.\\n   *\\n   *  On return:\\n   *  dataset[ind[0..lim1-1]][cutfeat]<cutval\\n   *  dataset[ind[lim1..lim2-1]][cutfeat]==cutval\\n   *  dataset[ind[lim2..count]][cutfeat]>cutval\\n   */\\n  void planeSplit(Derived &obj, IndexType *ind, const IndexType count,\\n                  int cutfeat, DistanceType &cutval, IndexType &lim1,\\n                  IndexType &lim2) {\\n    /* Move vector indices for left subtree to front of list. */\\n    IndexType left = 0;\\n    IndexType right = count - 1;\\n    for (;;) {\\n      while (left <= right && dataset_get(obj, ind[left], cutfeat) < cutval)\\n        ++left;\\n      while (right && left <= right &&\\n             dataset_get(obj, ind[right], cutfeat) >= cutval)\\n        --right;\\n      if (left > right || !right)\\n        break; // \"!right\" was added to support unsigned Index types\\n      std::swap(ind[left], ind[right]);\\n      ++left;\\n      --right;\\n    }\\n    /* If either list is empty, it means that all remaining features\\n     * are identical. Split in the middle to maintain a balanced tree.\\n     */\\n    lim1 = left;\\n    right = count - 1;\\n    for (;;) {\\n      while (left <= right && dataset_get(obj, ind[left], cutfeat) <= cutval)\\n        ++left;\\n      while (right && left <= right &&\\n             dataset_get(obj, ind[right], cutfeat) > cutval)\\n        --right;\\n      if (left > right || !right)\\n        break; // \"!right\" was added to support unsigned Index types\\n      std::swap(ind[left], ind[right]);\\n      ++left;\\n      --right;\\n    }\\n    lim2 = left;\\n  }\\n\\n  DistanceType computeInitialDistances(const Derived &obj,\\n                                       const ElementType *vec,\\n                                       distance_vector_t &dists) const {\\n    assert(vec);\\n    DistanceType distsq = DistanceType();\\n\\n    for (int i = 0; i < (DIM > 0 ? DIM : obj.dim); ++i) {\\n      if (vec[i] < obj.root_bbox[i].low) {\\n        dists[i] = obj.distance.accum_dist(vec[i], obj.root_bbox[i].low, i);\\n        distsq += dists[i];\\n      }\\n      if (vec[i] > obj.root_bbox[i].high) {\\n        dists[i] = obj.distance.accum_dist(vec[i], obj.root_bbox[i].high, i);\\n        distsq += dists[i];\\n      }\\n    }\\n    return distsq;\\n  }\\n\\n  void save_tree(Derived &obj, FILE *stream, NodePtr tree) {\\n    save_value(stream, *tree);\\n    if (tree->child1 != NULL) {\\n      save_tree(obj, stream, tree->child1);\\n    }\\n    if (tree->child2 != NULL) {\\n      save_tree(obj, stream, tree->child2);\\n    }\\n  }\\n\\n  void load_tree(Derived &obj, FILE *stream, NodePtr &tree) {\\n    tree = obj.pool.template allocate<Node>();\\n    load_value(stream, *tree);\\n    if (tree->child1 != NULL) {\\n      load_tree(obj, stream, tree->child1);\\n    }\\n    if (tree->child2 != NULL) {\\n      load_tree(obj, stream, tree->child2);\\n    }\\n  }\\n\\n  /**  Stores the index in a binary file.\\n   *   IMPORTANT NOTE: The set of data points is NOT stored in the file, so when\\n   * loading the index object it must be constructed associated to the same\\n   * source of data points used while building it. See the example:\\n   * examples/saveload_example.cpp \\\\sa loadIndex  */\\n  void saveIndex_(Derived &obj, FILE *stream) {\\n    save_value(stream, obj.m_size);\\n    save_value(stream, obj.dim);\\n    save_value(stream, obj.root_bbox);\\n    save_value(stream, obj.m_leaf_max_size);\\n    save_value(stream, obj.vind);\\n    save_tree(obj, stream, obj.root_node);\\n  }\\n\\n  /**  Loads a previous index from a binary file.\\n   *   IMPORTANT NOTE: The set of data points is NOT stored in the file, so the\\n   * index object must be constructed associated to the same source of data\\n   * points used while building the index. See the example:\\n   * examples/saveload_example.cpp \\\\sa loadIndex  */\\n  void loadIndex_(Derived &obj, FILE *stream) {\\n    load_value(stream, obj.m_size);\\n    load_value(stream, obj.dim);\\n    load_value(stream, obj.root_bbox);\\n    load_value(stream, obj.m_leaf_max_size);\\n    load_value(stream, obj.vind);\\n    load_tree(obj, stream, obj.root_node);\\n  }\\n};\\n\\n/** @addtogroup kdtrees_grp KD-tree classes and adaptors\\n * @{ */\\n\\n/** kd-tree static index\\n *\\n * Contains the k-d trees and other information for indexing a set of points\\n * for nearest-neighbor matching.\\n *\\n *  The class \"DatasetAdaptor\" must provide the following interface (can be\\n * non-virtual, inlined methods):\\n *\\n *  \\\\code\\n *   // Must return the number of data poins\\n *   inline size_t kdtree_get_point_count() const { ... }\\n *\\n *\\n *   // Must return the dim\\'th component of the idx\\'th point in the class:\\n *   inline T kdtree_get_pt(const size_t idx, const size_t dim) const { ... }\\n *\\n *   // Optional bounding-box computation: return false to default to a standard\\n * bbox computation loop.\\n *   //   Return true if the BBOX was already computed by the class and returned\\n * in \"bb\" so it can be avoided to redo it again.\\n *   //   Look at bb.size() to find out the expected dimensionality (e.g. 2 or 3\\n * for point clouds) template <class BBOX> bool kdtree_get_bbox(BBOX &bb) const\\n *   {\\n *      bb[0].low = ...; bb[0].high = ...;  // 0th dimension limits\\n *      bb[1].low = ...; bb[1].high = ...;  // 1st dimension limits\\n *      ...\\n *      return true;\\n *   }\\n *\\n *  \\\\endcode\\n *\\n * \\\\tparam DatasetAdaptor The user-provided adaptor (see comments above).\\n * \\\\tparam Distance The distance metric to use: nanoflann::metric_L1,\\n * nanoflann::metric_L2, nanoflann::metric_L2_Simple, etc. \\\\tparam DIM\\n * Dimensionality of data points (e.g. 3 for 3D points) \\\\tparam IndexType Will\\n * be typically size_t or int\\n */\\ntemplate <typename Distance, class DatasetAdaptor, int DIM = -1,\\n          typename IndexType = size_t>\\nclass KDTreeSingleIndexAdaptor\\n    : public KDTreeBaseClass<\\n          KDTreeSingleIndexAdaptor<Distance, DatasetAdaptor, DIM, IndexType>,\\n          Distance, DatasetAdaptor, DIM, IndexType> {\\npublic:\\n  /** Deleted copy constructor*/\\n  KDTreeSingleIndexAdaptor(\\n      const KDTreeSingleIndexAdaptor<Distance, DatasetAdaptor, DIM, IndexType>\\n          &) = delete;\\n\\n  /**\\n   * The dataset used by this index\\n   */\\n  const DatasetAdaptor &dataset; //!< The source of our data\\n\\n  const KDTreeSingleIndexAdaptorParams index_params;\\n\\n  Distance distance;\\n\\n  typedef typename nanoflann::KDTreeBaseClass<\\n      nanoflann::KDTreeSingleIndexAdaptor<Distance, DatasetAdaptor, DIM,\\n                                          IndexType>,\\n      Distance, DatasetAdaptor, DIM, IndexType>\\n      BaseClassRef;\\n\\n  typedef typename BaseClassRef::ElementType ElementType;\\n  typedef typename BaseClassRef::DistanceType DistanceType;\\n\\n  typedef typename BaseClassRef::Node Node;\\n  typedef Node *NodePtr;\\n\\n  typedef typename BaseClassRef::Interval Interval;\\n  /** Define \"BoundingBox\" as a fixed-size or variable-size container depending\\n   * on \"DIM\" */\\n  typedef typename BaseClassRef::BoundingBox BoundingBox;\\n\\n  /** Define \"distance_vector_t\" as a fixed-size or variable-size container\\n   * depending on \"DIM\" */\\n  typedef typename BaseClassRef::distance_vector_t distance_vector_t;\\n\\n  /**\\n   * KDTree constructor\\n   *\\n   * Refer to docs in README.md or online in\\n   * https://github.com/jlblancoc/nanoflann\\n   *\\n   * The KD-Tree point dimension (the length of each point in the datase, e.g. 3\\n   * for 3D points) is determined by means of:\\n   *  - The \\\\a DIM template parameter if >0 (highest priority)\\n   *  - Otherwise, the \\\\a dimensionality parameter of this constructor.\\n   *\\n   * @param inputData Dataset with the input features\\n   * @param params Basically, the maximum leaf node size\\n   */\\n  KDTreeSingleIndexAdaptor(const int dimensionality,\\n                           const DatasetAdaptor &inputData,\\n                           const KDTreeSingleIndexAdaptorParams &params =\\n                               KDTreeSingleIndexAdaptorParams())\\n      : dataset(inputData), index_params(params), distance(inputData) {\\n    BaseClassRef::root_node = NULL;\\n    BaseClassRef::m_size = dataset.kdtree_get_point_count();\\n    BaseClassRef::m_size_at_index_build = BaseClassRef::m_size;\\n    BaseClassRef::dim = dimensionality;\\n    if (DIM > 0)\\n      BaseClassRef::dim = DIM;\\n    BaseClassRef::m_leaf_max_size = params.leaf_max_size;\\n\\n    // Create a permutable array of indices to the input vectors.\\n    init_vind();\\n  }\\n\\n  /**\\n   * Builds the index\\n   */\\n  void buildIndex() {\\n    BaseClassRef::m_size = dataset.kdtree_get_point_count();\\n    BaseClassRef::m_size_at_index_build = BaseClassRef::m_size;\\n    init_vind();\\n    this->freeIndex(*this);\\n    BaseClassRef::m_size_at_index_build = BaseClassRef::m_size;\\n    if (BaseClassRef::m_size == 0)\\n      return;\\n    computeBoundingBox(BaseClassRef::root_bbox);\\n    BaseClassRef::root_node =\\n        this->divideTree(*this, 0, BaseClassRef::m_size,\\n                         BaseClassRef::root_bbox); // construct the tree\\n  }\\n\\n  /** \\\\name Query methods\\n   * @{ */\\n\\n  /**\\n   * Find set of nearest neighbors to vec[0:dim-1]. Their indices are stored\\n   * inside the result object.\\n   *\\n   * Params:\\n   *     result = the result object in which the indices of the\\n   * nearest-neighbors are stored vec = the vector for which to search the\\n   * nearest neighbors\\n   *\\n   * \\\\tparam RESULTSET Should be any ResultSet<DistanceType>\\n   * \\\\return  True if the requested neighbors could be found.\\n   * \\\\sa knnSearch, radiusSearch\\n   */\\n  template <typename RESULTSET>\\n  bool findNeighbors(RESULTSET &result, const ElementType *vec,\\n                     const SearchParams &searchParams) const {\\n    assert(vec);\\n    if (this->size(*this) == 0)\\n      return false;\\n    if (!BaseClassRef::root_node)\\n      throw std::runtime_error(\\n          \"[nanoflann] findNeighbors() called before building the index.\");\\n    float epsError = 1 + searchParams.eps;\\n\\n    distance_vector_t\\n        dists; // fixed or variable-sized container (depending on DIM)\\n    auto zero = static_cast<decltype(result.worstDist())>(0);\\n    assign(dists, (DIM > 0 ? DIM : BaseClassRef::dim),\\n           zero); // Fill it with zeros.\\n    DistanceType distsq = this->computeInitialDistances(*this, vec, dists);\\n\\n    searchLevel(result, vec, BaseClassRef::root_node, distsq, dists,\\n                epsError); // \"count_leaf\" parameter removed since was neither\\n                           // used nor returned to the user.\\n\\n    return result.full();\\n  }\\n\\n  /**\\n   * Find the \"num_closest\" nearest neighbors to the \\\\a query_point[0:dim-1].\\n   * Their indices are stored inside the result object. \\\\sa radiusSearch,\\n   * findNeighbors \\\\note nChecks_IGNORED is ignored but kept for compatibility\\n   * with the original FLANN interface. \\\\return Number `N` of valid points in\\n   * the result set. Only the first `N` entries in `out_indices` and\\n   * `out_distances_sq` will be valid. Return may be less than `num_closest`\\n   * only if the number of elements in the tree is less than `num_closest`.\\n   */\\n  size_t knnSearch(const ElementType *query_point, const size_t num_closest,\\n                   IndexType *out_indices, DistanceType *out_distances_sq,\\n                   const int /* nChecks_IGNORED */ = 10) const {\\n    nanoflann::KNNResultSet<DistanceType, IndexType> resultSet(num_closest);\\n    resultSet.init(out_indices, out_distances_sq);\\n    this->findNeighbors(resultSet, query_point, nanoflann::SearchParams());\\n    return resultSet.size();\\n  }\\n\\n  /**\\n   * Find all the neighbors to \\\\a query_point[0:dim-1] within a maximum radius.\\n   *  The output is given as a vector of pairs, of which the first element is a\\n   * point index and the second the corresponding distance. Previous contents of\\n   * \\\\a IndicesDists are cleared.\\n   *\\n   *  If searchParams.sorted==true, the output list is sorted by ascending\\n   * distances.\\n   *\\n   *  For a better performance, it is advisable to do a .reserve() on the vector\\n   * if you have any wild guess about the number of expected matches.\\n   *\\n   *  \\\\sa knnSearch, findNeighbors, radiusSearchCustomCallback\\n   * \\\\return The number of points within the given radius (i.e. indices.size()\\n   * or dists.size() )\\n   */\\n  size_t\\n  radiusSearch(const ElementType *query_point, const DistanceType &radius,\\n               std::vector<std::pair<IndexType, DistanceType>> &IndicesDists,\\n               const SearchParams &searchParams) const {\\n    RadiusResultSet<DistanceType, IndexType> resultSet(radius, IndicesDists);\\n    const size_t nFound =\\n        radiusSearchCustomCallback(query_point, resultSet, searchParams);\\n    if (searchParams.sorted)\\n      std::sort(IndicesDists.begin(), IndicesDists.end(), IndexDist_Sorter());\\n    return nFound;\\n  }\\n\\n  /**\\n   * Just like radiusSearch() but with a custom callback class for each point\\n   * found in the radius of the query. See the source of RadiusResultSet<> as a\\n   * start point for your own classes. \\\\sa radiusSearch\\n   */\\n  template <class SEARCH_CALLBACK>\\n  size_t radiusSearchCustomCallback(\\n      const ElementType *query_point, SEARCH_CALLBACK &resultSet,\\n      const SearchParams &searchParams = SearchParams()) const {\\n    this->findNeighbors(resultSet, query_point, searchParams);\\n    return resultSet.size();\\n  }\\n\\n  /** @} */\\n\\npublic:\\n  /** Make sure the auxiliary list \\\\a vind has the same size than the current\\n   * dataset, and re-generate if size has changed. */\\n  void init_vind() {\\n    // Create a permutable array of indices to the input vectors.\\n    BaseClassRef::m_size = dataset.kdtree_get_point_count();\\n    if (BaseClassRef::vind.size() != BaseClassRef::m_size)\\n      BaseClassRef::vind.resize(BaseClassRef::m_size);\\n    for (size_t i = 0; i < BaseClassRef::m_size; i++)\\n      BaseClassRef::vind[i] = i;\\n  }\\n\\n  void computeBoundingBox(BoundingBox &bbox) {\\n    resize(bbox, (DIM > 0 ? DIM : BaseClassRef::dim));\\n    if (dataset.kdtree_get_bbox(bbox)) {\\n      // Done! It was implemented in derived class\\n    } else {\\n      const size_t N = dataset.kdtree_get_point_count();\\n      if (!N)\\n        throw std::runtime_error(\"[nanoflann] computeBoundingBox() called but \"\\n                                 \"no data points found.\");\\n      for (int i = 0; i < (DIM > 0 ? DIM : BaseClassRef::dim); ++i) {\\n        bbox[i].low = bbox[i].high = this->dataset_get(*this, 0, i);\\n      }\\n      for (size_t k = 1; k < N; ++k) {\\n        for (int i = 0; i < (DIM > 0 ? DIM : BaseClassRef::dim); ++i) {\\n          if (this->dataset_get(*this, k, i) < bbox[i].low)\\n            bbox[i].low = this->dataset_get(*this, k, i);\\n          if (this->dataset_get(*this, k, i) > bbox[i].high)\\n            bbox[i].high = this->dataset_get(*this, k, i);\\n        }\\n      }\\n    }\\n  }\\n\\n  /**\\n   * Performs an exact search in the tree starting from a node.\\n   * \\\\tparam RESULTSET Should be any ResultSet<DistanceType>\\n   * \\\\return true if the search should be continued, false if the results are\\n   * sufficient\\n   */\\n  template <class RESULTSET>\\n  bool searchLevel(RESULTSET &result_set, const ElementType *vec,\\n                   const NodePtr node, DistanceType mindistsq,\\n                   distance_vector_t &dists, const float epsError) const {\\n    /* If this is a leaf node, then do check and return. */\\n    if ((node->child1 == NULL) && (node->child2 == NULL)) {\\n      // count_leaf += (node->lr.right-node->lr.left);  // Removed since was\\n      // neither used nor returned to the user.\\n      DistanceType worst_dist = result_set.worstDist();\\n      for (IndexType i = node->node_type.lr.left; i < node->node_type.lr.right;\\n           ++i) {\\n        const IndexType index = BaseClassRef::vind[i]; // reorder... : i;\\n        DistanceType dist = distance.evalMetric(\\n            vec, index, (DIM > 0 ? DIM : BaseClassRef::dim));\\n        if (dist < worst_dist) {\\n          if (!result_set.addPoint(dist, BaseClassRef::vind[i])) {\\n            // the resultset doesn\\'t want to receive any more points, we\\'re done\\n            // searching!\\n            return false;\\n          }\\n        }\\n      }\\n      return true;\\n    }\\n\\n    /* Which child branch should be taken first? */\\n    int idx = node->node_type.sub.divfeat;\\n    ElementType val = vec[idx];\\n    DistanceType diff1 = val - node->node_type.sub.divlow;\\n    DistanceType diff2 = val - node->node_type.sub.divhigh;\\n\\n    NodePtr bestChild;\\n    NodePtr otherChild;\\n    DistanceType cut_dist;\\n    if ((diff1 + diff2) < 0) {\\n      bestChild = node->child1;\\n      otherChild = node->child2;\\n      cut_dist = distance.accum_dist(val, node->node_type.sub.divhigh, idx);\\n    } else {\\n      bestChild = node->child2;\\n      otherChild = node->child1;\\n      cut_dist = distance.accum_dist(val, node->node_type.sub.divlow, idx);\\n    }\\n\\n    /* Call recursively to search next level down. */\\n    if (!searchLevel(result_set, vec, bestChild, mindistsq, dists, epsError)) {\\n      // the resultset doesn\\'t want to receive any more points, we\\'re done\\n      // searching!\\n      return false;\\n    }\\n\\n    DistanceType dst = dists[idx];\\n    mindistsq = mindistsq + cut_dist - dst;\\n    dists[idx] = cut_dist;\\n    if (mindistsq * epsError <= result_set.worstDist()) {\\n      if (!searchLevel(result_set, vec, otherChild, mindistsq, dists,\\n                       epsError)) {\\n        // the resultset doesn\\'t want to receive any more points, we\\'re done\\n        // searching!\\n        return false;\\n      }\\n    }\\n    dists[idx] = dst;\\n    return true;\\n  }\\n\\npublic:\\n  /**  Stores the index in a binary file.\\n   *   IMPORTANT NOTE: The set of data points is NOT stored in the file, so when\\n   * loading the index object it must be constructed associated to the same\\n   * source of data points used while building it. See the example:\\n   * examples/saveload_example.cpp \\\\sa loadIndex  */\\n  void saveIndex(FILE *stream) { this->saveIndex_(*this, stream); }\\n\\n  /**  Loads a previous index from a binary file.\\n   *   IMPORTANT NOTE: The set of data points is NOT stored in the file, so the\\n   * index object must be constructed associated to the same source of data\\n   * points used while building the index. See the example:\\n   * examples/saveload_example.cpp \\\\sa loadIndex  */\\n  void loadIndex(FILE *stream) { this->loadIndex_(*this, stream); }\\n\\n}; // class KDTree\\n\\n/** kd-tree dynamic index\\n *\\n * Contains the k-d trees and other information for indexing a set of points\\n * for nearest-neighbor matching.\\n *\\n *  The class \"DatasetAdaptor\" must provide the following interface (can be\\n * non-virtual, inlined methods):\\n *\\n *  \\\\code\\n *   // Must return the number of data poins\\n *   inline size_t kdtree_get_point_count() const { ... }\\n *\\n *   // Must return the dim\\'th component of the idx\\'th point in the class:\\n *   inline T kdtree_get_pt(const size_t idx, const size_t dim) const { ... }\\n *\\n *   // Optional bounding-box computation: return false to default to a standard\\n * bbox computation loop.\\n *   //   Return true if the BBOX was already computed by the class and returned\\n * in \"bb\" so it can be avoided to redo it again.\\n *   //   Look at bb.size() to find out the expected dimensionality (e.g. 2 or 3\\n * for point clouds) template <class BBOX> bool kdtree_get_bbox(BBOX &bb) const\\n *   {\\n *      bb[0].low = ...; bb[0].high = ...;  // 0th dimension limits\\n *      bb[1].low = ...; bb[1].high = ...;  // 1st dimension limits\\n *      ...\\n *      return true;\\n *   }\\n *\\n *  \\\\endcode\\n *\\n * \\\\tparam DatasetAdaptor The user-provided adaptor (see comments above).\\n * \\\\tparam Distance The distance metric to use: nanoflann::metric_L1,\\n * nanoflann::metric_L2, nanoflann::metric_L2_Simple, etc. \\\\tparam DIM\\n * Dimensionality of data points (e.g. 3 for 3D points) \\\\tparam IndexType Will\\n * be typically size_t or int\\n */\\ntemplate <typename Distance, class DatasetAdaptor, int DIM = -1,\\n          typename IndexType = size_t>\\nclass KDTreeSingleIndexDynamicAdaptor_\\n    : public KDTreeBaseClass<KDTreeSingleIndexDynamicAdaptor_<\\n                                 Distance, DatasetAdaptor, DIM, IndexType>,\\n                             Distance, DatasetAdaptor, DIM, IndexType> {\\npublic:\\n  /**\\n   * The dataset used by this index\\n   */\\n  const DatasetAdaptor &dataset; //!< The source of our data\\n\\n  KDTreeSingleIndexAdaptorParams index_params;\\n\\n  std::vector<int> &treeIndex;\\n\\n  Distance distance;\\n\\n  typedef typename nanoflann::KDTreeBaseClass<\\n      nanoflann::KDTreeSingleIndexDynamicAdaptor_<Distance, DatasetAdaptor, DIM,\\n                                                  IndexType>,\\n      Distance, DatasetAdaptor, DIM, IndexType>\\n      BaseClassRef;\\n\\n  typedef typename BaseClassRef::ElementType ElementType;\\n  typedef typename BaseClassRef::DistanceType DistanceType;\\n\\n  typedef typename BaseClassRef::Node Node;\\n  typedef Node *NodePtr;\\n\\n  typedef typename BaseClassRef::Interval Interval;\\n  /** Define \"BoundingBox\" as a fixed-size or variable-size container depending\\n   * on \"DIM\" */\\n  typedef typename BaseClassRef::BoundingBox BoundingBox;\\n\\n  /** Define \"distance_vector_t\" as a fixed-size or variable-size container\\n   * depending on \"DIM\" */\\n  typedef typename BaseClassRef::distance_vector_t distance_vector_t;\\n\\n  /**\\n   * KDTree constructor\\n   *\\n   * Refer to docs in README.md or online in\\n   * https://github.com/jlblancoc/nanoflann\\n   *\\n   * The KD-Tree point dimension (the length of each point in the datase, e.g. 3\\n   * for 3D points) is determined by means of:\\n   *  - The \\\\a DIM template parameter if >0 (highest priority)\\n   *  - Otherwise, the \\\\a dimensionality parameter of this constructor.\\n   *\\n   * @param inputData Dataset with the input features\\n   * @param params Basically, the maximum leaf node size\\n   */\\n  KDTreeSingleIndexDynamicAdaptor_(\\n      const int dimensionality, const DatasetAdaptor &inputData,\\n      std::vector<int> &treeIndex_,\\n      const KDTreeSingleIndexAdaptorParams &params =\\n          KDTreeSingleIndexAdaptorParams())\\n      : dataset(inputData), index_params(params), treeIndex(treeIndex_),\\n        distance(inputData) {\\n    BaseClassRef::root_node = NULL;\\n    BaseClassRef::m_size = 0;\\n    BaseClassRef::m_size_at_index_build = 0;\\n    BaseClassRef::dim = dimensionality;\\n    if (DIM > 0)\\n      BaseClassRef::dim = DIM;\\n    BaseClassRef::m_leaf_max_size = params.leaf_max_size;\\n  }\\n\\n  /** Assignment operator definiton */\\n  KDTreeSingleIndexDynamicAdaptor_\\n  operator=(const KDTreeSingleIndexDynamicAdaptor_ &rhs) {\\n    KDTreeSingleIndexDynamicAdaptor_ tmp(rhs);\\n    std::swap(BaseClassRef::vind, tmp.BaseClassRef::vind);\\n    std::swap(BaseClassRef::m_leaf_max_size, tmp.BaseClassRef::m_leaf_max_size);\\n    std::swap(index_params, tmp.index_params);\\n    std::swap(treeIndex, tmp.treeIndex);\\n    std::swap(BaseClassRef::m_size, tmp.BaseClassRef::m_size);\\n    std::swap(BaseClassRef::m_size_at_index_build,\\n              tmp.BaseClassRef::m_size_at_index_build);\\n    std::swap(BaseClassRef::root_node, tmp.BaseClassRef::root_node);\\n    std::swap(BaseClassRef::root_bbox, tmp.BaseClassRef::root_bbox);\\n    std::swap(BaseClassRef::pool, tmp.BaseClassRef::pool);\\n    return *this;\\n  }\\n\\n  /**\\n   * Builds the index\\n   */\\n  void buildIndex() {\\n    BaseClassRef::m_size = BaseClassRef::vind.size();\\n    this->freeIndex(*this);\\n    BaseClassRef::m_size_at_index_build = BaseClassRef::m_size;\\n    if (BaseClassRef::m_size == 0)\\n      return;\\n    computeBoundingBox(BaseClassRef::root_bbox);\\n    BaseClassRef::root_node =\\n        this->divideTree(*this, 0, BaseClassRef::m_size,\\n                         BaseClassRef::root_bbox); // construct the tree\\n  }\\n\\n  /** \\\\name Query methods\\n   * @{ */\\n\\n  /**\\n   * Find set of nearest neighbors to vec[0:dim-1]. Their indices are stored\\n   * inside the result object.\\n   *\\n   * Params:\\n   *     result = the result object in which the indices of the\\n   * nearest-neighbors are stored vec = the vector for which to search the\\n   * nearest neighbors\\n   *\\n   * \\\\tparam RESULTSET Should be any ResultSet<DistanceType>\\n   * \\\\return  True if the requested neighbors could be found.\\n   * \\\\sa knnSearch, radiusSearch\\n   */\\n  template <typename RESULTSET>\\n  bool findNeighbors(RESULTSET &result, const ElementType *vec,\\n                     const SearchParams &searchParams) const {\\n    assert(vec);\\n    if (this->size(*this) == 0)\\n      return false;\\n    if (!BaseClassRef::root_node)\\n      return false;\\n    float epsError = 1 + searchParams.eps;\\n\\n    // fixed or variable-sized container (depending on DIM)\\n    distance_vector_t dists;\\n    // Fill it with zeros.\\n    assign(dists, (DIM > 0 ? DIM : BaseClassRef::dim),\\n           static_cast<typename distance_vector_t::value_type>(0));\\n    DistanceType distsq = this->computeInitialDistances(*this, vec, dists);\\n\\n    searchLevel(result, vec, BaseClassRef::root_node, distsq, dists,\\n                epsError); // \"count_leaf\" parameter removed since was neither\\n                           // used nor returned to the user.\\n\\n    return result.full();\\n  }\\n\\n  /**\\n   * Find the \"num_closest\" nearest neighbors to the \\\\a query_point[0:dim-1].\\n   * Their indices are stored inside the result object. \\\\sa radiusSearch,\\n   * findNeighbors \\\\note nChecks_IGNORED is ignored but kept for compatibility\\n   * with the original FLANN interface. \\\\return Number `N` of valid points in\\n   * the result set. Only the first `N` entries in `out_indices` and\\n   * `out_distances_sq` will be valid. Return may be less than `num_closest`\\n   * only if the number of elements in the tree is less than `num_closest`.\\n   */\\n  size_t knnSearch(const ElementType *query_point, const size_t num_closest,\\n                   IndexType *out_indices, DistanceType *out_distances_sq,\\n                   const int /* nChecks_IGNORED */ = 10) const {\\n    nanoflann::KNNResultSet<DistanceType, IndexType> resultSet(num_closest);\\n    resultSet.init(out_indices, out_distances_sq);\\n    this->findNeighbors(resultSet, query_point, nanoflann::SearchParams());\\n    return resultSet.size();\\n  }\\n\\n  /**\\n   * Find all the neighbors to \\\\a query_point[0:dim-1] within a maximum radius.\\n   *  The output is given as a vector of pairs, of which the first element is a\\n   * point index and the second the corresponding distance. Previous contents of\\n   * \\\\a IndicesDists are cleared.\\n   *\\n   *  If searchParams.sorted==true, the output list is sorted by ascending\\n   * distances.\\n   *\\n   *  For a better performance, it is advisable to do a .reserve() on the vector\\n   * if you have any wild guess about the number of expected matches.\\n   *\\n   *  \\\\sa knnSearch, findNeighbors, radiusSearchCustomCallback\\n   * \\\\return The number of points within the given radius (i.e. indices.size()\\n   * or dists.size() )\\n   */\\n  size_t\\n  radiusSearch(const ElementType *query_point, const DistanceType &radius,\\n               std::vector<std::pair<IndexType, DistanceType>> &IndicesDists,\\n               const SearchParams &searchParams) const {\\n    RadiusResultSet<DistanceType, IndexType> resultSet(radius, IndicesDists);\\n    const size_t nFound =\\n        radiusSearchCustomCallback(query_point, resultSet, searchParams);\\n    if (searchParams.sorted)\\n      std::sort(IndicesDists.begin(), IndicesDists.end(), IndexDist_Sorter());\\n    return nFound;\\n  }\\n\\n  /**\\n   * Just like radiusSearch() but with a custom callback class for each point\\n   * found in the radius of the query. See the source of RadiusResultSet<> as a\\n   * start point for your own classes. \\\\sa radiusSearch\\n   */\\n  template <class SEARCH_CALLBACK>\\n  size_t radiusSearchCustomCallback(\\n      const ElementType *query_point, SEARCH_CALLBACK &resultSet,\\n      const SearchParams &searchParams = SearchParams()) const {\\n    this->findNeighbors(resultSet, query_point, searchParams);\\n    return resultSet.size();\\n  }\\n\\n  /** @} */\\n\\npublic:\\n  void computeBoundingBox(BoundingBox &bbox) {\\n    resize(bbox, (DIM > 0 ? DIM : BaseClassRef::dim));\\n\\n    if (dataset.kdtree_get_bbox(bbox)) {\\n      // Done! It was implemented in derived class\\n    } else {\\n      const size_t N = BaseClassRef::m_size;\\n      if (!N)\\n        throw std::runtime_error(\"[nanoflann] computeBoundingBox() called but \"\\n                                 \"no data points found.\");\\n      for (int i = 0; i < (DIM > 0 ? DIM : BaseClassRef::dim); ++i) {\\n        bbox[i].low = bbox[i].high =\\n            this->dataset_get(*this, BaseClassRef::vind[0], i);\\n      }\\n      for (size_t k = 1; k < N; ++k) {\\n        for (int i = 0; i < (DIM > 0 ? DIM : BaseClassRef::dim); ++i) {\\n          if (this->dataset_get(*this, BaseClassRef::vind[k], i) < bbox[i].low)\\n            bbox[i].low = this->dataset_get(*this, BaseClassRef::vind[k], i);\\n          if (this->dataset_get(*this, BaseClassRef::vind[k], i) > bbox[i].high)\\n            bbox[i].high = this->dataset_get(*this, BaseClassRef::vind[k], i);\\n        }\\n      }\\n    }\\n  }\\n\\n  /**\\n   * Performs an exact search in the tree starting from a node.\\n   * \\\\tparam RESULTSET Should be any ResultSet<DistanceType>\\n   */\\n  template <class RESULTSET>\\n  void searchLevel(RESULTSET &result_set, const ElementType *vec,\\n                   const NodePtr node, DistanceType mindistsq,\\n                   distance_vector_t &dists, const float epsError) const {\\n    /* If this is a leaf node, then do check and return. */\\n    if ((node->child1 == NULL) && (node->child2 == NULL)) {\\n      // count_leaf += (node->lr.right-node->lr.left);  // Removed since was\\n      // neither used nor returned to the user.\\n      DistanceType worst_dist = result_set.worstDist();\\n      for (IndexType i = node->node_type.lr.left; i < node->node_type.lr.right;\\n           ++i) {\\n        const IndexType index = BaseClassRef::vind[i]; // reorder... : i;\\n        if (treeIndex[index] == -1)\\n          continue;\\n        DistanceType dist = distance.evalMetric(\\n            vec, index, (DIM > 0 ? DIM : BaseClassRef::dim));\\n        if (dist < worst_dist) {\\n          if (!result_set.addPoint(\\n                  static_cast<typename RESULTSET::DistanceType>(dist),\\n                  static_cast<typename RESULTSET::IndexType>(\\n                      BaseClassRef::vind[i]))) {\\n            // the resultset doesn\\'t want to receive any more points, we\\'re done\\n            // searching!\\n            return; // false;\\n          }\\n        }\\n      }\\n      return;\\n    }\\n\\n    /* Which child branch should be taken first? */\\n    int idx = node->node_type.sub.divfeat;\\n    ElementType val = vec[idx];\\n    DistanceType diff1 = val - node->node_type.sub.divlow;\\n    DistanceType diff2 = val - node->node_type.sub.divhigh;\\n\\n    NodePtr bestChild;\\n    NodePtr otherChild;\\n    DistanceType cut_dist;\\n    if ((diff1 + diff2) < 0) {\\n      bestChild = node->child1;\\n      otherChild = node->child2;\\n      cut_dist = distance.accum_dist(val, node->node_type.sub.divhigh, idx);\\n    } else {\\n      bestChild = node->child2;\\n      otherChild = node->child1;\\n      cut_dist = distance.accum_dist(val, node->node_type.sub.divlow, idx);\\n    }\\n\\n    /* Call recursively to search next level down. */\\n    searchLevel(result_set, vec, bestChild, mindistsq, dists, epsError);\\n\\n    DistanceType dst = dists[idx];\\n    mindistsq = mindistsq + cut_dist - dst;\\n    dists[idx] = cut_dist;\\n    if (mindistsq * epsError <= result_set.worstDist()) {\\n      searchLevel(result_set, vec, otherChild, mindistsq, dists, epsError);\\n    }\\n    dists[idx] = dst;\\n  }\\n\\npublic:\\n  /**  Stores the index in a binary file.\\n   *   IMPORTANT NOTE: The set of data points is NOT stored in the file, so when\\n   * loading the index object it must be constructed associated to the same\\n   * source of data points used while building it. See the example:\\n   * examples/saveload_example.cpp \\\\sa loadIndex  */\\n  void saveIndex(FILE *stream) { this->saveIndex_(*this, stream); }\\n\\n  /**  Loads a previous index from a binary file.\\n   *   IMPORTANT NOTE: The set of data points is NOT stored in the file, so the\\n   * index object must be constructed associated to the same source of data\\n   * points used while building the index. See the example:\\n   * examples/saveload_example.cpp \\\\sa loadIndex  */\\n  void loadIndex(FILE *stream) { this->loadIndex_(*this, stream); }\\n};\\n\\n/** kd-tree dynaimic index\\n *\\n * class to create multiple static index and merge their results to behave as\\n * single dynamic index as proposed in Logarithmic Approach.\\n *\\n *  Example of usage:\\n *  examples/dynamic_pointcloud_example.cpp\\n *\\n * \\\\tparam DatasetAdaptor The user-provided adaptor (see comments above).\\n * \\\\tparam Distance The distance metric to use: nanoflann::metric_L1,\\n * nanoflann::metric_L2, nanoflann::metric_L2_Simple, etc. \\\\tparam DIM\\n * Dimensionality of data points (e.g. 3 for 3D points) \\\\tparam IndexType Will\\n * be typically size_t or int\\n */\\ntemplate <typename Distance, class DatasetAdaptor, int DIM = -1,\\n          typename IndexType = size_t>\\nclass KDTreeSingleIndexDynamicAdaptor {\\npublic:\\n  typedef typename Distance::ElementType ElementType;\\n  typedef typename Distance::DistanceType DistanceType;\\n\\nprotected:\\n  size_t m_leaf_max_size;\\n  size_t treeCount;\\n  size_t pointCount;\\n\\n  /**\\n   * The dataset used by this index\\n   */\\n  const DatasetAdaptor &dataset; //!< The source of our data\\n\\n  std::vector<int> treeIndex; //!< treeIndex[idx] is the index of tree in which\\n                              //!< point at idx is stored. treeIndex[idx]=-1\\n                              //!< means that point has been removed.\\n\\n  KDTreeSingleIndexAdaptorParams index_params;\\n\\n  int dim; //!< Dimensionality of each data point\\n\\n  typedef KDTreeSingleIndexDynamicAdaptor_<Distance, DatasetAdaptor, DIM>\\n      index_container_t;\\n  std::vector<index_container_t> index;\\n\\npublic:\\n  /** Get a const ref to the internal list of indices; the number of indices is\\n   * adapted dynamically as the dataset grows in size. */\\n  const std::vector<index_container_t> &getAllIndices() const { return index; }\\n\\nprivate:\\n  /** finds position of least significant unset bit */\\n  int First0Bit(IndexType num) {\\n    int pos = 0;\\n    while (num & 1) {\\n      num = num >> 1;\\n      pos++;\\n    }\\n    return pos;\\n  }\\n\\n  /** Creates multiple empty trees to handle dynamic support */\\n  void init() {\\n    typedef KDTreeSingleIndexDynamicAdaptor_<Distance, DatasetAdaptor, DIM>\\n        my_kd_tree_t;\\n    std::vector<my_kd_tree_t> index_(\\n        treeCount, my_kd_tree_t(dim /*dim*/, dataset, treeIndex, index_params));\\n    index = index_;\\n  }\\n\\npublic:\\n  Distance distance;\\n\\n  /**\\n   * KDTree constructor\\n   *\\n   * Refer to docs in README.md or online in\\n   * https://github.com/jlblancoc/nanoflann\\n   *\\n   * The KD-Tree point dimension (the length of each point in the datase, e.g. 3\\n   * for 3D points) is determined by means of:\\n   *  - The \\\\a DIM template parameter if >0 (highest priority)\\n   *  - Otherwise, the \\\\a dimensionality parameter of this constructor.\\n   *\\n   * @param inputData Dataset with the input features\\n   * @param params Basically, the maximum leaf node size\\n   */\\n  KDTreeSingleIndexDynamicAdaptor(const int dimensionality,\\n                                  const DatasetAdaptor &inputData,\\n                                  const KDTreeSingleIndexAdaptorParams &params =\\n                                      KDTreeSingleIndexAdaptorParams(),\\n                                  const size_t maximumPointCount = 1000000000U)\\n      : dataset(inputData), index_params(params), distance(inputData) {\\n    treeCount = static_cast<size_t>(std::log2(maximumPointCount));\\n    pointCount = 0U;\\n    dim = dimensionality;\\n    treeIndex.clear();\\n    if (DIM > 0)\\n      dim = DIM;\\n    m_leaf_max_size = params.leaf_max_size;\\n    init();\\n    const size_t num_initial_points = dataset.kdtree_get_point_count();\\n    if (num_initial_points > 0) {\\n      addPoints(0, num_initial_points - 1);\\n    }\\n  }\\n\\n  /** Deleted copy constructor*/\\n  KDTreeSingleIndexDynamicAdaptor(\\n      const KDTreeSingleIndexDynamicAdaptor<Distance, DatasetAdaptor, DIM,\\n                                            IndexType> &) = delete;\\n\\n  /** Add points to the set, Inserts all points from [start, end] */\\n  void addPoints(IndexType start, IndexType end) {\\n    size_t count = end - start + 1;\\n    treeIndex.resize(treeIndex.size() + count);\\n    for (IndexType idx = start; idx <= end; idx++) {\\n      int pos = First0Bit(pointCount);\\n      index[pos].vind.clear();\\n      treeIndex[pointCount] = pos;\\n      for (int i = 0; i < pos; i++) {\\n        for (int j = 0; j < static_cast<int>(index[i].vind.size()); j++) {\\n          index[pos].vind.push_back(index[i].vind[j]);\\n          if (treeIndex[index[i].vind[j]] != -1)\\n            treeIndex[index[i].vind[j]] = pos;\\n        }\\n        index[i].vind.clear();\\n        index[i].freeIndex(index[i]);\\n      }\\n      index[pos].vind.push_back(idx);\\n      index[pos].buildIndex();\\n      pointCount++;\\n    }\\n  }\\n\\n  /** Remove a point from the set (Lazy Deletion) */\\n  void removePoint(size_t idx) {\\n    if (idx >= pointCount)\\n      return;\\n    treeIndex[idx] = -1;\\n  }\\n\\n  /**\\n   * Find set of nearest neighbors to vec[0:dim-1]. Their indices are stored\\n   * inside the result object.\\n   *\\n   * Params:\\n   *     result = the result object in which the indices of the\\n   * nearest-neighbors are stored vec = the vector for which to search the\\n   * nearest neighbors\\n   *\\n   * \\\\tparam RESULTSET Should be any ResultSet<DistanceType>\\n   * \\\\return  True if the requested neighbors could be found.\\n   * \\\\sa knnSearch, radiusSearch\\n   */\\n  template <typename RESULTSET>\\n  bool findNeighbors(RESULTSET &result, const ElementType *vec,\\n                     const SearchParams &searchParams) const {\\n    for (size_t i = 0; i < treeCount; i++) {\\n      index[i].findNeighbors(result, &vec[0], searchParams);\\n    }\\n    return result.full();\\n  }\\n};\\n\\n/** An L2-metric KD-tree adaptor for working with data directly stored in an\\n * Eigen Matrix, without duplicating the data storage. Each row in the matrix\\n * represents a point in the state space.\\n *\\n *  Example of usage:\\n * \\\\code\\n * \\tEigen::Matrix<num_t,Dynamic,Dynamic>  mat;\\n * \\t// Fill out \"mat\"...\\n *\\n * \\ttypedef KDTreeEigenMatrixAdaptor< Eigen::Matrix<num_t,Dynamic,Dynamic> >\\n * my_kd_tree_t; const int max_leaf = 10; my_kd_tree_t   mat_index(mat, max_leaf\\n * ); mat_index.index->buildIndex(); mat_index.index->... \\\\endcode\\n *\\n *  \\\\tparam DIM If set to >0, it specifies a compile-time fixed dimensionality\\n * for the points in the data set, allowing more compiler optimizations. \\\\tparam\\n * Distance The distance metric to use: nanoflann::metric_L1,\\n * nanoflann::metric_L2, nanoflann::metric_L2_Simple, etc.\\n */\\ntemplate <class MatrixType, int DIM = -1, class Distance = nanoflann::metric_L2>\\nstruct KDTreeEigenMatrixAdaptor {\\n  typedef KDTreeEigenMatrixAdaptor<MatrixType, DIM, Distance> self_t;\\n  typedef typename MatrixType::Scalar num_t;\\n  typedef typename MatrixType::Index IndexType;\\n  typedef\\n      typename Distance::template traits<num_t, self_t>::distance_t metric_t;\\n  typedef KDTreeSingleIndexAdaptor<metric_t, self_t,\\n                                   MatrixType::ColsAtCompileTime, IndexType>\\n      index_t;\\n\\n  index_t *index; //! The kd-tree index for the user to call its methods as\\n                  //! usual with any other FLANN index.\\n\\n  /// Constructor: takes a const ref to the matrix object with the data points\\n  KDTreeEigenMatrixAdaptor(const size_t dimensionality,\\n                           const std::reference_wrapper<const MatrixType> &mat,\\n                           const int leaf_max_size = 10)\\n      : m_data_matrix(mat) {\\n    const auto dims = mat.get().cols();\\n    if (size_t(dims) != dimensionality)\\n      throw std::runtime_error(\\n          \"Error: \\'dimensionality\\' must match column count in data matrix\");\\n    if (DIM > 0 && int(dims) != DIM)\\n      throw std::runtime_error(\\n          \"Data set dimensionality does not match the \\'DIM\\' template argument\");\\n    index =\\n        new index_t(static_cast<int>(dims), *this /* adaptor */,\\n                    nanoflann::KDTreeSingleIndexAdaptorParams(leaf_max_size));\\n    index->buildIndex();\\n  }\\n\\npublic:\\n  /** Deleted copy constructor */\\n  KDTreeEigenMatrixAdaptor(const self_t &) = delete;\\n\\n  ~KDTreeEigenMatrixAdaptor() { delete index; }\\n\\n  const std::reference_wrapper<const MatrixType> m_data_matrix;\\n\\n  /** Query for the \\\\a num_closest closest points to a given point (entered as\\n   * query_point[0:dim-1]). Note that this is a short-cut method for\\n   * index->findNeighbors(). The user can also call index->... methods as\\n   * desired. \\\\note nChecks_IGNORED is ignored but kept for compatibility with\\n   * the original FLANN interface.\\n   */\\n  inline void query(const num_t *query_point, const size_t num_closest,\\n                    IndexType *out_indices, num_t *out_distances_sq,\\n                    const int /* nChecks_IGNORED */ = 10) const {\\n    nanoflann::KNNResultSet<num_t, IndexType> resultSet(num_closest);\\n    resultSet.init(out_indices, out_distances_sq);\\n    index->findNeighbors(resultSet, query_point, nanoflann::SearchParams());\\n  }\\n\\n  /** @name Interface expected by KDTreeSingleIndexAdaptor\\n   * @{ */\\n\\n  const self_t &derived() const { return *this; }\\n  self_t &derived() { return *this; }\\n\\n  // Must return the number of data points\\n  inline size_t kdtree_get_point_count() const {\\n    return m_data_matrix.get().rows();\\n  }\\n\\n  // Returns the dim\\'th component of the idx\\'th point in the class:\\n  inline num_t kdtree_get_pt(const IndexType idx, size_t dim) const {\\n    return m_data_matrix.get().coeff(idx, IndexType(dim));\\n  }\\n\\n  // Optional bounding-box computation: return false to default to a standard\\n  // bbox computation loop.\\n  //   Return true if the BBOX was already computed by the class and returned in\\n  //   \"bb\" so it can be avoided to redo it again. Look at bb.size() to find out\\n  //   the expected dimensionality (e.g. 2 or 3 for point clouds)\\n  template <class BBOX> bool kdtree_get_bbox(BBOX & /*bb*/) const {\\n    return false;\\n  }\\n\\n  /** @} */\\n\\n}; // end of KDTreeEigenMatrixAdaptor\\n   /** @} */\\n\\n/** @} */ // end of grouping\\n} // namespace nanoflann\\n\\n#endif /* NANOFLANN_HPP_ */\\n\\n\\n***Folder: models\\nFile Name: architectures.py\\nContents of Filearchitectures.py: #\\n#\\n#      0=================================0\\n#      |    Kernel Point Convolutions    |\\n#      0=================================0\\n#\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Define network architectures\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Hugues THOMAS - 06/03/2020\\n#\\n\\nfrom models.blocks import *\\nimport numpy as np\\n\\n\\ndef p2p_fitting_regularizer(net):\\n\\n    fitting_loss = 0\\n    repulsive_loss = 0\\n\\n    for m in net.modules():\\n\\n        if isinstance(m, KPConv) and m.deformable:\\n\\n            ##############\\n            # Fitting loss\\n            ##############\\n\\n            # Get the distance to closest input point and normalize to be independant from layers\\n            KP_min_d2 = m.min_d2 / (m.KP_extent ** 2)\\n\\n            # Loss will be the square distance to closest input point. We use L1 because dist is already squared\\n            fitting_loss += net.l1(KP_min_d2, torch.zeros_like(KP_min_d2))\\n\\n            ################\\n            # Repulsive loss\\n            ################\\n\\n            # Normalized KP locations\\n            KP_locs = m.deformed_KP / m.KP_extent\\n\\n            # Point should not be close to each other\\n            for i in range(net.K):\\n                other_KP = torch.cat([KP_locs[:, :i, :], KP_locs[:, i + 1:, :]], dim=1).detach()\\n                distances = torch.sqrt(torch.sum((other_KP - KP_locs[:, i:i + 1, :]) ** 2, dim=2))\\n                rep_loss = torch.sum(torch.clamp_max(distances - net.repulse_extent, max=0.0) ** 2, dim=1)\\n                repulsive_loss += net.l1(rep_loss, torch.zeros_like(rep_loss)) / net.K\\n\\n    return net.deform_fitting_power * (2 * fitting_loss + repulsive_loss)\\n\\n\\nclass KPCNN(nn.Module):\\n    \"\"\"\\n    Class defining KPCNN\\n    \"\"\"\\n\\n    def __init__(self, config):\\n        super(KPCNN, self).__init__()\\n\\n        #####################\\n        # Network opperations\\n        #####################\\n\\n        # Current radius of convolution and feature dimension\\n        layer = 0\\n        r = config.first_subsampling_dl * config.conv_radius\\n        in_dim = config.in_features_dim\\n        out_dim = config.first_features_dim\\n        self.K = config.num_kernel_points\\n\\n        # Save all block operations in a list of modules\\n        self.block_ops = nn.ModuleList()\\n\\n        # Loop over consecutive blocks\\n        block_in_layer = 0\\n        for block_i, block in enumerate(config.architecture):\\n\\n            # Check equivariance\\n            if (\\'equivariant\\' in block) and (not out_dim % 3 == 0):\\n                raise ValueError(\\'Equivariant block but features dimension is not a factor of 3\\')\\n\\n            # Detect upsampling block to stop\\n            if \\'upsample\\' in block:\\n                break\\n\\n            # Apply the good block function defining tf ops\\n            self.block_ops.append(block_decider(block,\\n                                                r,\\n                                                in_dim,\\n                                                out_dim,\\n                                                layer,\\n                                                config))\\n\\n\\n            # Index of block in this layer\\n            block_in_layer += 1\\n\\n            # Update dimension of input from output\\n            if \\'simple\\' in block:\\n                in_dim = out_dim // 2\\n            else:\\n                in_dim = out_dim\\n\\n\\n            # Detect change to a subsampled layer\\n            if \\'pool\\' in block or \\'strided\\' in block:\\n                # Update radius and feature dimension for next layer\\n                layer += 1\\n                r *= 2\\n                out_dim *= 2\\n                block_in_layer = 0\\n\\n        self.head_mlp = UnaryBlock(out_dim, 1024, False, 0)\\n        self.head_softmax = UnaryBlock(1024, config.num_classes, False, 0)\\n\\n        ################\\n        # Network Losses\\n        ################\\n\\n        self.criterion = torch.nn.CrossEntropyLoss()\\n        self.deform_fitting_mode = config.deform_fitting_mode\\n        self.deform_fitting_power = config.deform_fitting_power\\n        self.deform_lr_factor = config.deform_lr_factor\\n        self.repulse_extent = config.repulse_extent\\n        self.output_loss = 0\\n        self.reg_loss = 0\\n        self.l1 = nn.L1Loss()\\n\\n        return\\n\\n    def forward(self, batch, config):\\n\\n        # Save all block operations in a list of modules\\n        x = batch.features.clone().detach()\\n\\n        # Loop over consecutive blocks\\n        for block_op in self.block_ops:\\n            x = block_op(x, batch)\\n\\n        # Head of network\\n        x = self.head_mlp(x, batch)\\n        x = self.head_softmax(x, batch)\\n\\n        return x\\n\\n    def loss(self, outputs, labels):\\n        \"\"\"\\n        Runs the loss on outputs of the model\\n        :param outputs: logits\\n        :param labels: labels\\n        :return: loss\\n        \"\"\"\\n\\n        # Cross entropy loss\\n        self.output_loss = self.criterion(outputs, labels)\\n\\n        # Regularization of deformable offsets\\n        if self.deform_fitting_mode == \\'point2point\\':\\n            self.reg_loss = p2p_fitting_regularizer(self)\\n        elif self.deform_fitting_mode == \\'point2plane\\':\\n            raise ValueError(\\'point2plane fitting mode not implemented yet.\\')\\n        else:\\n            raise ValueError(\\'Unknown fitting mode: \\' + self.deform_fitting_mode)\\n\\n        # Combined loss\\n        return self.output_loss + self.reg_loss\\n\\n    @staticmethod\\n    def accuracy(outputs, labels):\\n        \"\"\"\\n        Computes accuracy of the current batch\\n        :param outputs: logits predicted by the network\\n        :param labels: labels\\n        :return: accuracy value\\n        \"\"\"\\n\\n        predicted = torch.argmax(outputs.data, dim=1)\\n        total = labels.size(0)\\n        correct = (predicted == labels).sum().item()\\n\\n        return correct / total\\n\\n\\nclass KPFCNN(nn.Module):\\n    \"\"\"\\n    Class defining KPFCNN\\n    \"\"\"\\n\\n    def __init__(self, config, lbl_values, ign_lbls):\\n        super(KPFCNN, self).__init__()\\n\\n        ############\\n        # Parameters\\n        ############\\n\\n        # Current radius of convolution and feature dimension\\n        layer = 0\\n        r = config.first_subsampling_dl * config.conv_radius\\n        in_dim = config.in_features_dim\\n        out_dim = config.first_features_dim\\n        self.K = config.num_kernel_points\\n        self.C = len(lbl_values) - len(ign_lbls)\\n\\n        #####################\\n        # List Encoder blocks\\n        #####################\\n\\n        # Save all block operations in a list of modules\\n        self.encoder_blocks = nn.ModuleList()\\n        self.encoder_skip_dims = []\\n        self.encoder_skips = []\\n\\n        # Loop over consecutive blocks\\n        for block_i, block in enumerate(config.architecture):\\n\\n            # Check equivariance\\n            if (\\'equivariant\\' in block) and (not out_dim % 3 == 0):\\n                raise ValueError(\\'Equivariant block but features dimension is not a factor of 3\\')\\n\\n            # Detect change to next layer for skip connection\\n            if np.any([tmp in block for tmp in [\\'pool\\', \\'strided\\', \\'upsample\\', \\'global\\']]):\\n                self.encoder_skips.append(block_i)\\n                self.encoder_skip_dims.append(in_dim)\\n\\n            # Detect upsampling block to stop\\n            if \\'upsample\\' in block:\\n                break\\n\\n            # Apply the good block function defining tf ops\\n            self.encoder_blocks.append(block_decider(block,\\n                                                    r,\\n                                                    in_dim,\\n                                                    out_dim,\\n                                                    layer,\\n                                                    config))\\n\\n            # Update dimension of input from output\\n            if \\'simple\\' in block:\\n                in_dim = out_dim // 2\\n            else:\\n                in_dim = out_dim\\n\\n            # Detect change to a subsampled layer\\n            if \\'pool\\' in block or \\'strided\\' in block:\\n                # Update radius and feature dimension for next layer\\n                layer += 1\\n                r *= 2\\n                out_dim *= 2\\n\\n        #####################\\n        # List Decoder blocks\\n        #####################\\n\\n        # Save all block operations in a list of modules\\n        self.decoder_blocks = nn.ModuleList()\\n        self.decoder_concats = []\\n\\n        # Find first upsampling block\\n        start_i = 0\\n        for block_i, block in enumerate(config.architecture):\\n            if \\'upsample\\' in block:\\n                start_i = block_i\\n                break\\n\\n        # Loop over consecutive blocks\\n        for block_i, block in enumerate(config.architecture[start_i:]):\\n\\n            # Add dimension of skip connection concat\\n            if block_i > 0 and \\'upsample\\' in config.architecture[start_i + block_i - 1]:\\n                in_dim += self.encoder_skip_dims[layer]\\n                self.decoder_concats.append(block_i)\\n\\n            # Apply the good block function defining tf ops\\n            self.decoder_blocks.append(block_decider(block,\\n                                                    r,\\n                                                    in_dim,\\n                                                    out_dim,\\n                                                    layer,\\n                                                    config))\\n\\n            # Update dimension of input from output\\n            in_dim = out_dim\\n\\n            # Detect change to a subsampled layer\\n            if \\'upsample\\' in block:\\n                # Update radius and feature dimension for next layer\\n                layer -= 1\\n                r *= 0.5\\n                out_dim = out_dim // 2\\n\\n        self.head_mlp = UnaryBlock(out_dim, config.first_features_dim, False, 0)\\n        self.head_softmax = UnaryBlock(config.first_features_dim, self.C, False, 0)\\n\\n        ################\\n        # Network Losses\\n        ################\\n\\n        # List of valid labels (those not ignored in loss)\\n        self.valid_labels = np.sort([c for c in lbl_values if c not in ign_lbls])\\n\\n        # Choose segmentation loss\\n        if len(config.class_w) > 0:\\n            class_w = torch.from_numpy(np.array(config.class_w, dtype=np.float32))\\n            self.criterion = torch.nn.CrossEntropyLoss(weight=class_w, ignore_index=-1)\\n        else:\\n            self.criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\\n        self.deform_fitting_mode = config.deform_fitting_mode\\n        self.deform_fitting_power = config.deform_fitting_power\\n        self.deform_lr_factor = config.deform_lr_factor\\n        self.repulse_extent = config.repulse_extent\\n        self.output_loss = 0\\n        self.reg_loss = 0\\n        self.l1 = nn.L1Loss()\\n\\n        return\\n\\n    def forward(self, batch, config):\\n\\n        # Get input features\\n        x = batch.features.clone().detach()\\n\\n        # Loop over consecutive blocks\\n        skip_x = []\\n        for block_i, block_op in enumerate(self.encoder_blocks):\\n            if block_i in self.encoder_skips:\\n                skip_x.append(x)\\n            x = block_op(x, batch)\\n\\n        for block_i, block_op in enumerate(self.decoder_blocks):\\n            if block_i in self.decoder_concats:\\n                x = torch.cat([x, skip_x.pop()], dim=1)\\n            x = block_op(x, batch)\\n\\n        # Head of network\\n        x = self.head_mlp(x, batch)\\n        x = self.head_softmax(x, batch)\\n\\n        return x\\n\\n    def loss(self, outputs, labels):\\n        \"\"\"\\n        Runs the loss on outputs of the model\\n        :param outputs: logits\\n        :param labels: labels\\n        :return: loss\\n        \"\"\"\\n\\n        # Set all ignored labels to -1 and correct the other label to be in [0, C-1] range\\n        target = - torch.ones_like(labels)\\n        for i, c in enumerate(self.valid_labels):\\n            target[labels == c] = i\\n\\n        # Reshape to have a minibatch size of 1\\n        outputs = torch.transpose(outputs, 0, 1)\\n        outputs = outputs.unsqueeze(0)\\n        target = target.unsqueeze(0)\\n\\n        # Cross entropy loss\\n        self.output_loss = self.criterion(outputs, target)\\n\\n        # Regularization of deformable offsets\\n        if self.deform_fitting_mode == \\'point2point\\':\\n            self.reg_loss = p2p_fitting_regularizer(self)\\n        elif self.deform_fitting_mode == \\'point2plane\\':\\n            raise ValueError(\\'point2plane fitting mode not implemented yet.\\')\\n        else:\\n            raise ValueError(\\'Unknown fitting mode: \\' + self.deform_fitting_mode)\\n\\n        # Combined loss\\n        return self.output_loss + self.reg_loss\\n\\n    def accuracy(self, outputs, labels):\\n        \"\"\"\\n        Computes accuracy of the current batch\\n        :param outputs: logits predicted by the network\\n        :param labels: labels\\n        :return: accuracy value\\n        \"\"\"\\n\\n        # Set all ignored labels to -1 and correct the other label to be in [0, C-1] range\\n        target = - torch.ones_like(labels)\\n        for i, c in enumerate(self.valid_labels):\\n            target[labels == c] = i\\n\\n        predicted = torch.argmax(outputs.data, dim=1)\\n        total = target.size(0)\\n        correct = (predicted == target).sum().item()\\n\\n        return correct / total\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFile Name: blocks.py\\nContents of Fileblocks.py: #\\n#\\n#      0=================================0\\n#      |    Kernel Point Convolutions    |\\n#      0=================================0\\n#\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Define network blocks\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Hugues THOMAS - 06/03/2020\\n#\\n\\n\\nimport time\\nimport math\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn.parameter import Parameter\\nfrom torch.nn.init import kaiming_uniform_\\nfrom kernels.kernel_points import load_kernels\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Simple functions\\n#       \\\\**********************/\\n#\\n\\n\\ndef gather(x, idx, method=2):\\n    \"\"\"\\n    implementation of a custom gather operation for faster backwards.\\n    :param x: input with shape [N, D_1, ... D_d]\\n    :param idx: indexing with shape [n_1, ..., n_m]\\n    :param method: Choice of the method\\n    :return: x[idx] with shape [n_1, ..., n_m, D_1, ... D_d]\\n    \"\"\"\\n\\n    if method == 0:\\n        return x[idx]\\n    elif method == 1:\\n        x = x.unsqueeze(1)\\n        x = x.expand((-1, idx.shape[-1], -1))\\n        idx = idx.unsqueeze(2)\\n        idx = idx.expand((-1, -1, x.shape[-1]))\\n        return x.gather(0, idx)\\n    elif method == 2:\\n        for i, ni in enumerate(idx.size()[1:]):\\n            x = x.unsqueeze(i+1)\\n            new_s = list(x.size())\\n            new_s[i+1] = ni\\n            x = x.expand(new_s)\\n        n = len(idx.size())\\n        for i, di in enumerate(x.size()[n:]):\\n            idx = idx.unsqueeze(i+n)\\n            new_s = list(idx.size())\\n            new_s[i+n] = di\\n            idx = idx.expand(new_s)\\n        return x.gather(0, idx)\\n    else:\\n        raise ValueError(\\'Unkown method\\')\\n\\n\\ndef radius_gaussian(sq_r, sig, eps=1e-9):\\n    \"\"\"\\n    Compute a radius gaussian (gaussian of distance)\\n    :param sq_r: input radiuses [dn, ..., d1, d0]\\n    :param sig: extents of gaussians [d1, d0] or [d0] or float\\n    :return: gaussian of sq_r [dn, ..., d1, d0]\\n    \"\"\"\\n    return torch.exp(-sq_r / (2 * sig**2 + eps))\\n\\n\\ndef closest_pool(x, inds):\\n    \"\"\"\\n    Pools features from the closest neighbors. WARNING: this function assumes the neighbors are ordered.\\n    :param x: [n1, d] features matrix\\n    :param inds: [n2, max_num] Only the first column is used for pooling\\n    :return: [n2, d] pooled features matrix\\n    \"\"\"\\n\\n    # Add a last row with minimum features for shadow pools\\n    x = torch.cat((x, torch.zeros_like(x[:1, :])), 0)\\n\\n    # Get features for each pooling location [n2, d]\\n    return gather(x, inds[:, 0])\\n\\n\\ndef max_pool(x, inds):\\n    \"\"\"\\n    Pools features with the maximum values.\\n    :param x: [n1, d] features matrix\\n    :param inds: [n2, max_num] pooling indices\\n    :return: [n2, d] pooled features matrix\\n    \"\"\"\\n\\n    # Add a last row with minimum features for shadow pools\\n    x = torch.cat((x, torch.zeros_like(x[:1, :])), 0)\\n\\n    # Get all features for each pooling location [n2, max_num, d]\\n    pool_features = gather(x, inds)\\n\\n    # Pool the maximum [n2, d]\\n    max_features, _ = torch.max(pool_features, 1)\\n    return max_features\\n\\n\\ndef global_average(x, batch_lengths):\\n    \"\"\"\\n    Block performing a global average over batch pooling\\n    :param x: [N, D] input features\\n    :param batch_lengths: [B] list of batch lengths\\n    :return: [B, D] averaged features\\n    \"\"\"\\n\\n    # Loop over the clouds of the batch\\n    averaged_features = []\\n    i0 = 0\\n    for b_i, length in enumerate(batch_lengths):\\n\\n        # Average features for each batch cloud\\n        averaged_features.append(torch.mean(x[i0:i0 + length], dim=0))\\n\\n        # Increment for next cloud\\n        i0 += length\\n\\n    # Average features in each batch\\n    return torch.stack(averaged_features)\\n\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           KPConv class\\n#       \\\\******************/\\n#\\n\\n\\nclass KPConv(nn.Module):\\n\\n    def __init__(self, kernel_size, p_dim, in_channels, out_channels, KP_extent, radius,\\n                 fixed_kernel_points=\\'center\\', KP_influence=\\'linear\\', aggregation_mode=\\'sum\\',\\n                 deformable=False, modulated=False):\\n        \"\"\"\\n        Initialize parameters for KPConvDeformable.\\n        :param kernel_size: Number of kernel points.\\n        :param p_dim: dimension of the point space.\\n        :param in_channels: dimension of input features.\\n        :param out_channels: dimension of output features.\\n        :param KP_extent: influence radius of each kernel point.\\n        :param radius: radius used for kernel point init. Even for deformable, use the config.conv_radius\\n        :param fixed_kernel_points: fix position of certain kernel points (\\'none\\', \\'center\\' or \\'verticals\\').\\n        :param KP_influence: influence function of the kernel points (\\'constant\\', \\'linear\\', \\'gaussian\\').\\n        :param aggregation_mode: choose to sum influences, or only keep the closest (\\'closest\\', \\'sum\\').\\n        :param deformable: choose deformable or not\\n        :param modulated: choose if kernel weights are modulated in addition to deformed\\n        \"\"\"\\n        super(KPConv, self).__init__()\\n\\n        # Save parameters\\n        self.K = kernel_size\\n        self.p_dim = p_dim\\n        self.in_channels = in_channels\\n        self.out_channels = out_channels\\n        self.radius = radius\\n        self.KP_extent = KP_extent\\n        self.fixed_kernel_points = fixed_kernel_points\\n        self.KP_influence = KP_influence\\n        self.aggregation_mode = aggregation_mode\\n        self.deformable = deformable\\n        self.modulated = modulated\\n\\n        # Running variable containing deformed KP distance to input points. (used in regularization loss)\\n        self.min_d2 = None\\n        self.deformed_KP = None\\n        self.offset_features = None\\n\\n        # Initialize weights\\n        self.weights = Parameter(torch.zeros((self.K, in_channels, out_channels), dtype=torch.float32),\\n                                 requires_grad=True)\\n\\n        # Initiate weights for offsets\\n        if deformable:\\n            if modulated:\\n                self.offset_dim = (self.p_dim + 1) * self.K\\n            else:\\n                self.offset_dim = self.p_dim * self.K\\n            self.offset_conv = KPConv(self.K,\\n                                      self.p_dim,\\n                                      self.in_channels,\\n                                      self.offset_dim,\\n                                      KP_extent,\\n                                      radius,\\n                                      fixed_kernel_points=fixed_kernel_points,\\n                                      KP_influence=KP_influence,\\n                                      aggregation_mode=aggregation_mode)\\n            self.offset_bias = Parameter(torch.zeros(self.offset_dim, dtype=torch.float32), requires_grad=True)\\n\\n        else:\\n            self.offset_dim = None\\n            self.offset_conv = None\\n            self.offset_bias = None\\n\\n        # Reset parameters\\n        self.reset_parameters()\\n\\n        # Initialize kernel points\\n        self.kernel_points = self.init_KP()\\n\\n        return\\n\\n    def reset_parameters(self):\\n        kaiming_uniform_(self.weights, a=math.sqrt(5))\\n        if self.deformable:\\n            nn.init.zeros_(self.offset_bias)\\n        return\\n\\n    def init_KP(self):\\n        \"\"\"\\n        Initialize the kernel point positions in a sphere\\n        :return: the tensor of kernel points\\n        \"\"\"\\n\\n        # Create one kernel disposition (as numpy array). Choose the KP distance to center thanks to the KP extent\\n        K_points_numpy = load_kernels(self.radius,\\n                                      self.K,\\n                                      dimension=self.p_dim,\\n                                      fixed=self.fixed_kernel_points)\\n\\n        return Parameter(torch.tensor(K_points_numpy, dtype=torch.float32),\\n                         requires_grad=False)\\n\\n    def forward(self, q_pts, s_pts, neighb_inds, x):\\n\\n        ###################\\n        # Offset generation\\n        ###################\\n\\n        if self.deformable:\\n\\n            # Get offsets with a KPConv that only takes part of the features\\n            self.offset_features = self.offset_conv(q_pts, s_pts, neighb_inds, x) + self.offset_bias\\n\\n            if self.modulated:\\n\\n                # Get offset (in normalized scale) from features\\n                unscaled_offsets = self.offset_features[:, :self.p_dim * self.K]\\n                unscaled_offsets = unscaled_offsets.view(-1, self.K, self.p_dim)\\n\\n                # Get modulations\\n                modulations = 2 * torch.sigmoid(self.offset_features[:, self.p_dim * self.K:])\\n\\n            else:\\n\\n                # Get offset (in normalized scale) from features\\n                unscaled_offsets = self.offset_features.view(-1, self.K, self.p_dim)\\n\\n                # No modulations\\n                modulations = None\\n\\n            # Rescale offset for this layer\\n            offsets = unscaled_offsets * self.KP_extent\\n\\n        else:\\n            offsets = None\\n            modulations = None\\n\\n        ######################\\n        # Deformed convolution\\n        ######################\\n\\n        # Add a fake point in the last row for shadow neighbors\\n        s_pts = torch.cat((s_pts, torch.zeros_like(s_pts[:1, :]) + 1e6), 0)\\n\\n        # Get neighbor points [n_points, n_neighbors, dim]\\n        neighbors = s_pts[neighb_inds, :]\\n\\n        # Center every neighborhood\\n        neighbors = neighbors - q_pts.unsqueeze(1)\\n\\n        # Apply offsets to kernel points [n_points, n_kpoints, dim]\\n        if self.deformable:\\n            self.deformed_KP = offsets + self.kernel_points\\n            deformed_K_points = self.deformed_KP.unsqueeze(1)\\n        else:\\n            deformed_K_points = self.kernel_points\\n\\n        # Get all difference matrices [n_points, n_neighbors, n_kpoints, dim]\\n        neighbors.unsqueeze_(2)\\n        differences = neighbors - deformed_K_points\\n\\n        # Get the square distances [n_points, n_neighbors, n_kpoints]\\n        sq_distances = torch.sum(differences ** 2, dim=3)\\n\\n        # Optimization by ignoring points outside a deformed KP range\\n        if self.deformable:\\n\\n            # Save distances for loss\\n            self.min_d2, _ = torch.min(sq_distances, dim=1)\\n\\n            # Boolean of the neighbors in range of a kernel point [n_points, n_neighbors]\\n            in_range = torch.any(sq_distances < self.KP_extent ** 2, dim=2).type(torch.int32)\\n\\n            # New value of max neighbors\\n            new_max_neighb = torch.max(torch.sum(in_range, dim=1))\\n\\n            # For each row of neighbors, indices of the ones that are in range [n_points, new_max_neighb]\\n            neighb_row_bool, neighb_row_inds = torch.topk(in_range, new_max_neighb.item(), dim=1)\\n\\n            # Gather new neighbor indices [n_points, new_max_neighb]\\n            new_neighb_inds = neighb_inds.gather(1, neighb_row_inds, sparse_grad=False)\\n\\n            # Gather new distances to KP [n_points, new_max_neighb, n_kpoints]\\n            neighb_row_inds.unsqueeze_(2)\\n            neighb_row_inds = neighb_row_inds.expand(-1, -1, self.K)\\n            sq_distances = sq_distances.gather(1, neighb_row_inds, sparse_grad=False)\\n\\n            # New shadow neighbors have to point to the last shadow point\\n            new_neighb_inds *= neighb_row_bool\\n            new_neighb_inds -= (neighb_row_bool.type(torch.int64) - 1) * int(s_pts.shape[0] - 1)\\n        else:\\n            new_neighb_inds = neighb_inds\\n\\n        # Get Kernel point influences [n_points, n_kpoints, n_neighbors]\\n        if self.KP_influence == \\'constant\\':\\n            # Every point get an influence of 1.\\n            all_weights = torch.ones_like(sq_distances)\\n            all_weights = torch.transpose(all_weights, 1, 2)\\n\\n        elif self.KP_influence == \\'linear\\':\\n            # Influence decrease linearly with the distance, and get to zero when d = KP_extent.\\n            all_weights = torch.clamp(1 - torch.sqrt(sq_distances) / self.KP_extent, min=0.0)\\n            all_weights = torch.transpose(all_weights, 1, 2)\\n\\n        elif self.KP_influence == \\'gaussian\\':\\n            # Influence in gaussian of the distance.\\n            sigma = self.KP_extent * 0.3\\n            all_weights = radius_gaussian(sq_distances, sigma)\\n            all_weights = torch.transpose(all_weights, 1, 2)\\n        else:\\n            raise ValueError(\\'Unknown influence function type (config.KP_influence)\\')\\n\\n        # In case of closest mode, only the closest KP can influence each point\\n        if self.aggregation_mode == \\'closest\\':\\n            neighbors_1nn = torch.argmin(sq_distances, dim=2)\\n            all_weights *= torch.transpose(nn.functional.one_hot(neighbors_1nn, self.K), 1, 2)\\n\\n        elif self.aggregation_mode != \\'sum\\':\\n            raise ValueError(\"Unknown convolution mode. Should be \\'closest\\' or \\'sum\\'\")\\n\\n        # Add a zero feature for shadow neighbors\\n        x = torch.cat((x, torch.zeros_like(x[:1, :])), 0)\\n\\n        # Get the features of each neighborhood [n_points, n_neighbors, in_fdim]\\n        neighb_x = gather(x, new_neighb_inds)\\n\\n        # Apply distance weights [n_points, n_kpoints, in_fdim]\\n        weighted_features = torch.matmul(all_weights, neighb_x)\\n\\n        # Apply modulations\\n        if self.deformable and self.modulated:\\n            weighted_features *= modulations.unsqueeze(2)\\n\\n        # Apply network weights [n_kpoints, n_points, out_fdim]\\n        weighted_features = weighted_features.permute((1, 0, 2))\\n        kernel_outputs = torch.matmul(weighted_features, self.weights)\\n\\n        # Convolution sum [n_points, out_fdim]\\n        return torch.sum(kernel_outputs, dim=0)\\n\\n    def __repr__(self):\\n        return \\'KPConv(radius: {:.2f}, in_feat: {:d}, out_feat: {:d})\\'.format(self.radius,\\n                                                                              self.in_channels,\\n                                                                              self.out_channels)\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Complex blocks\\n#       \\\\********************/\\n#\\n\\ndef block_decider(block_name,\\n                  radius,\\n                  in_dim,\\n                  out_dim,\\n                  layer_ind,\\n                  config):\\n\\n    if block_name == \\'unary\\':\\n        return UnaryBlock(in_dim, out_dim, config.use_batch_norm, config.batch_norm_momentum)\\n\\n    elif block_name in [\\'simple\\',\\n                        \\'simple_deformable\\',\\n                        \\'simple_invariant\\',\\n                        \\'simple_equivariant\\',\\n                        \\'simple_strided\\',\\n                        \\'simple_deformable_strided\\',\\n                        \\'simple_invariant_strided\\',\\n                        \\'simple_equivariant_strided\\']:\\n        return SimpleBlock(block_name, in_dim, out_dim, radius, layer_ind, config)\\n\\n    elif block_name in [\\'resnetb\\',\\n                        \\'resnetb_invariant\\',\\n                        \\'resnetb_equivariant\\',\\n                        \\'resnetb_deformable\\',\\n                        \\'resnetb_strided\\',\\n                        \\'resnetb_deformable_strided\\',\\n                        \\'resnetb_equivariant_strided\\',\\n                        \\'resnetb_invariant_strided\\']:\\n        return ResnetBottleneckBlock(block_name, in_dim, out_dim, radius, layer_ind, config)\\n\\n    elif block_name == \\'max_pool\\' or block_name == \\'max_pool_wide\\':\\n        return MaxPoolBlock(layer_ind)\\n\\n    elif block_name == \\'global_average\\':\\n        return GlobalAverageBlock()\\n\\n    elif block_name == \\'nearest_upsample\\':\\n        return NearestUpsampleBlock(layer_ind)\\n\\n    else:\\n        raise ValueError(\\'Unknown block name in the architecture definition : \\' + block_name)\\n\\n\\nclass BatchNormBlock(nn.Module):\\n\\n    def __init__(self, in_dim, use_bn, bn_momentum):\\n        \"\"\"\\n        Initialize a batch normalization block. If network does not use batch normalization, replace with biases.\\n        :param in_dim: dimension input features\\n        :param use_bn: boolean indicating if we use Batch Norm\\n        :param bn_momentum: Batch norm momentum\\n        \"\"\"\\n        super(BatchNormBlock, self).__init__()\\n        self.bn_momentum = bn_momentum\\n        self.use_bn = use_bn\\n        self.in_dim = in_dim\\n        if self.use_bn:\\n            self.batch_norm = nn.BatchNorm1d(in_dim, momentum=bn_momentum)\\n            #self.batch_norm = nn.InstanceNorm1d(in_dim, momentum=bn_momentum)\\n        else:\\n            self.bias = Parameter(torch.zeros(in_dim, dtype=torch.float32), requires_grad=True)\\n        return\\n\\n    def reset_parameters(self):\\n        nn.init.zeros_(self.bias)\\n\\n    def forward(self, x):\\n        if self.use_bn:\\n\\n            x = x.unsqueeze(2)\\n            x = x.transpose(0, 2)\\n            x = self.batch_norm(x)\\n            x = x.transpose(0, 2)\\n            return x.squeeze()\\n        else:\\n            return x + self.bias\\n\\n    def __repr__(self):\\n        return \\'BatchNormBlock(in_feat: {:d}, momentum: {:.3f}, only_bias: {:s})\\'.format(self.in_dim,\\n                                                                                         self.bn_momentum,\\n                                                                                         str(not self.use_bn))\\n\\n\\nclass UnaryBlock(nn.Module):\\n\\n    def __init__(self, in_dim, out_dim, use_bn, bn_momentum, no_relu=False):\\n        \"\"\"\\n        Initialize a standard unary block with its ReLU and BatchNorm.\\n        :param in_dim: dimension input features\\n        :param out_dim: dimension input features\\n        :param use_bn: boolean indicating if we use Batch Norm\\n        :param bn_momentum: Batch norm momentum\\n        \"\"\"\\n\\n        super(UnaryBlock, self).__init__()\\n        self.bn_momentum = bn_momentum\\n        self.use_bn = use_bn\\n        self.no_relu = no_relu\\n        self.in_dim = in_dim\\n        self.out_dim = out_dim\\n        self.mlp = nn.Linear(in_dim, out_dim, bias=False)\\n        self.batch_norm = BatchNormBlock(out_dim, self.use_bn, self.bn_momentum)\\n        if not no_relu:\\n            self.leaky_relu = nn.LeakyReLU(0.1)\\n        return\\n\\n    def forward(self, x, batch=None):\\n        x = self.mlp(x)\\n        x = self.batch_norm(x)\\n        if not self.no_relu:\\n            x = self.leaky_relu(x)\\n        return x\\n\\n    def __repr__(self):\\n        return \\'UnaryBlock(in_feat: {:d}, out_feat: {:d}, BN: {:s}, ReLU: {:s})\\'.format(self.in_dim,\\n                                                                                        self.out_dim,\\n                                                                                        str(self.use_bn),\\n                                                                                        str(not self.no_relu))\\n\\n\\nclass SimpleBlock(nn.Module):\\n\\n    def __init__(self, block_name, in_dim, out_dim, radius, layer_ind, config):\\n        \"\"\"\\n        Initialize a simple convolution block with its ReLU and BatchNorm.\\n        :param in_dim: dimension input features\\n        :param out_dim: dimension input features\\n        :param radius: current radius of convolution\\n        :param config: parameters\\n        \"\"\"\\n        super(SimpleBlock, self).__init__()\\n\\n        # get KP_extent from current radius\\n        current_extent = radius * config.KP_extent / config.conv_radius\\n\\n        # Get other parameters\\n        self.bn_momentum = config.batch_norm_momentum\\n        self.use_bn = config.use_batch_norm\\n        self.layer_ind = layer_ind\\n        self.block_name = block_name\\n        self.in_dim = in_dim\\n        self.out_dim = out_dim\\n\\n        # Define the KPConv class\\n        self.KPConv = KPConv(config.num_kernel_points,\\n                             config.in_points_dim,\\n                             in_dim,\\n                             out_dim // 2,\\n                             current_extent,\\n                             radius,\\n                             fixed_kernel_points=config.fixed_kernel_points,\\n                             KP_influence=config.KP_influence,\\n                             aggregation_mode=config.aggregation_mode,\\n                             deformable=\\'deform\\' in block_name,\\n                             modulated=config.modulated)\\n\\n        # Other opperations\\n        self.batch_norm = BatchNormBlock(out_dim // 2, self.use_bn, self.bn_momentum)\\n        self.leaky_relu = nn.LeakyReLU(0.1)\\n\\n        return\\n\\n    def forward(self, x, batch):\\n\\n        if \\'strided\\' in self.block_name:\\n            q_pts = batch.points[self.layer_ind + 1]\\n            s_pts = batch.points[self.layer_ind]\\n            neighb_inds = batch.pools[self.layer_ind]\\n        else:\\n            q_pts = batch.points[self.layer_ind]\\n            s_pts = batch.points[self.layer_ind]\\n            neighb_inds = batch.neighbors[self.layer_ind]\\n\\n        x = self.KPConv(q_pts, s_pts, neighb_inds, x)\\n        return self.leaky_relu(self.batch_norm(x))\\n\\n\\nclass ResnetBottleneckBlock(nn.Module):\\n\\n    def __init__(self, block_name, in_dim, out_dim, radius, layer_ind, config):\\n        \"\"\"\\n        Initialize a resnet bottleneck block.\\n        :param in_dim: dimension input features\\n        :param out_dim: dimension input features\\n        :param radius: current radius of convolution\\n        :param config: parameters\\n        \"\"\"\\n        super(ResnetBottleneckBlock, self).__init__()\\n\\n        # get KP_extent from current radius\\n        current_extent = radius * config.KP_extent / config.conv_radius\\n\\n        # Get other parameters\\n        self.bn_momentum = config.batch_norm_momentum\\n        self.use_bn = config.use_batch_norm\\n        self.block_name = block_name\\n        self.layer_ind = layer_ind\\n        self.in_dim = in_dim\\n        self.out_dim = out_dim\\n\\n        # First downscaling mlp\\n        if in_dim != out_dim // 4:\\n            self.unary1 = UnaryBlock(in_dim, out_dim // 4, self.use_bn, self.bn_momentum)\\n        else:\\n            self.unary1 = nn.Identity()\\n\\n        # KPConv block\\n        self.KPConv = KPConv(config.num_kernel_points,\\n                             config.in_points_dim,\\n                             out_dim // 4,\\n                             out_dim // 4,\\n                             current_extent,\\n                             radius,\\n                             fixed_kernel_points=config.fixed_kernel_points,\\n                             KP_influence=config.KP_influence,\\n                             aggregation_mode=config.aggregation_mode,\\n                             deformable=\\'deform\\' in block_name,\\n                             modulated=config.modulated)\\n        self.batch_norm_conv = BatchNormBlock(out_dim // 4, self.use_bn, self.bn_momentum)\\n\\n        # Second upscaling mlp\\n        self.unary2 = UnaryBlock(out_dim // 4, out_dim, self.use_bn, self.bn_momentum, no_relu=True)\\n\\n        # Shortcut optional mpl\\n        if in_dim != out_dim:\\n            self.unary_shortcut = UnaryBlock(in_dim, out_dim, self.use_bn, self.bn_momentum, no_relu=True)\\n        else:\\n            self.unary_shortcut = nn.Identity()\\n\\n        # Other operations\\n        self.leaky_relu = nn.LeakyReLU(0.1)\\n\\n        return\\n\\n    def forward(self, features, batch):\\n\\n        if \\'strided\\' in self.block_name:\\n            q_pts = batch.points[self.layer_ind + 1]\\n            s_pts = batch.points[self.layer_ind]\\n            neighb_inds = batch.pools[self.layer_ind]\\n        else:\\n            q_pts = batch.points[self.layer_ind]\\n            s_pts = batch.points[self.layer_ind]\\n            neighb_inds = batch.neighbors[self.layer_ind]\\n\\n        # First downscaling mlp\\n        x = self.unary1(features)\\n\\n        # Convolution\\n        x = self.KPConv(q_pts, s_pts, neighb_inds, x)\\n        x = self.leaky_relu(self.batch_norm_conv(x))\\n\\n        # Second upscaling mlp\\n        x = self.unary2(x)\\n\\n        # Shortcut\\n        if \\'strided\\' in self.block_name:\\n            shortcut = max_pool(features, neighb_inds)\\n        else:\\n            shortcut = features\\n        shortcut = self.unary_shortcut(shortcut)\\n\\n        return self.leaky_relu(x + shortcut)\\n\\n\\nclass GlobalAverageBlock(nn.Module):\\n\\n    def __init__(self):\\n        \"\"\"\\n        Initialize a global average block with its ReLU and BatchNorm.\\n        \"\"\"\\n        super(GlobalAverageBlock, self).__init__()\\n        return\\n\\n    def forward(self, x, batch):\\n        return global_average(x, batch.lengths[-1])\\n\\n\\nclass NearestUpsampleBlock(nn.Module):\\n\\n    def __init__(self, layer_ind):\\n        \"\"\"\\n        Initialize a nearest upsampling block with its ReLU and BatchNorm.\\n        \"\"\"\\n        super(NearestUpsampleBlock, self).__init__()\\n        self.layer_ind = layer_ind\\n        return\\n\\n    def forward(self, x, batch):\\n        return closest_pool(x, batch.upsamples[self.layer_ind - 1])\\n\\n    def __repr__(self):\\n        return \\'NearestUpsampleBlock(layer: {:d} -> {:d})\\'.format(self.layer_ind,\\n                                                                  self.layer_ind - 1)\\n\\n\\nclass MaxPoolBlock(nn.Module):\\n\\n    def __init__(self, layer_ind):\\n        \"\"\"\\n        Initialize a max pooling block with its ReLU and BatchNorm.\\n        \"\"\"\\n        super(MaxPoolBlock, self).__init__()\\n        self.layer_ind = layer_ind\\n        return\\n\\n    def forward(self, x, batch):\\n        return max_pool(x, batch.pools[self.layer_ind + 1])\\n\\n\\n\\n***Folder: kernels\\nFile Name: kernel_points.py\\nContents of Filekernel_points.py: #\\n#\\n#      0=================================0\\n#      |    Kernel Point Convolutions    |\\n#      0=================================0\\n#\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Functions handling the disposition of kernel points.\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Hugues THOMAS - 11/06/2018\\n#\\n\\n\\n# ------------------------------------------------------------------------------------------\\n#\\n#          Imports and global variables\\n#      \\\\**********************************/\\n#\\n\\n\\n# Import numpy package and name it \"np\"\\nimport time\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom matplotlib import cm\\nfrom os import makedirs\\nfrom os.path import join, exists\\n\\nfrom utils.config import bcolors\\nfrom utils.las import write_las, read_las_points\\n\\n\\n# ------------------------------------------------------------------------------------------\\n#\\n#           Functions\\n#       \\\\***************/\\n#\\n#\\n\\ndef create_3D_rotations(axis, angle):\\n    \"\"\"\\n    Create rotation matrices from a list of axes and angles. Code from wikipedia on quaternions\\n    :param axis: float32[N, 3]\\n    :param angle: float32[N,]\\n    :return: float32[N, 3, 3]\\n    \"\"\"\\n\\n    t1 = np.cos(angle)\\n    t2 = 1 - t1\\n    t3 = axis[:, 0] * axis[:, 0]\\n    t6 = t2 * axis[:, 0]\\n    t7 = t6 * axis[:, 1]\\n    t8 = np.sin(angle)\\n    t9 = t8 * axis[:, 2]\\n    t11 = t6 * axis[:, 2]\\n    t12 = t8 * axis[:, 1]\\n    t15 = axis[:, 1] * axis[:, 1]\\n    t19 = t2 * axis[:, 1] * axis[:, 2]\\n    t20 = t8 * axis[:, 0]\\n    t24 = axis[:, 2] * axis[:, 2]\\n    R = np.stack([t1 + t2 * t3,\\n                  t7 - t9,\\n                  t11 + t12,\\n                  t7 + t9,\\n                  t1 + t2 * t15,\\n                  t19 - t20,\\n                  t11 - t12,\\n                  t19 + t20,\\n                  t1 + t2 * t24], axis=1)\\n\\n    return np.reshape(R, (-1, 3, 3))\\n\\n\\ndef spherical_Lloyd(radius, num_cells, dimension=3, fixed=\\'center\\', approximation=\\'monte-carlo\\',\\n                    approx_n=5000, max_iter=500, momentum=0.9, verbose=0):\\n    \"\"\"\\n    Creation of kernel point via Lloyd algorithm. We use an approximation of the algorithm, and compute the Voronoi\\n    cell centers with discretization  of space. The exact formula is not trivial with part of the sphere as sides.\\n    :param radius: Radius of the kernels\\n    :param num_cells: Number of cell (kernel points) in the Voronoi diagram.\\n    :param dimension: dimension of the space\\n    :param fixed: fix position of certain kernel points (\\'none\\', \\'center\\' or \\'verticals\\')\\n    :param approximation: Approximation method for Lloyd\\'s algorithm (\\'discretization\\', \\'monte-carlo\\')\\n    :param approx_n: Number of point used for approximation.\\n    :param max_iter: Maximum nu;ber of iteration for the algorithm.\\n    :param momentum: Momentum of the low pass filter smoothing kernel point positions\\n    :param verbose: display option\\n    :return: points [num_kernels, num_points, dimension]\\n    \"\"\"\\n\\n    #######################\\n    # Parameters definition\\n    #######################\\n\\n    # Radius used for optimization (points are rescaled afterwards)\\n    radius0 = 1.0\\n\\n    #######################\\n    # Kernel initialization\\n    #######################\\n\\n    # Random kernel points (Uniform distribution in a sphere)\\n    kernel_points = np.zeros((0, dimension))\\n    while kernel_points.shape[0] < num_cells:\\n        new_points = np.random.rand(num_cells, dimension) * 2 * radius0 - radius0\\n        kernel_points = np.vstack((kernel_points, new_points))\\n        d2 = np.sum(np.power(kernel_points, 2), axis=1)\\n        kernel_points = kernel_points[np.logical_and(d2 < radius0 ** 2, (0.9 * radius0) ** 2 < d2), :]\\n    kernel_points = kernel_points[:num_cells, :].reshape((num_cells, -1))\\n\\n    # Optional fixing\\n    if fixed == \\'center\\':\\n        kernel_points[0, :] *= 0\\n    if fixed == \\'verticals\\':\\n        kernel_points[:3, :] *= 0\\n        kernel_points[1, -1] += 2 * radius0 / 3\\n        kernel_points[2, -1] -= 2 * radius0 / 3\\n\\n    ##############################\\n    # Approximation initialization\\n    ##############################\\n\\n    # Initialize figure\\n    if verbose > 1:\\n        fig = plt.figure()\\n\\n    # Initialize discretization in this method is chosen\\n    if approximation == \\'discretization\\':\\n        side_n = int(np.floor(approx_n ** (1. / dimension)))\\n        dl = 2 * radius0 / side_n\\n        coords = np.arange(-radius0 + dl/2, radius0, dl)\\n        if dimension == 2:\\n            x, y = np.meshgrid(coords, coords)\\n            X = np.vstack((np.ravel(x), np.ravel(y))).T\\n        elif dimension == 3:\\n            x, y, z = np.meshgrid(coords, coords, coords)\\n            X = np.vstack((np.ravel(x), np.ravel(y), np.ravel(z))).T\\n        elif dimension == 4:\\n            x, y, z, t = np.meshgrid(coords, coords, coords, coords)\\n            X = np.vstack((np.ravel(x), np.ravel(y), np.ravel(z), np.ravel(t))).T\\n        else:\\n            raise ValueError(\\'Unsupported dimension (max is 4)\\')\\n    elif approximation == \\'monte-carlo\\':\\n        X = np.zeros((0, dimension))\\n    else:\\n        raise ValueError(\\'Wrong approximation method chosen: \"{:s}\"\\'.format(approximation))\\n\\n    # Only points inside the sphere are used\\n    d2 = np.sum(np.power(X, 2), axis=1)\\n    X = X[d2 < radius0 * radius0, :]\\n\\n    #####################\\n    # Kernel optimization\\n    #####################\\n\\n    # Warning if at least one kernel point has no cell\\n    warning = False\\n\\n    # moving vectors of kernel points saved to detect convergence\\n    max_moves = np.zeros((0,))\\n\\n    for iter in range(max_iter):\\n\\n        # In the case of monte-carlo, renew the sampled points\\n        if approximation == \\'monte-carlo\\':\\n            X = np.random.rand(approx_n, dimension) * 2 * radius0 - radius0\\n            d2 = np.sum(np.power(X, 2), axis=1)\\n            X = X[d2 < radius0 * radius0, :]\\n\\n        # Get the distances matrix [n_approx, K, dim]\\n        differences = np.expand_dims(X, 1) - kernel_points\\n        sq_distances = np.sum(np.square(differences), axis=2)\\n\\n        # Compute cell centers\\n        cell_inds = np.argmin(sq_distances, axis=1)\\n        centers = []\\n        for c in range(num_cells):\\n            bool_c = (cell_inds == c)\\n            num_c = np.sum(bool_c.astype(np.int32))\\n            if num_c > 0:\\n                centers.append(np.sum(X[bool_c, :], axis=0) / num_c)\\n            else:\\n                warning = True\\n                centers.append(kernel_points[c])\\n\\n        # Update kernel points with low pass filter to smooth mote carlo\\n        centers = np.vstack(centers)\\n        moves = (1 - momentum) * (centers - kernel_points)\\n        kernel_points += moves\\n\\n        # Check moves for convergence\\n        max_moves = np.append(max_moves, np.max(np.linalg.norm(moves, axis=1)))\\n\\n        # Optional fixing\\n        if fixed == \\'center\\':\\n            kernel_points[0, :] *= 0\\n        if fixed == \\'verticals\\':\\n            kernel_points[0, :] *= 0\\n            kernel_points[:3, :-1] *= 0\\n\\n        if verbose:\\n            print(\\'iter {:5d} / max move = {:f}\\'.format(iter, np.max(np.linalg.norm(moves, axis=1))))\\n            if warning:\\n                print(\\'{:}WARNING: at least one point has no cell{:}\\'.format(bcolors.WARNING, bcolors.ENDC))\\n        if verbose > 1:\\n            plt.clf()\\n            plt.scatter(X[:, 0], X[:, 1], c=cell_inds, s=20.0,\\n                        marker=\\'.\\', cmap=plt.get_cmap(\\'tab20\\'))\\n            #plt.scatter(kernel_points[:, 0], kernel_points[:, 1], c=np.arange(num_cells), s=100.0,\\n            #            marker=\\'+\\', cmap=plt.get_cmap(\\'tab20\\'))\\n            plt.plot(kernel_points[:, 0], kernel_points[:, 1], \\'k+\\')\\n            circle = plt.Circle((0, 0), radius0, color=\\'r\\', fill=False)\\n            fig.axes[0].add_artist(circle)\\n            fig.axes[0].set_xlim((-radius0 * 1.1, radius0 * 1.1))\\n            fig.axes[0].set_ylim((-radius0 * 1.1, radius0 * 1.1))\\n            fig.axes[0].set_aspect(\\'equal\\')\\n            plt.draw()\\n            plt.pause(0.001)\\n            plt.show(block=False)\\n\\n    ###################\\n    # User verification\\n    ###################\\n\\n    # Show the convergence to ask user if this kernel is correct\\n    if verbose:\\n        if dimension == 2:\\n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[10.4, 4.8])\\n            ax1.plot(max_moves)\\n            ax2.scatter(X[:, 0], X[:, 1], c=cell_inds, s=20.0,\\n                        marker=\\'.\\', cmap=plt.get_cmap(\\'tab20\\'))\\n            # plt.scatter(kernel_points[:, 0], kernel_points[:, 1], c=np.arange(num_cells), s=100.0,\\n            #            marker=\\'+\\', cmap=plt.get_cmap(\\'tab20\\'))\\n            ax2.plot(kernel_points[:, 0], kernel_points[:, 1], \\'k+\\')\\n            circle = plt.Circle((0, 0), radius0, color=\\'r\\', fill=False)\\n            ax2.add_artist(circle)\\n            ax2.set_xlim((-radius0 * 1.1, radius0 * 1.1))\\n            ax2.set_ylim((-radius0 * 1.1, radius0 * 1.1))\\n            ax2.set_aspect(\\'equal\\')\\n            plt.title(\\'Check if kernel is correct.\\')\\n            plt.draw()\\n            plt.show()\\n\\n        if dimension > 2:\\n            plt.figure()\\n            plt.plot(max_moves)\\n            plt.title(\\'Check if kernel is correct.\\')\\n            plt.show()\\n\\n    # Rescale kernels with real radius\\n    return kernel_points * radius\\n\\n\\ndef kernel_point_optimization_debug(radius, num_points, num_kernels=1, dimension=3,\\n                                    fixed=\\'center\\', ratio=0.66, verbose=0):\\n    \"\"\"\\n    Creation of kernel point via optimization of potentials.\\n    :param radius: Radius of the kernels\\n    :param num_points: points composing kernels\\n    :param num_kernels: number of wanted kernels\\n    :param dimension: dimension of the space\\n    :param fixed: fix position of certain kernel points (\\'none\\', \\'center\\' or \\'verticals\\')\\n    :param ratio: ratio of the radius where you want the kernels points to be placed\\n    :param verbose: display option\\n    :return: points [num_kernels, num_points, dimension]\\n    \"\"\"\\n\\n    #######################\\n    # Parameters definition\\n    #######################\\n\\n    # Radius used for optimization (points are rescaled afterwards)\\n    radius0 = 1\\n    diameter0 = 2\\n\\n    # Factor multiplicating gradients for moving points (~learning rate)\\n    moving_factor = 1e-2\\n    continuous_moving_decay = 0.9995\\n\\n    # Gradient threshold to stop optimization\\n    thresh = 1e-5\\n\\n    # Gradient clipping value\\n    clip = 0.05 * radius0\\n\\n    #######################\\n    # Kernel initialization\\n    #######################\\n\\n    # Random kernel points\\n    kernel_points = np.random.rand(num_kernels * num_points - 1, dimension) * diameter0 - radius0\\n    while (kernel_points.shape[0] < num_kernels * num_points):\\n        new_points = np.random.rand(num_kernels * num_points - 1, dimension) * diameter0 - radius0\\n        kernel_points = np.vstack((kernel_points, new_points))\\n        d2 = np.sum(np.power(kernel_points, 2), axis=1)\\n        kernel_points = kernel_points[d2 < 0.5 * radius0 * radius0, :]\\n    kernel_points = kernel_points[:num_kernels * num_points, :].reshape((num_kernels, num_points, -1))\\n\\n    # Optionnal fixing\\n    if fixed == \\'center\\':\\n        kernel_points[:, 0, :] *= 0\\n    if fixed == \\'verticals\\':\\n        kernel_points[:, :3, :] *= 0\\n        kernel_points[:, 1, -1] += 2 * radius0 / 3\\n        kernel_points[:, 2, -1] -= 2 * radius0 / 3\\n\\n    #####################\\n    # Kernel optimization\\n    #####################\\n\\n    # Initialize figure\\n    if verbose>1:\\n        fig = plt.figure()\\n\\n    saved_gradient_norms = np.zeros((10000, num_kernels))\\n    old_gradient_norms = np.zeros((num_kernels, num_points))\\n    step = -1\\n    while step < 10000:\\n\\n        # Increment\\n        step += 1\\n\\n        # Compute gradients\\n        # *****************\\n\\n        # Derivative of the sum of potentials of all points\\n        A = np.expand_dims(kernel_points, axis=2)\\n        B = np.expand_dims(kernel_points, axis=1)\\n        interd2 = np.sum(np.power(A - B, 2), axis=-1)\\n        inter_grads = (A - B) / (np.power(np.expand_dims(interd2, -1), 3/2) + 1e-6)\\n        inter_grads = np.sum(inter_grads, axis=1)\\n\\n        # Derivative of the radius potential\\n        circle_grads = 10*kernel_points\\n\\n        # All gradients\\n        gradients = inter_grads + circle_grads\\n\\n        if fixed == \\'verticals\\':\\n            gradients[:, 1:3, :-1] = 0\\n\\n        # Stop condition\\n        # **************\\n\\n        # Compute norm of gradients\\n        gradients_norms = np.sqrt(np.sum(np.power(gradients, 2), axis=-1))\\n        saved_gradient_norms[step, :] = np.max(gradients_norms, axis=1)\\n\\n        # Stop if all moving points are gradients fixed (low gradients diff)\\n\\n        if fixed == \\'center\\' and np.max(np.abs(old_gradient_norms[:, 1:] - gradients_norms[:, 1:])) < thresh:\\n            break\\n        elif fixed == \\'verticals\\' and np.max(np.abs(old_gradient_norms[:, 3:] - gradients_norms[:, 3:])) < thresh:\\n            break\\n        elif np.max(np.abs(old_gradient_norms - gradients_norms)) < thresh:\\n            break\\n        old_gradient_norms = gradients_norms\\n\\n        # Move points\\n        # ***********\\n\\n        # Clip gradient to get moving dists\\n        moving_dists = np.minimum(moving_factor * gradients_norms, clip)\\n\\n        # Fix central point\\n        if fixed == \\'center\\':\\n            moving_dists[:, 0] = 0\\n        if fixed == \\'verticals\\':\\n            moving_dists[:, 0] = 0\\n\\n        # Move points\\n        kernel_points -= np.expand_dims(moving_dists, -1) * gradients / np.expand_dims(gradients_norms + 1e-6, -1)\\n\\n        if verbose:\\n            print(\\'step {:5d} / max grad = {:f}\\'.format(step, np.max(gradients_norms[:, 3:])))\\n        if verbose > 1:\\n            plt.clf()\\n            plt.plot(kernel_points[0, :, 0], kernel_points[0, :, 1], \\'.\\')\\n            circle = plt.Circle((0, 0), radius, color=\\'r\\', fill=False)\\n            fig.axes[0].add_artist(circle)\\n            fig.axes[0].set_xlim((-radius*1.1, radius*1.1))\\n            fig.axes[0].set_ylim((-radius*1.1, radius*1.1))\\n            fig.axes[0].set_aspect(\\'equal\\')\\n            plt.draw()\\n            plt.pause(0.001)\\n            plt.show(block=False)\\n            print(moving_factor)\\n\\n        # moving factor decay\\n        moving_factor *= continuous_moving_decay\\n\\n    # Remove unused lines in the saved gradients\\n    if step < 10000:\\n        saved_gradient_norms = saved_gradient_norms[:step+1, :]\\n\\n    # Rescale radius to fit the wanted ratio of radius\\n    r = np.sqrt(np.sum(np.power(kernel_points, 2), axis=-1))\\n    kernel_points *= ratio / np.mean(r[:, 1:])\\n\\n    # Rescale kernels with real radius\\n    return kernel_points * radius, saved_gradient_norms\\n\\n\\ndef load_kernels(radius, num_kpoints, dimension, fixed, lloyd=False):\\n\\n    # Kernel directory\\n    kernel_dir = \\'kernels/dispositions\\'\\n    if not exists(kernel_dir):\\n        makedirs(kernel_dir)\\n\\n    # To many points switch to Lloyds\\n    if num_kpoints > 30:\\n        lloyd = True\\n\\n    # Kernel_file\\n    kernel_file = join(kernel_dir, \\'k_{:03d}_{:s}_{:d}D.laz\\'.format(num_kpoints, fixed, dimension))\\n\\n    # Check if already done\\n    if not exists(kernel_file):\\n        if lloyd:\\n            # Create kernels\\n            kernel_points = spherical_Lloyd(1.0,\\n                                            num_kpoints,\\n                                            dimension=dimension,\\n                                            fixed=fixed,\\n                                            verbose=0)\\n\\n        else:\\n            # Create kernels\\n            kernel_points, grad_norms = kernel_point_optimization_debug(1.0,\\n                                                                        num_kpoints,\\n                                                                        num_kernels=100,\\n                                                                        dimension=dimension,\\n                                                                        fixed=fixed,\\n                                                                        verbose=0)\\n\\n            # Find best candidate\\n            best_k = np.argmin(grad_norms[-1, :])\\n\\n            # Save points\\n            kernel_points = kernel_points[best_k, :, :]\\n\\n        foo = np.core.records.fromarrays(np.vstack((kernel_points.T)),names=\\'X,Y,Z\\',formats=\\'f8,f8,f8\\')\\n        write_las(kernel_file, foo)\\n\\n    else:\\n        kernel_points = read_las_points(kernel_file)\\n        # kernel_points = np.vstack((data[\\'X\\'], data[\\'Y\\'], data[\\'Z\\'])).T\\n\\n    # Random roations for the kernel\\n    # N.B. 4D random rotations not supported yet\\n    R = np.eye(dimension)\\n    theta = np.random.rand() * 2 * np.pi\\n    if dimension == 2:\\n        if fixed != \\'vertical\\':\\n            c, s = np.cos(theta), np.sin(theta)\\n            R = np.array([[c, -s], [s, c]], dtype=np.float32)\\n\\n    elif dimension == 3:\\n        if fixed != \\'vertical\\':\\n            c, s = np.cos(theta), np.sin(theta)\\n            R = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]], dtype=np.float32)\\n\\n        else:\\n            phi = (np.random.rand() - 0.5) * np.pi\\n\\n            # Create the first vector in carthesian coordinates\\n            u = np.array([np.cos(theta) * np.cos(phi), np.sin(theta) * np.cos(phi), np.sin(phi)])\\n\\n            # Choose a random rotation angle\\n            alpha = np.random.rand() * 2 * np.pi\\n\\n            # Create the rotation matrix with this vector and angle\\n            R = create_3D_rotations(np.reshape(u, (1, -1)), np.reshape(alpha, (1, -1)))[0]\\n\\n            R = R.astype(np.float32)\\n\\n    # Add a small noise\\n    kernel_points = kernel_points + np.random.normal(scale=0.01, size=kernel_points.shape)\\n\\n    # Scale kernels\\n    kernel_points = radius * kernel_points\\n\\n    # Rotate kernels\\n    kernel_points = np.matmul(kernel_points, R)\\n\\n    return kernel_points.astype(np.float32)\\n\\n***Folder: datasets\\nFile Name: common.py\\nContents of Filecommon.py: #\\n#\\n#      0=================================0\\n#      |    Kernel Point Convolutions    |\\n#      0=================================0\\n#\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Class handling datasets\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Hugues THOMAS - 11/06/2018\\n#\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Imports and global variables\\n#       \\\\**********************************/\\n#\\n\\n# Common libs\\nimport time\\nimport os\\nimport numpy as np\\nimport sys\\nimport torch\\nfrom torch.utils.data import DataLoader, Dataset\\nfrom utils.config import Config\\nfrom kernels.kernel_points import create_3D_rotations\\n\\n# Subsampling extension\\nimport cpp_wrappers.cpp_subsampling.grid_subsampling as cpp_subsampling\\nimport cpp_wrappers.cpp_neighbors.radius_neighbors as cpp_neighbors\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Utility functions\\n#       \\\\***********************/\\n#\\n\\ndef grid_subsampling(points, features=None, labels=None, sampleDl=0.1, verbose=0):\\n    \"\"\"\\n    CPP wrapper for a grid subsampling (method = barycenter for points and features)\\n    :param points: (N, 3) matrix of input points\\n    :param features: optional (N, d) matrix of features (floating number)\\n    :param labels: optional (N,) matrix of integer labels\\n    :param sampleDl: parameter defining the size of grid voxels\\n    :param verbose: 1 to display\\n    :return: subsampled points, with features and/or labels depending of the input\\n    \"\"\"\\n\\n    if (features is None) and (labels is None):\\n        return cpp_subsampling.subsample(points,\\n                                         sampleDl=sampleDl,\\n                                         verbose=verbose)\\n    elif (labels is None):\\n        return cpp_subsampling.subsample(points,\\n                                         features=features,\\n                                         sampleDl=sampleDl,\\n                                         verbose=verbose)\\n    elif (features is None):\\n        return cpp_subsampling.subsample(points,\\n                                         classes=labels,\\n                                         sampleDl=sampleDl,\\n                                         verbose=verbose)\\n    else:\\n        return cpp_subsampling.subsample(points,\\n                                         features=features,\\n                                         classes=labels,\\n                                         sampleDl=sampleDl,\\n                                         verbose=verbose)\\n\\n\\ndef batch_grid_subsampling(points, batches_len, features=None, labels=None,\\n                           sampleDl=0.1, max_p=0, verbose=0, random_grid_orient=True):\\n    \"\"\"\\n    CPP wrapper for a grid subsampling (method = barycenter for points and features)\\n    :param points: (N, 3) matrix of input points\\n    :param features: optional (N, d) matrix of features (floating number)\\n    :param labels: optional (N,) matrix of integer labels\\n    :param sampleDl: parameter defining the size of grid voxels\\n    :param verbose: 1 to display\\n    :return: subsampled points, with features and/or labels depending of the input\\n    \"\"\"\\n\\n    R = None\\n    B = len(batches_len)\\n    if random_grid_orient:\\n\\n        ########################################################\\n        # Create a random rotation matrix for each batch element\\n        ########################################################\\n\\n        # Choose two random angles for the first vector in polar coordinates\\n        theta = np.random.rand(B) * 2 * np.pi\\n        phi = (np.random.rand(B) - 0.5) * np.pi\\n\\n        # Create the first vector in carthesian coordinates\\n        u = np.vstack([np.cos(theta) * np.cos(phi), np.sin(theta) * np.cos(phi), np.sin(phi)])\\n\\n        # Choose a random rotation angle\\n        alpha = np.random.rand(B) * 2 * np.pi\\n\\n        # Create the rotation matrix with this vector and angle\\n        R = create_3D_rotations(u.T, alpha).astype(np.float32)\\n\\n        #################\\n        # Apply rotations\\n        #################\\n\\n        i0 = 0\\n        points = points.copy()\\n        for bi, length in enumerate(batches_len):\\n            # Apply the rotation\\n            points[i0:i0 + length, :] = np.sum(np.expand_dims(points[i0:i0 + length, :], 2) * R[bi], axis=1)\\n            i0 += length\\n\\n    #######################\\n    # Sunsample and realign\\n    #######################\\n\\n    if (features is None) and (labels is None):\\n        s_points, s_len = cpp_subsampling.subsample_batch(points,\\n                                                          batches_len,\\n                                                          sampleDl=sampleDl,\\n                                                          max_p=max_p,\\n                                                          verbose=verbose)\\n        if random_grid_orient:\\n            i0 = 0\\n            for bi, length in enumerate(s_len):\\n                s_points[i0:i0 + length, :] = np.sum(np.expand_dims(s_points[i0:i0 + length, :], 2) * R[bi].T, axis=1)\\n                i0 += length\\n        return s_points, s_len\\n\\n    elif (labels is None):\\n        s_points, s_len, s_features = cpp_subsampling.subsample_batch(points,\\n                                                                      batches_len,\\n                                                                      features=features,\\n                                                                      sampleDl=sampleDl,\\n                                                                      max_p=max_p,\\n                                                                      verbose=verbose)\\n        if random_grid_orient:\\n            i0 = 0\\n            for bi, length in enumerate(s_len):\\n                # Apply the rotation\\n                s_points[i0:i0 + length, :] = np.sum(np.expand_dims(s_points[i0:i0 + length, :], 2) * R[bi].T, axis=1)\\n                i0 += length\\n        return s_points, s_len, s_features\\n\\n    elif (features is None):\\n        s_points, s_len, s_labels = cpp_subsampling.subsample_batch(points,\\n                                                                    batches_len,\\n                                                                    classes=labels,\\n                                                                    sampleDl=sampleDl,\\n                                                                    max_p=max_p,\\n                                                                    verbose=verbose)\\n        if random_grid_orient:\\n            i0 = 0\\n            for bi, length in enumerate(s_len):\\n                # Apply the rotation\\n                s_points[i0:i0 + length, :] = np.sum(np.expand_dims(s_points[i0:i0 + length, :], 2) * R[bi].T, axis=1)\\n                i0 += length\\n        return s_points, s_len, s_labels\\n\\n    else:\\n        s_points, s_len, s_features, s_labels = cpp_subsampling.subsample_batch(points,\\n                                                                              batches_len,\\n                                                                              features=features,\\n                                                                              classes=labels,\\n                                                                              sampleDl=sampleDl,\\n                                                                              max_p=max_p,\\n                                                                              verbose=verbose)\\n        if random_grid_orient:\\n            i0 = 0\\n            for bi, length in enumerate(s_len):\\n                # Apply the rotation\\n                s_points[i0:i0 + length, :] = np.sum(np.expand_dims(s_points[i0:i0 + length, :], 2) * R[bi].T, axis=1)\\n                i0 += length\\n        return s_points, s_len, s_features, s_labels\\n\\n\\ndef batch_neighbors(queries, supports, q_batches, s_batches, radius):\\n    \"\"\"\\n    Computes neighbors for a batch of queries and supports\\n    :param queries: (N1, 3) the query points\\n    :param supports: (N2, 3) the support points\\n    :param q_batches: (B) the list of lengths of batch elements in queries\\n    :param s_batches: (B)the list of lengths of batch elements in supports\\n    :param radius: float32\\n    :return: neighbors indices\\n    \"\"\"\\n\\n    return cpp_neighbors.batch_query(queries, supports, q_batches, s_batches, radius=radius)\\n\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Class definition\\n#       \\\\**********************/\\n\\n\\nclass PointCloudDataset(Dataset):\\n    \"\"\"Parent class for Point Cloud Datasets.\"\"\"\\n\\n    def __init__(self, name):\\n        \"\"\"\\n        Initialize parameters of the dataset here.\\n        \"\"\"\\n\\n        self.name = name\\n        self.path = \\'\\'\\n        self.label_to_names = {}\\n        self.num_classes = 0\\n        self.label_values = np.zeros((0,), dtype=np.int32)\\n        self.label_names = []\\n        self.label_to_idx = {}\\n        self.name_to_label = {}\\n        self.config = Config()\\n        self.neighborhood_limits = []\\n\\n        return\\n\\n    def __len__(self):\\n        \"\"\"\\n        Return the length of data here\\n        \"\"\"\\n        return 0\\n\\n    def __getitem__(self, idx):\\n        \"\"\"\\n        Return the item at the given index\\n        \"\"\"\\n\\n        return 0\\n\\n    def init_labels(self):\\n\\n        # Initialize all label parameters given the label_to_names dict\\n        self.num_classes = len(self.label_to_names)\\n        self.label_values = np.sort([k for k, v in self.label_to_names.items()])\\n        self.label_names = [self.label_to_names[k] for k in self.label_values]\\n        self.label_to_idx = {l: i for i, l in enumerate(self.label_values)}\\n        self.name_to_label = {v: k for k, v in self.label_to_names.items()}\\n        self.valid_labels = np.array([label for label in self.label_values if label not in self.ignored_labels])\\n\\n    def augmentation_transform(self, points, normals=None, verbose=False):\\n        \"\"\"Implementation of an augmentation transform for point clouds.\"\"\"\\n\\n        ##########\\n        # Rotation\\n        ##########\\n\\n        # Initialize rotation matrix\\n        R = np.eye(points.shape[1])\\n\\n        if points.shape[1] == 3:\\n            if self.config.augment_rotation == \\'vertical\\':\\n\\n                # Create random rotations\\n                theta = np.random.rand() * 2 * np.pi\\n                c, s = np.cos(theta), np.sin(theta)\\n                R = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]], dtype=np.float32)\\n\\n            elif self.config.augment_rotation == \\'all\\':\\n\\n                # Choose two random angles for the first vector in polar coordinates\\n                theta = np.random.rand() * 2 * np.pi\\n                phi = (np.random.rand() - 0.5) * np.pi\\n\\n                # Create the first vector in carthesian coordinates\\n                u = np.array([np.cos(theta) * np.cos(phi), np.sin(theta) * np.cos(phi), np.sin(phi)])\\n\\n                # Choose a random rotation angle\\n                alpha = np.random.rand() * 2 * np.pi\\n\\n                # Create the rotation matrix with this vector and angle\\n                R = create_3D_rotations(np.reshape(u, (1, -1)), np.reshape(alpha, (1, -1)))[0]\\n\\n        R = R.astype(np.float32)\\n\\n        #######\\n        # Scale\\n        #######\\n\\n        # Choose random scales for each example\\n        min_s = self.config.augment_scale_min\\n        max_s = self.config.augment_scale_max\\n        if self.config.augment_scale_anisotropic:\\n            scale = np.random.rand(points.shape[1]) * (max_s - min_s) + min_s\\n        else:\\n            scale = np.random.rand() * (max_s - min_s) - min_s\\n\\n        # Add random symmetries to the scale factor\\n        symmetries = np.array(self.config.augment_symmetries).astype(np.int32)\\n        symmetries *= np.random.randint(2, size=points.shape[1])\\n        scale = (scale * (1 - symmetries * 2)).astype(np.float32)\\n\\n        #######\\n        # Noise\\n        #######\\n\\n        noise = (np.random.randn(points.shape[0], points.shape[1]) * self.config.augment_noise).astype(np.float32)\\n\\n        ##################\\n        # Apply transforms\\n        ##################\\n\\n        # Do not use np.dot because it is multi-threaded\\n        #augmented_points = np.dot(points, R) * scale + noise\\n        augmented_points = np.sum(np.expand_dims(points, 2) * R, axis=1) * scale + noise\\n\\n\\n        if normals is None:\\n            return augmented_points, scale, R\\n        else:\\n            # Anisotropic scale of the normals thanks to cross product formula\\n            normal_scale = scale[[1, 2, 0]] * scale[[2, 0, 1]]\\n            augmented_normals = np.dot(normals, R) * normal_scale\\n            # Renormalise\\n            augmented_normals *= 1 / (np.linalg.norm(augmented_normals, axis=1, keepdims=True) + 1e-6)\\n\\n            if verbose:\\n                test_p = [np.vstack([points, augmented_points])]\\n                test_n = [np.vstack([normals, augmented_normals])]\\n                test_l = [np.hstack([points[:, 2]*0, augmented_points[:, 2]*0+1])]\\n                show_ModelNet_examples(test_p, test_n, test_l)\\n\\n            return augmented_points, augmented_normals, scale, R\\n\\n    def big_neighborhood_filter(self, neighbors, layer):\\n        \"\"\"\\n        Filter neighborhoods with max number of neighbors. Limit is set to keep XX% of the neighborhoods untouched.\\n        Limit is computed at initialization\\n        \"\"\"\\n\\n        # crop neighbors matrix\\n        if len(self.neighborhood_limits) > 0:\\n            return neighbors[:, :self.neighborhood_limits[layer]]\\n        else:\\n            return neighbors\\n\\n    def classification_inputs(self,\\n                              stacked_points,\\n                              stacked_features,\\n                              labels,\\n                              stack_lengths):\\n\\n        # Starting radius of convolutions\\n        r_normal = self.config.first_subsampling_dl * self.config.conv_radius\\n\\n        # Starting layer\\n        layer_blocks = []\\n\\n        # Lists of inputs\\n        input_points = []\\n        input_neighbors = []\\n        input_pools = []\\n        input_stack_lengths = []\\n        deform_layers = []\\n\\n        ######################\\n        # Loop over the blocks\\n        ######################\\n\\n        arch = self.config.architecture\\n\\n        for block_i, block in enumerate(arch):\\n\\n            # Get all blocks of the layer\\n            if not (\\'pool\\' in block or \\'strided\\' in block or \\'global\\' in block or \\'upsample\\' in block):\\n                layer_blocks += [block]\\n                continue\\n\\n            # Convolution neighbors indices\\n            # *****************************\\n\\n            deform_layer = False\\n            if layer_blocks:\\n                # Convolutions are done in this layer, compute the neighbors with the good radius\\n                if np.any([\\'deformable\\' in blck for blck in layer_blocks]):\\n                    r = r_normal * self.config.deform_radius / self.config.conv_radius\\n                    deform_layer = True\\n                else:\\n                    r = r_normal\\n                conv_i = batch_neighbors(stacked_points, stacked_points, stack_lengths, stack_lengths, r)\\n\\n            else:\\n                # This layer only perform pooling, no neighbors required\\n                conv_i = np.zeros((0, 1), dtype=np.int32)\\n\\n            # Pooling neighbors indices\\n            # *************************\\n\\n            # If end of layer is a pooling operation\\n            if \\'pool\\' in block or \\'strided\\' in block:\\n\\n                # New subsampling length\\n                dl = 2 * r_normal / self.config.conv_radius\\n\\n                # Subsampled points\\n                pool_p, pool_b = batch_grid_subsampling(stacked_points, stack_lengths, sampleDl=dl)\\n\\n                # Radius of pooled neighbors\\n                if \\'deformable\\' in block:\\n                    r = r_normal * self.config.deform_radius / self.config.conv_radius\\n                    deform_layer = True\\n                else:\\n                    r = r_normal\\n\\n                # Subsample indices\\n                pool_i = batch_neighbors(pool_p, stacked_points, pool_b, stack_lengths, r)\\n\\n            else:\\n                # No pooling in the end of this layer, no pooling indices required\\n                pool_i = np.zeros((0, 1), dtype=np.int32)\\n                pool_p = np.zeros((0, 1), dtype=np.float32)\\n                pool_b = np.zeros((0,), dtype=np.int32)\\n\\n            # Reduce size of neighbors matrices by eliminating furthest point\\n            conv_i = self.big_neighborhood_filter(conv_i, len(input_points))\\n            pool_i = self.big_neighborhood_filter(pool_i, len(input_points))\\n\\n            # Updating input lists\\n            input_points += [stacked_points]\\n            input_neighbors += [conv_i.astype(np.int64)]\\n            input_pools += [pool_i.astype(np.int64)]\\n            input_stack_lengths += [stack_lengths]\\n            deform_layers += [deform_layer]\\n\\n            # New points for next layer\\n            stacked_points = pool_p\\n            stack_lengths = pool_b\\n\\n            # Update radius and reset blocks\\n            r_normal *= 2\\n            layer_blocks = []\\n\\n            # Stop when meeting a global pooling or upsampling\\n            if \\'global\\' in block or \\'upsample\\' in block:\\n                break\\n\\n        ###############\\n        # Return inputs\\n        ###############\\n\\n        # Save deform layers\\n\\n        # list of network inputs\\n        li = input_points + input_neighbors + input_pools + input_stack_lengths\\n        li += [stacked_features, labels]\\n\\n        return li\\n\\n\\n    def segmentation_inputs(self,\\n                            stacked_points,\\n                            stacked_features,\\n                            labels,\\n                            stack_lengths):\\n\\n        # Starting radius of convolutions\\n        r_normal = self.config.first_subsampling_dl * self.config.conv_radius\\n\\n        # Starting layer\\n        layer_blocks = []\\n\\n        # Lists of inputs\\n        input_points = []\\n        input_neighbors = []\\n        input_pools = []\\n        input_upsamples = []\\n        input_stack_lengths = []\\n        deform_layers = []\\n\\n        ######################\\n        # Loop over the blocks\\n        ######################\\n\\n        arch = self.config.architecture\\n\\n        for block_i, block in enumerate(arch):\\n\\n            # Get all blocks of the layer\\n            if not (\\'pool\\' in block or \\'strided\\' in block or \\'global\\' in block or \\'upsample\\' in block):\\n                layer_blocks += [block]\\n                continue\\n\\n            # Convolution neighbors indices\\n            # *****************************\\n\\n            deform_layer = False\\n            if layer_blocks:\\n                # Convolutions are done in this layer, compute the neighbors with the good radius\\n                if np.any([\\'deformable\\' in blck for blck in layer_blocks]):\\n                    r = r_normal * self.config.deform_radius / self.config.conv_radius\\n                    deform_layer = True\\n                else:\\n                    r = r_normal\\n                conv_i = batch_neighbors(stacked_points, stacked_points, stack_lengths, stack_lengths, r)\\n\\n            else:\\n                # This layer only perform pooling, no neighbors required\\n                conv_i = np.zeros((0, 1), dtype=np.int32)\\n\\n            # Pooling neighbors indices\\n            # *************************\\n\\n            # If end of layer is a pooling operation\\n            if \\'pool\\' in block or \\'strided\\' in block:\\n\\n                # New subsampling length\\n                dl = 2 * r_normal / self.config.conv_radius\\n\\n                # Subsampled points\\n                pool_p, pool_b = batch_grid_subsampling(stacked_points, stack_lengths, sampleDl=dl)\\n\\n                # Radius of pooled neighbors\\n                if \\'deformable\\' in block:\\n                    r = r_normal * self.config.deform_radius / self.config.conv_radius\\n                    deform_layer = True\\n                else:\\n                    r = r_normal\\n\\n                # Subsample indices\\n                pool_i = batch_neighbors(pool_p, stacked_points, pool_b, stack_lengths, r)\\n\\n                # Upsample indices (with the radius of the next layer to keep wanted density)\\n                up_i = batch_neighbors(stacked_points, pool_p, stack_lengths, pool_b, 2 * r)\\n\\n            else:\\n                # No pooling in the end of this layer, no pooling indices required\\n                pool_i = np.zeros((0, 1), dtype=np.int32)\\n                pool_p = np.zeros((0, 3), dtype=np.float32)\\n                pool_b = np.zeros((0,), dtype=np.int32)\\n                up_i = np.zeros((0, 1), dtype=np.int32)\\n\\n            # Reduce size of neighbors matrices by eliminating furthest point\\n            conv_i = self.big_neighborhood_filter(conv_i, len(input_points))\\n            pool_i = self.big_neighborhood_filter(pool_i, len(input_points))\\n            if up_i.shape[0] > 0:\\n                up_i = self.big_neighborhood_filter(up_i, len(input_points)+1)\\n\\n            # Updating input lists\\n            input_points += [stacked_points]\\n            input_neighbors += [conv_i.astype(np.int64)]\\n            input_pools += [pool_i.astype(np.int64)]\\n            input_upsamples += [up_i.astype(np.int64)]\\n            input_stack_lengths += [stack_lengths]\\n            deform_layers += [deform_layer]\\n\\n            # New points for next layer\\n            stacked_points = pool_p\\n            stack_lengths = pool_b\\n\\n            # Update radius and reset blocks\\n            r_normal *= 2\\n            layer_blocks = []\\n\\n            # Stop when meeting a global pooling or upsampling\\n            if \\'global\\' in block or \\'upsample\\' in block:\\n                break\\n\\n        ###############\\n        # Return inputs\\n        ###############\\n\\n        # list of network inputs\\n        li = input_points + input_neighbors + input_pools + input_upsamples + input_stack_lengths\\n        li += [stacked_features, labels]\\n\\n        return li\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFile Name: LAS.py\\nContents of FileLAS.py: #\\n#\\n#      0=================================0\\n#      |    Kernel Point Convolutions    |\\n#      0=================================0\\n#\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Class handling LAS dataset.\\n#      Implements a Dataset, a Sampler, and a collate_fn\\n#\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#      Derived from S3DIS.py by Hugues THOMAS - 11/06/2018\\n#      Brad CHAMBERS - 05/13/2021\\n#\\n\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Imports and global variables\\n#       \\\\**********************************/\\n#\\n\\n# Common libs\\nimport time\\nimport numpy as np\\nimport pickle\\nimport torch\\nimport math\\nfrom multiprocessing import Lock\\n\\n\\n# OS functions\\nfrom os import listdir, makedirs\\nfrom os.path import exists, join, isdir\\n\\n# Dataset parent class\\nfrom datasets.common import PointCloudDataset\\nfrom torch.utils.data import Sampler, get_worker_info\\n# from utils.mayavi_visu import *\\n\\nfrom datasets.common import grid_subsampling\\nfrom utils.config import bcolors\\nfrom utils.las import read_processed_las, read_raw_las, read_subsampled_las, write_las\\nfrom sklearn.neighbors import KDTree\\n\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Dataset class definition\\n#       \\\\******************************/\\n\\n\\nclass LASDataset(PointCloudDataset):\\n    \"\"\"Class to handle LAS dataset.\"\"\"\\n\\n    def __init__(self, config, set=\\'training\\', use_potentials=True, load_data=True):\\n        \"\"\"\\n        This dataset is small enough to be stored in-memory, so load all point clouds here\\n        \"\"\"\\n        PointCloudDataset.__init__(self, \\'LAS\\')\\n\\n        ############\\n        # Parameters\\n        ############\\n\\n        # Dict from labels to names\\n        self.label_to_names = {0: \\'never_classified\\',\\n                               1: \\'unassigned\\',\\n                               2: \\'ground\\',\\n                               3: \\'low_vegetation\\',\\n                               4: \\'medium_vegetation\\',\\n                               5: \\'high_vegetation\\',\\n                               6: \\'building\\',\\n                               7: \\'low_point\\',\\n                               8: \\'model_keypoint\\',\\n                               9: \\'water\\',\\n                               10: \\'rail\\',\\n                               11: \\'road_surface\\',\\n                               12: \\'overlap\\',\\n                               13: \\'wire_guard\\',\\n                               14: \\'wire_conductor\\',\\n                               15: \\'transmission_tower\\',\\n                               16: \\'wire_structure_connector\\',\\n                               17: \\'bridge_deck\\',\\n                               18: \\'high_noise\\'}\\n\\n        # List of classes ignored during training (can be empty)\\n        self.ignored_labels = np.array([1,3,4,7,8,10,11,12,13,14,15,16,18])\\n\\n        # Initialize a bunch of variables concerning class labels\\n        self.init_labels()\\n\\n        # Type of task conducted on this dataset\\n        self.dataset_task = \\'cloud_segmentation\\'\\n\\n        # Update number of class and data task in configuration\\n        config.num_classes = self.num_classes - len(self.ignored_labels)\\n        config.dataset_task = self.dataset_task\\n\\n        # Parameters from config\\n        self.config = config\\n\\n        # Training or test set\\n        self.set = set\\n\\n        # Using potential or random epoch generation\\n        self.use_potentials = use_potentials\\n\\n        # Path of the training files\\n        self.train_path = join(self.config.path, \\'train\\')\\n        self.test_path = join(self.config.path, \\'validation\\')\\n        # self.test_path = join(self.config.path, \\'test\\')\\n        # self.test_path = join(self.config.path, \\'predict\\')\\n\\n        # Proportion of validation scenes\\n        self.train_files = np.sort([join(self.train_path, f) for f in listdir(self.train_path) if f[-4:] == \\'.laz\\'])\\n        self.test_files = np.sort([join(self.test_path, f) for f in listdir(self.test_path) if f[-4:] == \\'.laz\\'])\\n        self.all_splits = [i for i in range(len(self.train_files))]\\n        self.validation_split = len(self.train_files)-1\\n\\n        # Number of models used per epoch\\n        if self.set == \\'training\\':\\n            self.epoch_n = config.epoch_steps * config.batch_num\\n        elif self.set in [\\'validation\\', \\'test\\', \\'ERF\\']:\\n            self.epoch_n = config.validation_size * config.batch_num\\n        else:\\n            raise ValueError(\\'Unknown set for LAS data: \\', self.set)\\n\\n        ################\\n        # Load las files\\n        ################\\n\\n        if self.set == \\'training\\':\\n            self.files = self.train_files\\n        elif self.set in [\\'validation\\', \\'test\\', \\'ERF\\']:\\n            self.files = self.test_files\\n        else:\\n            raise ValueError(\\'Unknown set for LAS data: \\', self.set)\\n\\n        if 0 < self.config.first_subsampling_dl <= 0.01:\\n            raise ValueError(\\'subsampling_parameter too low (should be over 1 cm\\')\\n\\n        # Initiate containers\\n        self.input_trees = []\\n        self.input_features = []\\n        self.input_labels = []\\n        self.pot_trees = []\\n        self.num_clouds = 0\\n        self.test_proj = []\\n        self.validation_labels = []\\n\\n        # Start loading\\n        self.load_subsampled_clouds()\\n\\n        ############################\\n        # Batch selection parameters\\n        ############################\\n\\n        # Initialize value for batch limit (max number of points per batch).\\n        self.batch_limit = torch.tensor([1], dtype=torch.float32)\\n        self.batch_limit.share_memory_()\\n\\n        # Initialize potentials\\n        if use_potentials:\\n            self.potentials = []\\n            self.min_potentials = []\\n            self.argmin_potentials = []\\n            for i, tree in enumerate(self.pot_trees):\\n                self.potentials += [torch.from_numpy(np.random.rand(tree.data.shape[0]) * 1e-3)]\\n                min_ind = int(torch.argmin(self.potentials[-1]))\\n                self.argmin_potentials += [min_ind]\\n                self.min_potentials += [float(self.potentials[-1][min_ind])]\\n\\n            # Share potential memory\\n            self.argmin_potentials = torch.from_numpy(np.array(self.argmin_potentials, dtype=np.int64))\\n            self.min_potentials = torch.from_numpy(np.array(self.min_potentials, dtype=np.float64))\\n            self.argmin_potentials.share_memory_()\\n            self.min_potentials.share_memory_()\\n            for i, _ in enumerate(self.pot_trees):\\n                self.potentials[i].share_memory_()\\n\\n            self.worker_waiting = torch.tensor([0 for _ in range(config.input_threads)], dtype=torch.int32)\\n            self.worker_waiting.share_memory_()\\n            self.epoch_inds = None\\n            self.epoch_i = 0\\n\\n        else:\\n            self.potentials = None\\n            self.min_potentials = None\\n            self.argmin_potentials = None\\n            self.epoch_inds = torch.from_numpy(np.zeros((2, self.epoch_n), dtype=np.int64))\\n            self.epoch_i = torch.from_numpy(np.zeros((1,), dtype=np.int64))\\n            self.epoch_i.share_memory_()\\n            self.epoch_inds.share_memory_()\\n\\n        self.worker_lock = Lock()\\n\\n        # For ERF visualization, we want only one cloud per batch and no randomness\\n        if self.set == \\'ERF\\':\\n            self.batch_limit = torch.tensor([1], dtype=torch.float32)\\n            self.batch_limit.share_memory_()\\n            np.random.seed(42)\\n\\n        return\\n\\n    def __len__(self):\\n        \"\"\"\\n        Return the length of data here\\n        \"\"\"\\n        return len(self.cloud_names)\\n\\n    def __getitem__(self, batch_i):\\n        \"\"\"\\n        The main thread gives a list of indices to load a batch. Each worker is going to work in parallel to load a\\n        different list of indices.\\n        \"\"\"\\n\\n        if self.use_potentials:\\n            return self.potential_item(batch_i)\\n        else:\\n            return self.random_item(batch_i)\\n\\n    def potential_item(self, batch_i, debug_workers=False):\\n\\n        t = [time.time()]\\n\\n        # Initiate concatanation lists\\n        p_list = []\\n        f_list = []\\n        l_list = []\\n        i_list = []\\n        pi_list = []\\n        ci_list = []\\n        s_list = []\\n        R_list = []\\n        batch_n = 0\\n\\n        info = get_worker_info()\\n        if info is not None:\\n            wid = info.id\\n        else:\\n            wid = None\\n\\n        while True:\\n\\n            t += [time.time()]\\n\\n            if debug_workers:\\n                message = \\'\\'\\n                for wi in range(info.num_workers):\\n                    if wi == wid:\\n                        message += \\' {:}X{:} \\'.format(bcolors.FAIL, bcolors.ENDC)\\n                    elif self.worker_waiting[wi] == 0:\\n                        message += \\'   \\'\\n                    elif self.worker_waiting[wi] == 1:\\n                        message += \\' | \\'\\n                    elif self.worker_waiting[wi] == 2:\\n                        message += \\' o \\'\\n                print(message)\\n                self.worker_waiting[wid] = 0\\n\\n            with self.worker_lock:\\n\\n                if debug_workers:\\n                    message = \\'\\'\\n                    for wi in range(info.num_workers):\\n                        if wi == wid:\\n                            message += \\' {:}v{:} \\'.format(bcolors.OKGREEN, bcolors.ENDC)\\n                        elif self.worker_waiting[wi] == 0:\\n                            message += \\'   \\'\\n                        elif self.worker_waiting[wi] == 1:\\n                            message += \\' | \\'\\n                        elif self.worker_waiting[wi] == 2:\\n                            message += \\' o \\'\\n                    print(message)\\n                    self.worker_waiting[wid] = 1\\n\\n                # Get potential minimum\\n                cloud_ind = int(torch.argmin(self.min_potentials))\\n                point_ind = int(self.argmin_potentials[cloud_ind])\\n\\n                # Get potential points from tree structure\\n                pot_points = np.array(self.pot_trees[cloud_ind].data, copy=False)\\n\\n                # Center point of input region\\n                center_point = pot_points[point_ind, :].reshape(1, -1)\\n\\n                # Add a small noise to center point\\n                if self.set != \\'ERF\\':\\n                    center_point += np.random.normal(scale=self.config.in_radius / 10, size=center_point.shape)\\n\\n                # Indices of points in input region\\n                pot_inds, dists = self.pot_trees[cloud_ind].query_radius(center_point,\\n                                                                         r=self.config.in_radius,\\n                                                                         return_distance=True)\\n\\n                d2s = np.square(dists[0])\\n                pot_inds = pot_inds[0]\\n\\n                # Update potentials (Tukey weights)\\n                if self.set != \\'ERF\\':\\n                    tukeys = np.square(1 - d2s / np.square(self.config.in_radius))\\n                    tukeys[d2s > np.square(self.config.in_radius)] = 0\\n                    self.potentials[cloud_ind][pot_inds] += tukeys\\n                    min_ind = torch.argmin(self.potentials[cloud_ind])\\n                    self.min_potentials[[cloud_ind]] = self.potentials[cloud_ind][min_ind]\\n                    self.argmin_potentials[[cloud_ind]] = min_ind\\n\\n            t += [time.time()]\\n\\n            # Get points from tree structure\\n            points = np.array(self.input_trees[cloud_ind].data, copy=False)\\n\\n\\n            # Indices of points in input region\\n            input_inds = self.input_trees[cloud_ind].query_radius(center_point,\\n                                                                  r=self.config.in_radius)[0]#,\\n                                                                #   return_distance=True,\\n                                                                #   sort_results=True)\\n\\n            # print(input_inds[0].shape)\\n            # input_inds = input_inds[0][::30]\\n            # print(input_inds.shape)\\n\\n            t += [time.time()]\\n\\n            # Number collected\\n            n = input_inds.shape[0]\\n\\n            # print(n)\\n            if n<2:\\n                print(\"Seems bad, try to continue without this sample\")\\n                continue\\n\\n            # Collect labels and colors\\n            input_points = (points[input_inds] - center_point).astype(np.float32)\\n            input_features = self.input_features[cloud_ind][input_inds]\\n            if self.set in [\\'test\\', \\'ERF\\']:\\n                input_labels = np.zeros(input_points.shape[0])\\n            else:\\n                input_labels = self.input_labels[cloud_ind][input_inds]\\n                # print(np.unique(input_labels))\\n                # print(self.label_to_idx)\\n                input_labels = np.array([self.label_to_idx[l] for l in input_labels])\\n\\n            t += [time.time()]\\n\\n            # Data augmentation\\n            input_points, scale, R = self.augmentation_transform(input_points)\\n\\n            # Color augmentation\\n            if np.random.rand() > self.config.augment_color:\\n               input_features *= 0\\n\\n            # Get original height as additional feature\\n            # input_features = np.hstack((input_features, input_points[:, 2:] + center_point[:, 2:])).astype(np.float32)\\n            # input_features = np.hstack((input_points[:, 2:] + center_point[:, 2:])).astype(np.float32)\\n\\n            t += [time.time()]\\n\\n            # Stack batch\\n            p_list += [input_points]\\n            f_list += [input_features.astype(np.float32)]\\n            l_list += [input_labels]\\n            pi_list += [input_inds]\\n            i_list += [point_ind]\\n            ci_list += [cloud_ind]\\n            s_list += [scale]\\n            R_list += [R]\\n\\n            # Update batch size\\n            batch_n += n\\n\\n            # In case batch is full, stop\\n            if batch_n > int(self.batch_limit):\\n                break\\n\\n            # Randomly drop some points (act as an augmentation process and a safety for GPU memory consumption)\\n            # if n > int(self.batch_limit):\\n            #    input_inds = np.random.choice(input_inds, size=int(self.batch_limit) - 1, replace=False)\\n            #    n = input_inds.shape[0]\\n\\n        ###################\\n        # Concatenate batch\\n        ###################\\n\\n        stacked_points = np.concatenate(p_list, axis=0)\\n        features = np.concatenate(f_list, axis=0)\\n        labels = np.concatenate(l_list, axis=0)\\n        point_inds = np.array(i_list, dtype=np.int32)\\n        cloud_inds = np.array(ci_list, dtype=np.int32)\\n        input_inds = np.concatenate(pi_list, axis=0)\\n        stack_lengths = np.array([pp.shape[0] for pp in p_list], dtype=np.int32)\\n        scales = np.array(s_list, dtype=np.float32)\\n        rots = np.stack(R_list, axis=0)\\n\\n        # Input features\\n        stacked_features = np.ones_like(stacked_points[:, :1], dtype=np.float32)\\n        if self.config.in_features_dim == 1:\\n            pass\\n        else:\\n            stacked_features = np.hstack((stacked_features, features))\\n        # elif self.config.in_features_dim == 2:\\n        #     stacked_features = np.hstack((stacked_features, features[:, :1]))\\n        # elif self.config.in_features_dim == 3:\\n        #     stacked_features = np.hstack((stacked_features, features[:, :2]))\\n        # elif self.config.in_features_dim == 4:\\n        #     stacked_features = np.hstack((stacked_features, features[:, :3]))\\n        # elif self.config.in_features_dim == 5:\\n        #     stacked_features = np.hstack((stacked_features, features[:, :4]))\\n        # else:\\n        #     raise ValueError(\\'Only accepted input dimensions are 1, 4 and 7 (without and with XYZ)\\')\\n\\n        #######################\\n        # Create network inputs\\n        #######################\\n        #\\n        #   Points, neighbors, pooling indices for each layers\\n        #\\n\\n        t += [time.time()]\\n\\n        # Get the whole input list\\n        input_list = self.segmentation_inputs(stacked_points,\\n                                              stacked_features,\\n                                              labels,\\n                                              stack_lengths)\\n\\n        t += [time.time()]\\n\\n        # Add scale and rotation for testing\\n        input_list += [scales, rots, cloud_inds, point_inds, input_inds]\\n\\n        if debug_workers:\\n            message = \\'\\'\\n            for wi in range(info.num_workers):\\n                if wi == wid:\\n                    message += \\' {:}0{:} \\'.format(bcolors.OKBLUE, bcolors.ENDC)\\n                elif self.worker_waiting[wi] == 0:\\n                    message += \\'   \\'\\n                elif self.worker_waiting[wi] == 1:\\n                    message += \\' | \\'\\n                elif self.worker_waiting[wi] == 2:\\n                    message += \\' o \\'\\n            print(message)\\n            self.worker_waiting[wid] = 2\\n\\n        t += [time.time()]\\n\\n        # Display timings\\n        debugT = False\\n        if debugT:\\n            print(\\'\\\\n************************\\\\n\\')\\n            print(\\'Timings:\\')\\n            ti = 0\\n            N = 5\\n            mess = \\'Init ...... {:5.1f}ms /\\'\\n            loop_times = [1000 * (t[ti + N * i + 1] - t[ti + N * i]) for i in range(len(stack_lengths))]\\n            for dt in loop_times:\\n                mess += \\' {:5.1f}\\'.format(dt)\\n            print(mess.format(np.sum(loop_times)))\\n            ti += 1\\n            mess = \\'Pots ...... {:5.1f}ms /\\'\\n            loop_times = [1000 * (t[ti + N * i + 1] - t[ti + N * i]) for i in range(len(stack_lengths))]\\n            for dt in loop_times:\\n                mess += \\' {:5.1f}\\'.format(dt)\\n            print(mess.format(np.sum(loop_times)))\\n            ti += 1\\n            mess = \\'Sphere .... {:5.1f}ms /\\'\\n            loop_times = [1000 * (t[ti + N * i + 1] - t[ti + N * i]) for i in range(len(stack_lengths))]\\n            for dt in loop_times:\\n                mess += \\' {:5.1f}\\'.format(dt)\\n            print(mess.format(np.sum(loop_times)))\\n            ti += 1\\n            mess = \\'Collect ... {:5.1f}ms /\\'\\n            loop_times = [1000 * (t[ti + N * i + 1] - t[ti + N * i]) for i in range(len(stack_lengths))]\\n            for dt in loop_times:\\n                mess += \\' {:5.1f}\\'.format(dt)\\n            print(mess.format(np.sum(loop_times)))\\n            ti += 1\\n            mess = \\'Augment ... {:5.1f}ms /\\'\\n            loop_times = [1000 * (t[ti + N * i + 1] - t[ti + N * i]) for i in range(len(stack_lengths))]\\n            for dt in loop_times:\\n                mess += \\' {:5.1f}\\'.format(dt)\\n            print(mess.format(np.sum(loop_times)))\\n            ti += N * (len(stack_lengths) - 1) + 1\\n            print(\\'concat .... {:5.1f}ms\\'.format(1000 * (t[ti+1] - t[ti])))\\n            ti += 1\\n            print(\\'input ..... {:5.1f}ms\\'.format(1000 * (t[ti+1] - t[ti])))\\n            ti += 1\\n            print(\\'stack ..... {:5.1f}ms\\'.format(1000 * (t[ti+1] - t[ti])))\\n            ti += 1\\n            print(\\'\\\\n************************\\\\n\\')\\n        return input_list\\n\\n    def random_item(self, batch_i):\\n\\n        # Initiate concatanation lists\\n        p_list = []\\n        f_list = []\\n        l_list = []\\n        i_list = []\\n        pi_list = []\\n        ci_list = []\\n        s_list = []\\n        R_list = []\\n        batch_n = 0\\n\\n        while True:\\n\\n            with self.worker_lock:\\n\\n                if self.epoch_i >= self.epoch_n:\\n                    # good idea?\\n                    break\\n                    # raise IndexError(\\'{} exceeded {}\\'.format(self.epoch_i, self.epoch_n))\\n\\n                # Get potential minimum\\n                cloud_ind = int(self.epoch_inds[0, self.epoch_i])\\n                point_ind = int(self.epoch_inds[1, self.epoch_i])\\n\\n                # Update epoch indice\\n                self.epoch_i += 1\\n\\n            # Get points from tree structure\\n            points = np.array(self.input_trees[cloud_ind].data, copy=False)\\n\\n            # Center point of input region\\n            center_point = points[point_ind, :].reshape(1, -1)\\n\\n            # Add a small noise to center point\\n            if self.set != \\'ERF\\':\\n                center_point += np.random.normal(scale=self.config.in_radius / 10, size=center_point.shape)\\n\\n            # Indices of points in input region\\n            input_inds = self.input_trees[cloud_ind].query_radius(center_point,\\n                                                                  r=self.config.in_radius)[0]\\n\\n            # Number collected\\n            n = input_inds.shape[0]\\n\\n            # Collect labels and colors\\n            input_points = (points[input_inds] - center_point).astype(np.float32)\\n            input_features = self.input_features[cloud_ind][input_inds]\\n            if self.set in [\\'test\\', \\'ERF\\']:\\n                input_labels = np.zeros(input_points.shape[0])\\n            else:\\n                input_labels = self.input_labels[cloud_ind][input_inds]\\n                input_labels = np.array([self.label_to_idx[l] for l in input_labels])\\n\\n            # Data augmentation\\n            input_points, scale, R = self.augmentation_transform(input_points)\\n\\n            # Color augmentation\\n            if np.random.rand() > self.config.augment_color:\\n               input_features *= 0\\n\\n            # Get original height as additional feature\\n            # input_features = np.hstack((input_features, input_points[:, 2:] + center_point[:, 2:])).astype(np.float32)\\n            # input_features = np.hstack((input_points[:, 2:] + center_point[:, 2:])).astype(np.float32)\\n\\n            # Stack batch\\n            p_list += [input_points]\\n            f_list += [input_features.astype(np.float32)]\\n            l_list += [input_labels]\\n            pi_list += [input_inds]\\n            i_list += [point_ind]\\n            ci_list += [cloud_ind]\\n            s_list += [scale]\\n            R_list += [R]\\n\\n            # Update batch size\\n            batch_n += n\\n\\n            # In case batch is full, stop\\n            if batch_n > int(self.batch_limit):\\n                break\\n\\n            # Randomly drop some points (act as an augmentation process and a safety for GPU memory consumption)\\n            # if n > int(self.batch_limit):\\n            #    input_inds = np.random.choice(input_inds, size=int(self.batch_limit) - 1, replace=False)\\n            #    n = input_inds.shape[0]\\n\\n        ###################\\n        # Concatenate batch\\n        ###################\\n\\n        stacked_points = np.concatenate(p_list, axis=0)\\n        features = np.concatenate(f_list, axis=0)\\n        labels = np.concatenate(l_list, axis=0)\\n        point_inds = np.array(i_list, dtype=np.int32)\\n        cloud_inds = np.array(ci_list, dtype=np.int32)\\n        input_inds = np.concatenate(pi_list, axis=0)\\n        stack_lengths = np.array([pp.shape[0] for pp in p_list], dtype=np.int32)\\n        scales = np.array(s_list, dtype=np.float32)\\n        rots = np.stack(R_list, axis=0)\\n\\n        # Input features\\n        stacked_features = np.ones_like(stacked_points[:, :1], dtype=np.float32)\\n        if self.config.in_features_dim == 1:\\n            pass\\n        else:\\n            stacked_features = np.hstack((stacked_features, features))\\n        # elif self.config.in_features_dim == 2:\\n        #     stacked_features = np.hstack((stacked_features, features[:, :1]))\\n        # elif self.config.in_features_dim == 3:\\n        #     stacked_features = np.hstack((stacked_features, features[:, :2]))\\n        # elif self.config.in_features_dim == 4:\\n        #     stacked_features = np.hstack((stacked_features, features[:, :3]))\\n        # elif self.config.in_features_dim == 5:\\n        #     stacked_features = np.hstack((stacked_features, features[:, :4]))\\n        # else:\\n        #     raise ValueError(\\'Only accepted input dimensions are 1, 4 and 7 (without and with XYZ)\\')\\n\\n        #######################\\n        # Create network inputs\\n        #######################\\n        #\\n        #   Points, neighbors, pooling indices for each layers\\n        #\\n\\n        # Get the whole input list\\n        input_list = self.segmentation_inputs(stacked_points,\\n                                              stacked_features,\\n                                              labels,\\n                                              stack_lengths)\\n\\n        # Add scale and rotation for testing\\n        input_list += [scales, rots, cloud_inds, point_inds, input_inds]\\n\\n        return input_list\\n\\n    def load_subsampled_clouds(self):\\n\\n        # Parameter\\n        dl = self.config.first_subsampling_dl\\n\\n        # Create path for files\\n        tree_path = join(self.config.path, \\'input_{:.3f}\\'.format(dl))\\n        if not exists(tree_path):\\n            makedirs(tree_path)\\n\\n        ##############\\n        # Load KDTrees\\n        ##############\\n\\n        for i, file_path in enumerate(self.files):\\n\\n            # Restart timer\\n            t0 = time.time()\\n\\n            # Get cloud name\\n            # cloud_name = self.cloud_names[i]\\n            cloud_name = file_path.split(\\'/\\')[-1][:-4]\\n\\n            # Name of the input files\\n            KDTree_file = join(tree_path, \\'{:s}_KDTree.pkl\\'.format(cloud_name))\\n            sub_las_file = join(tree_path, \\'{:s}.laz\\'.format(cloud_name))\\n\\n            # Check if inputs have already been computed\\n            if exists(KDTree_file):\\n                print(\\'\\\\nFound KDTree for cloud {:s}, subsampled at {:.3f}\\'.format(cloud_name, dl))\\n\\n                # read ply with data\\n                #data = read_ply(sub_ply_file)\\n                _, sub_features, sub_labels = read_processed_las(sub_las_file)\\n                #sub_features = np.vstack((data[\\'red\\'], data[\\'green\\'], data[\\'blue\\'])).T\\n                # intensity = np.expand_dims(data[\\'Intensity\\'], 1).astype(np.float32)\\n                # intensity = np.minimum(intensity, 255.0)/255.0\\n                # sub_features = intensity\\n                # returnnumber = np.expand_dims(data[\\'ReturnNumber\\'], 1).astype(np.float32)\\n                # returnnumber = np.minimum(returnnumber, 4.0)/4.0\\n                # sub_features = np.hstack((intensity, returnnumber))\\n                # sub_labels = data[\\'Classification\\']\\n\\n                # Read pkl with search tree\\n                with open(KDTree_file, \\'rb\\') as f:\\n                    search_tree = pickle.load(f)\\n\\n            else:\\n                print(\\'\\\\nPreparing KDTree for cloud {:s}, subsampled at {:.3f}\\'.format(cloud_name, dl))\\n\\n                # Read ply file\\n                #data = read_ply(file_path)\\n                # points, features, labels = read_raw_las(file_path)\\n                # points = np.vstack((data[\\'X\\'], data[\\'Y\\'], data[\\'Z\\'])).T\\n                #features = np.vstack((data[\\'red\\'], data[\\'green\\'], data[\\'blue\\'])).T\\n                # intensity = np.expand_dims(data[\\'Intensity\\'], 1).astype(np.float32)\\n                # intensity = np.minimum(intensity, 255.0)/255.0\\n                # features = intensity\\n                # returnnumber = np.expand_dims(data[\\'ReturnNumber\\'], 1).astype(np.float32)\\n                # returnnumber = np.minimum(returnnumber, 4.0)/4.0\\n                # features = np.hstack((intensity, returnnumber))\\n                # labels = data[\\'Classification\\']\\n\\n                # Subsample cloud\\n                sub_points, sub_features, sub_labels = read_subsampled_las(file_path, dl)\\n\\n\\n                # Rescale float color and squeeze label\\n                #sub_features = sub_features / 255\\n                sub_labels = np.squeeze(sub_labels)\\n\\n                # Get chosen neighborhoods\\n                search_tree = KDTree(sub_points, leaf_size=10)\\n                #search_tree = nnfln.KDTree(n_neighbors=1, metric=\\'L2\\', leaf_size=10)\\n                #search_tree.fit(sub_points)\\n\\n                # Save KDTree\\n                with open(KDTree_file, \\'wb\\') as f:\\n                    pickle.dump(search_tree, f)\\n\\n                # Save ply\\n                #write_ply(sub_ply_file,\\n                #          [sub_points, sub_features, sub_labels],\\n                #          [\\'x\\', \\'y\\', \\'z\\', \\'red\\', \\'green\\', \\'blue\\', \\'class\\'])\\n                # foo = np.core.records.fromarrays(np.vstack((sub_points.T,sub_features.T,sub_labels.T)),names=\\'X,Y,Z,Intensity,Classification\\',formats=\\'f8,f8,f8,u8,u1\\')\\n                # foo = np.core.records.fromarrays(np.vstack((sub_points.T,sub_features.T,sub_labels.T)),names=\\'X,Y,Z,Linearity,Planarity,Scattering,Verticality,ReturnNumber,NumberOfReturns,Classification\\',formats=\\'f8,f8,f8,f8,f8,f8,f8,f8,f8,u1\\')\\n                # foo = np.core.records.fromarrays(np.vstack((sub_points.T,sub_features.T,sub_labels.T)),names=\\'X,Y,Z,Eigenvalue0,Eigenvalue1,Eigenvalue2,Classification\\',formats=\\'f8,f8,f8,f8,f8,f8,u1\\')\\n                foo = np.core.records.fromarrays(np.vstack((sub_points.T,sub_features.T,sub_labels.T)),names=\\'X,Y,Z,Linearity,Planarity,Scattering,Verticality,Classification\\',formats=\\'f8,f8,f8,f8,f8,f8,f8,u1\\')\\n                # foo = np.core.records.fromarrays(np.vstack((sub_points.T,sub_labels.T)),names=\\'X,Y,Z,Classification\\',formats=\\'f8,f8,f8,u1\\')\\n                write_las(sub_las_file, foo)\\n\\n            # Fill data containers\\n            self.input_trees += [search_tree]\\n            self.input_features += [sub_features]\\n            self.input_labels += [sub_labels]\\n\\n            size = sub_labels.shape[0] * 4 * 4\\n            print(\\'{:.1f} MB loaded in {:.1f}s\\'.format(size * 1e-6, time.time() - t0))\\n\\n        ############################\\n        # Coarse potential locations\\n        ############################\\n\\n        # Only necessary for validation and test sets\\n        if self.use_potentials:\\n            print(\\'\\\\nPreparing potentials\\')\\n\\n            # Restart timer\\n            t0 = time.time()\\n\\n            pot_dl = self.config.in_radius / 10\\n            cloud_ind = 0\\n\\n            for i, file_path in enumerate(self.files):\\n\\n                # Get cloud name\\n                # cloud_name = self.cloud_names[i]\\n                cloud_name = file_path.split(\\'/\\')[-1][:-4]\\n\\n                # Name of the input files\\n                coarse_KDTree_file = join(tree_path, \\'{:s}_coarse_KDTree.pkl\\'.format(cloud_name))\\n\\n                # Check if inputs have already been computed\\n                if exists(coarse_KDTree_file):\\n                    # Read pkl with search tree\\n                    with open(coarse_KDTree_file, \\'rb\\') as f:\\n                        search_tree = pickle.load(f)\\n\\n                else:\\n                    # Subsample cloud\\n                    sub_points = np.array(self.input_trees[cloud_ind].data, copy=False)\\n                    coarse_points = grid_subsampling(sub_points.astype(np.float32), sampleDl=pot_dl)\\n\\n                    # Get chosen neighborhoods\\n                    search_tree = KDTree(coarse_points, leaf_size=10)\\n\\n                    # Save KDTree\\n                    with open(coarse_KDTree_file, \\'wb\\') as f:\\n                        pickle.dump(search_tree, f)\\n\\n                # Fill data containers\\n                self.pot_trees += [search_tree]\\n                cloud_ind += 1\\n\\n            print(\\'Done in {:.1f}s\\'.format(time.time() - t0))\\n\\n        ######################\\n        # Reprojection indices\\n        ######################\\n\\n        # Get number of clouds\\n        self.num_clouds = len(self.input_trees)\\n\\n        # Only necessary for validation and test sets\\n        if self.set in [\\'validation\\', \\'test\\']:\\n\\n            print(\\'\\\\nPreparing reprojection indices for testing\\')\\n\\n            # Get validation/test reprojection indices\\n            for i, file_path in enumerate(self.files):\\n\\n                # Restart timer\\n                t0 = time.time()\\n\\n                # Get info on this cloud\\n                # cloud_name = self.cloud_names[i]\\n                cloud_name = file_path.split(\\'/\\')[-1][:-4]\\n\\n                # File name for saving\\n                proj_file = join(tree_path, \\'{:s}_proj.pkl\\'.format(cloud_name))\\n\\n                # Try to load previous indices\\n                if exists(proj_file):\\n                    with open(proj_file, \\'rb\\') as f:\\n                        proj_inds, labels = pickle.load(f)\\n                else:\\n                    points, _, labels = read_raw_las(file_path)\\n                    # points = np.vstack((data[\\'X\\'], data[\\'Y\\'], data[\\'Z\\'])).T\\n                    # labels = data[\\'Classification\\']\\n\\n                    # Compute projection inds\\n                    idxs = self.input_trees[i].query(points, return_distance=False)\\n                    #dists, idxs = self.input_trees[i_cloud].kneighbors(points)\\n                    proj_inds = np.squeeze(idxs).astype(np.int32)\\n\\n                    # Save\\n                    with open(proj_file, \\'wb\\') as f:\\n                        pickle.dump([proj_inds, labels], f)\\n\\n                self.test_proj += [proj_inds]\\n                self.validation_labels += [labels]\\n                print(\\'{:s} done in {:.1f}s\\'.format(cloud_name, time.time() - t0))\\n\\n        print()\\n        return\\n\\n    def load_evaluation_points(self, file_path):\\n        \"\"\"\\n        Load points (from test or validation split) on which the metrics should be evaluated\\n        \"\"\"\\n\\n        # Get original points\\n        points, _, _ = read_raw_las(file_path)\\n        return points #np.vstack((data[\\'X\\'], data[\\'Y\\'], data[\\'Z\\'])).T\\n\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Utility classes definition\\n#       \\\\********************************/\\n\\n\\nclass LASSampler(Sampler):\\n    \"\"\"Sampler for LAS\"\"\"\\n\\n    def __init__(self, dataset: LASDataset):\\n        Sampler.__init__(self, dataset)\\n\\n        # Dataset used by the sampler (no copy is made in memory)\\n        self.dataset = dataset\\n\\n        # Number of step per epoch\\n        if dataset.set == \\'training\\':\\n            self.N = dataset.config.epoch_steps\\n        else:\\n            self.N = dataset.config.validation_size\\n\\n        return\\n\\n    def __iter__(self):\\n        \"\"\"\\n        Yield next batch indices here. In this dataset, this is a dummy sampler that yield the index of batch element\\n        (input sphere) in epoch instead of the list of point indices\\n        \"\"\"\\n\\n        if not self.dataset.use_potentials:\\n\\n            # Initiate current epoch ind\\n            self.dataset.epoch_i *= 0\\n            self.dataset.epoch_inds *= 0\\n\\n            # Initiate container for indices\\n            all_epoch_inds = np.zeros((2, 0), dtype=np.int32)\\n\\n            # Number of sphere centers taken per class in each cloud\\n            num_centers = self.N * self.dataset.config.batch_num\\n            # random_pick_n = int(np.ceil(num_centers / (self.dataset.num_clouds * self.dataset.config.num_classes)))\\n            # print(num_centers, self.dataset.num_clouds, self.dataset.config.num_classes, random_pick_n)\\n\\n            # Make a first pass through all clouds to see which ones have each label and then figure out where to sample the classes\\n            random_pick_n = np.zeros_like(self.dataset.label_values)\\n            num_clouds_per_label = np.zeros_like(self.dataset.label_values)\\n            for cloud_ind, cloud_labels in enumerate(self.dataset.input_labels):\\n                for label_ind, label in enumerate(self.dataset.label_values):\\n                    if label not in self.dataset.ignored_labels:\\n                        label_indices = np.where(np.equal(cloud_labels, label))[0]\\n                        if len(label_indices)==0:\\n                            continue\\n                        num_clouds_per_label[label_ind]+=1\\n            for label_ind, label in enumerate(self.dataset.label_values):\\n                if label not in self.dataset.ignored_labels and num_clouds_per_label[label_ind] > 0:\\n                    random_pick_n[label_ind] = int(np.ceil(num_centers/(num_clouds_per_label[label_ind]*self.dataset.config.num_classes)))\\n\\n            print(num_clouds_per_label)\\n            print(random_pick_n)\\n\\n            # Choose random points of each class for each cloud\\n            for cloud_ind, cloud_labels in enumerate(self.dataset.input_labels):\\n                epoch_indices = np.empty((0,), dtype=np.int32)\\n                for label_ind, label in enumerate(self.dataset.label_values):\\n                    if label not in self.dataset.ignored_labels:\\n                        label_indices = np.where(np.equal(cloud_labels, label))[0]\\n                        if (len(label_indices)==0):\\n                            continue\\n                        if len(label_indices) <= random_pick_n[label_ind]:\\n                            epoch_indices = np.hstack((epoch_indices, label_indices))\\n                        elif len(label_indices) < 50 * random_pick_n[label_ind]:\\n                            new_randoms = np.random.choice(label_indices, size=random_pick_n[label_ind], replace=False)\\n                            epoch_indices = np.hstack((epoch_indices, new_randoms.astype(np.int32)))\\n                        else:\\n                            rand_inds = []\\n                            while len(rand_inds) < random_pick_n[label_ind]:\\n                                rand_inds = np.unique(np.random.choice(label_indices, size=5 * random_pick_n[label_ind], replace=True))\\n                            epoch_indices = np.hstack((epoch_indices, rand_inds[:random_pick_n[label_ind]].astype(np.int32)))\\n\\n                # Stack those indices with the cloud index\\n                epoch_indices = np.vstack((np.full(epoch_indices.shape, cloud_ind, dtype=np.int32), epoch_indices))\\n                \\n                # Update the global indice container\\n                all_epoch_inds = np.hstack((all_epoch_inds, epoch_indices))\\n                print(all_epoch_inds.shape)\\n\\n            # Random permutation of the indices\\n            random_order = np.random.permutation(all_epoch_inds.shape[1])\\n            print(len(random_order))\\n            all_epoch_inds = all_epoch_inds[:, random_order].astype(np.int64)\\n\\n            # Update epoch inds\\n            print(self.dataset.epoch_inds.shape)\\n            print(all_epoch_inds[:, :num_centers].shape)\\n            print(num_centers)\\n            self.dataset.epoch_inds += torch.from_numpy(all_epoch_inds[:, :num_centers])\\n\\n        # Generator loop\\n        for i in range(self.N):\\n            yield i\\n\\n    def __len__(self):\\n        \"\"\"\\n        The number of yielded samples is variable\\n        \"\"\"\\n        return self.N\\n\\n    def fast_calib(self):\\n        \"\"\"\\n        This method calibrates the batch sizes while ensuring the potentials are well initialized. Indeed on a dataset\\n        like Semantic3D, before potential have been updated over the dataset, there are cahnces that all the dense area\\n        are picked in the begining and in the end, we will have very large batch of small point clouds\\n        :return:\\n        \"\"\"\\n\\n        # Estimated average batch size and target value\\n        estim_b = 0\\n        target_b = self.dataset.config.batch_num\\n\\n        # Calibration parameters\\n        low_pass_T = 10\\n        Kp = 100.0\\n        finer = False\\n        breaking = False\\n\\n        # Convergence parameters\\n        smooth_errors = []\\n        converge_threshold = 0.1\\n\\n        t = [time.time()]\\n        last_display = time.time()\\n        mean_dt = np.zeros(2)\\n\\n        for epoch in range(10):\\n            for i, test in enumerate(self):\\n\\n                # New time\\n                t = t[-1:]\\n                t += [time.time()]\\n\\n                # batch length\\n                b = len(test)\\n\\n                # Update estim_b (low pass filter)\\n                estim_b += (b - estim_b) / low_pass_T\\n\\n                # Estimate error (noisy)\\n                error = target_b - b\\n\\n                # Save smooth errors for convergene check\\n                smooth_errors.append(target_b - estim_b)\\n                if len(smooth_errors) > 10:\\n                    smooth_errors = smooth_errors[1:]\\n\\n                # Update batch limit with P controller\\n                self.dataset.batch_limit += Kp * error\\n\\n                # finer low pass filter when closing in\\n                if not finer and np.abs(estim_b - target_b) < 1:\\n                    low_pass_T = 100\\n                    finer = True\\n\\n                # Convergence\\n                if finer and np.max(np.abs(smooth_errors)) < converge_threshold:\\n                    breaking = True\\n                    break\\n\\n                # Average timing\\n                t += [time.time()]\\n                mean_dt = 0.9 * mean_dt + 0.1 * (np.array(t[1:]) - np.array(t[:-1]))\\n\\n                # Console display (only one per second)\\n                if (t[-1] - last_display) > 1.0:\\n                    last_display = t[-1]\\n                    message = \\'Step {:5d}  estim_b ={:5.2f} batch_limit ={:7d},  //  {:.1f}ms {:.1f}ms\\'\\n                    print(message.format(i,\\n                                         estim_b,\\n                                         int(self.dataset.batch_limit),\\n                                         1000 * mean_dt[0],\\n                                         1000 * mean_dt[1]))\\n\\n            if breaking:\\n                break\\n\\n    def calibration(self, dataloader, untouched_ratio=0.9, verbose=False, force_redo=False):\\n        \"\"\"\\n        Method performing batch and neighbors calibration.\\n            Batch calibration: Set \"batch_limit\" (the maximum number of points allowed in every batch) so that the\\n                               average batch size (number of stacked pointclouds) is the one asked.\\n        Neighbors calibration: Set the \"neighborhood_limits\" (the maximum number of neighbors allowed in convolutions)\\n                               so that 90% of the neighborhoods remain untouched. There is a limit for each layer.\\n        \"\"\"\\n\\n        ##############################\\n        # Previously saved calibration\\n        ##############################\\n\\n        print(\\'\\\\nStarting Calibration (use verbose=True for more details)\\')\\n        t0 = time.time()\\n\\n        redo = force_redo\\n\\n        # Batch limit\\n        # ***********\\n\\n        # Load batch_limit dictionary\\n        batch_lim_file = join(self.dataset.path, \\'batch_limits.pkl\\')\\n        if exists(batch_lim_file):\\n            with open(batch_lim_file, \\'rb\\') as file:\\n                batch_lim_dict = pickle.load(file)\\n        else:\\n            batch_lim_dict = {}\\n\\n        # Check if the batch limit associated with current parameters exists\\n        if self.dataset.use_potentials:\\n            sampler_method = \\'potentials\\'\\n        else:\\n            sampler_method = \\'random\\'\\n        key = \\'{:s}_{:.3f}_{:.3f}_{:d}\\'.format(sampler_method,\\n                                               self.dataset.config.in_radius,\\n                                               self.dataset.config.first_subsampling_dl,\\n                                               self.dataset.config.batch_num)\\n        if not redo and key in batch_lim_dict:\\n            self.dataset.batch_limit[0] = batch_lim_dict[key]\\n        else:\\n            redo = True\\n\\n        if verbose:\\n            print(\\'\\\\nPrevious calibration found:\\')\\n            print(\\'Check batch limit dictionary\\')\\n            if key in batch_lim_dict:\\n                color = bcolors.OKGREEN\\n                v = str(int(batch_lim_dict[key]))\\n            else:\\n                color = bcolors.FAIL\\n                v = \\'?\\'\\n            print(\\'{:}\\\\\"{:s}\\\\\": {:s}{:}\\'.format(color, key, v, bcolors.ENDC))\\n\\n        # Neighbors limit\\n        # ***************\\n\\n        # Load neighb_limits dictionary\\n        neighb_lim_file = join(self.dataset.path, \\'neighbors_limits.pkl\\')\\n        if exists(neighb_lim_file):\\n            with open(neighb_lim_file, \\'rb\\') as file:\\n                neighb_lim_dict = pickle.load(file)\\n        else:\\n            neighb_lim_dict = {}\\n\\n        # Check if the limit associated with current parameters exists (for each layer)\\n        neighb_limits = []\\n        for layer_ind in range(self.dataset.config.num_layers):\\n\\n            dl = self.dataset.config.first_subsampling_dl * (2**layer_ind)\\n            if self.dataset.config.deform_layers[layer_ind]:\\n                r = dl * self.dataset.config.deform_radius\\n            else:\\n                r = dl * self.dataset.config.conv_radius\\n\\n            key = \\'{:.3f}_{:.3f}\\'.format(dl, r)\\n            if key in neighb_lim_dict:\\n                neighb_limits += [neighb_lim_dict[key]]\\n\\n        if not redo and len(neighb_limits) == self.dataset.config.num_layers:\\n            self.dataset.neighborhood_limits = neighb_limits\\n        else:\\n            redo = True\\n\\n        if verbose:\\n            print(\\'Check neighbors limit dictionary\\')\\n            for layer_ind in range(self.dataset.config.num_layers):\\n                dl = self.dataset.config.first_subsampling_dl * (2**layer_ind)\\n                if self.dataset.config.deform_layers[layer_ind]:\\n                    r = dl * self.dataset.config.deform_radius\\n                else:\\n                    r = dl * self.dataset.config.conv_radius\\n                key = \\'{:.3f}_{:.3f}\\'.format(dl, r)\\n\\n                if key in neighb_lim_dict:\\n                    color = bcolors.OKGREEN\\n                    v = str(neighb_lim_dict[key])\\n                else:\\n                    color = bcolors.FAIL\\n                    v = \\'?\\'\\n                print(\\'{:}\\\\\"{:s}\\\\\": {:s}{:}\\'.format(color, key, v, bcolors.ENDC))\\n\\n        if redo:\\n\\n            ############################\\n            # Neighbors calib parameters\\n            ############################\\n\\n            # From config parameter, compute higher bound of neighbors number in a neighborhood\\n            hist_n = int(np.ceil(4 / 3 * np.pi * (self.dataset.config.deform_radius + 1) ** 3))\\n\\n            # Histogram of neighborhood sizes\\n            neighb_hists = np.zeros((self.dataset.config.num_layers, hist_n), dtype=np.int32)\\n\\n            ########################\\n            # Batch calib parameters\\n            ########################\\n\\n            # Estimated average batch size and target value\\n            estim_b = 0\\n            target_b = self.dataset.config.batch_num\\n\\n            # Calibration parameters\\n            low_pass_T = 10\\n            Kp = 100.0\\n            finer = False\\n\\n            # Convergence parameters\\n            smooth_errors = []\\n            converge_threshold = 0.1\\n\\n            # Loop parameters\\n            last_display = time.time()\\n            i = 0\\n            breaking = False\\n\\n            #####################\\n            # Perform calibration\\n            #####################\\n\\n            for epoch in range(10):\\n                for batch_i, batch in enumerate(dataloader):\\n\\n                    # Update neighborhood histogram\\n                    counts = [np.sum(neighb_mat.numpy() < neighb_mat.shape[0], axis=1) for neighb_mat in batch.neighbors]\\n                    hists = [np.bincount(c, minlength=hist_n)[:hist_n] for c in counts]\\n                    neighb_hists += np.vstack(hists)\\n\\n                    # batch length\\n                    b = len(batch.cloud_inds)\\n\\n                    # Update estim_b (low pass filter)\\n                    estim_b += (b - estim_b) / low_pass_T\\n\\n                    # Estimate error (noisy)\\n                    error = target_b - b\\n\\n                    # Save smooth errors for convergene check\\n                    smooth_errors.append(target_b - estim_b)\\n                    if len(smooth_errors) > 10:\\n                        smooth_errors = smooth_errors[1:]\\n\\n                    # Update batch limit with P controller\\n                    self.dataset.batch_limit += Kp * error\\n\\n                    # finer low pass filter when closing in\\n                    if not finer and np.abs(estim_b - target_b) < 1:\\n                        low_pass_T = 100\\n                        finer = True\\n\\n                    # Convergence\\n                    if finer and np.max(np.abs(smooth_errors)) < converge_threshold:\\n                        breaking = True\\n                        break\\n\\n                    i += 1\\n                    t = time.time()\\n\\n                    # Console display (only one per second)\\n                    if verbose and (t - last_display) > 1.0:\\n                        last_display = t\\n                        message = \\'Step {:5d}  estim_b ={:5.2f} batch_limit ={:7d}\\'\\n                        print(message.format(i,\\n                                             estim_b,\\n                                             int(self.dataset.batch_limit)))\\n\\n                if breaking:\\n                    break\\n\\n            # Use collected neighbor histogram to get neighbors limit\\n            cumsum = np.cumsum(neighb_hists.T, axis=0)\\n            percentiles = np.sum(cumsum < (untouched_ratio * cumsum[hist_n - 1, :]), axis=0)\\n            self.dataset.neighborhood_limits = percentiles\\n\\n            if verbose:\\n\\n                # Crop histogram\\n                while np.sum(neighb_hists[:, -1]) == 0:\\n                    neighb_hists = neighb_hists[:, :-1]\\n                hist_n = neighb_hists.shape[1]\\n\\n                print(\\'\\\\n**************************************************\\\\n\\')\\n                line0 = \\'neighbors_num \\'\\n                for layer in range(neighb_hists.shape[0]):\\n                    line0 += \\'|  layer {:2d}  \\'.format(layer)\\n                print(line0)\\n                for neighb_size in range(hist_n):\\n                    line0 = \\'     {:4d}     \\'.format(neighb_size)\\n                    for layer in range(neighb_hists.shape[0]):\\n                        if neighb_size > percentiles[layer]:\\n                            color = bcolors.FAIL\\n                        else:\\n                            color = bcolors.OKGREEN\\n                        line0 += \\'|{:}{:10d}{:}  \\'.format(color,\\n                                                         neighb_hists[layer, neighb_size],\\n                                                         bcolors.ENDC)\\n\\n                    print(line0)\\n\\n                print(\\'\\\\n**************************************************\\\\n\\')\\n                print(\\'\\\\nchosen neighbors limits: \\', percentiles)\\n                print()\\n\\n            # Save batch_limit dictionary\\n            if self.dataset.use_potentials:\\n                sampler_method = \\'potentials\\'\\n            else:\\n                sampler_method = \\'random\\'\\n            key = \\'{:s}_{:.3f}_{:.3f}_{:d}\\'.format(sampler_method,\\n                                                   self.dataset.config.in_radius,\\n                                                   self.dataset.config.first_subsampling_dl,\\n                                                   self.dataset.config.batch_num)\\n            batch_lim_dict[key] = float(self.dataset.batch_limit)\\n            with open(batch_lim_file, \\'wb\\') as file:\\n                pickle.dump(batch_lim_dict, file)\\n\\n            # Save neighb_limit dictionary\\n            for layer_ind in range(self.dataset.config.num_layers):\\n                dl = self.dataset.config.first_subsampling_dl * (2 ** layer_ind)\\n                if self.dataset.config.deform_layers[layer_ind]:\\n                    r = dl * self.dataset.config.deform_radius\\n                else:\\n                    r = dl * self.dataset.config.conv_radius\\n                key = \\'{:.3f}_{:.3f}\\'.format(dl, r)\\n                neighb_lim_dict[key] = self.dataset.neighborhood_limits[layer_ind]\\n            with open(neighb_lim_file, \\'wb\\') as file:\\n                pickle.dump(neighb_lim_dict, file)\\n\\n\\n        print(\\'Calibration done in {:.1f}s\\\\n\\'.format(time.time() - t0))\\n        return\\n\\n\\nclass LASCustomBatch:\\n    \"\"\"Custom batch definition with memory pinning for LAS\"\"\"\\n\\n    def __init__(self, input_list):\\n\\n        # Get rid of batch dimension\\n        input_list = input_list[0]\\n\\n        # Number of layers\\n        L = (len(input_list) - 7) // 5\\n\\n        # Extract input tensors from the list of numpy array\\n        ind = 0\\n        self.points = [torch.from_numpy(nparray) for nparray in input_list[ind:ind+L]]\\n        ind += L\\n        self.neighbors = [torch.from_numpy(nparray) for nparray in input_list[ind:ind+L]]\\n        ind += L\\n        self.pools = [torch.from_numpy(nparray) for nparray in input_list[ind:ind+L]]\\n        ind += L\\n        self.upsamples = [torch.from_numpy(nparray) for nparray in input_list[ind:ind+L]]\\n        ind += L\\n        self.lengths = [torch.from_numpy(nparray) for nparray in input_list[ind:ind+L]]\\n        ind += L\\n        self.features = torch.from_numpy(input_list[ind])\\n        ind += 1\\n        self.labels = torch.from_numpy(input_list[ind])\\n        ind += 1\\n        self.scales = torch.from_numpy(input_list[ind])\\n        ind += 1\\n        self.rots = torch.from_numpy(input_list[ind])\\n        ind += 1\\n        self.cloud_inds = torch.from_numpy(input_list[ind])\\n        ind += 1\\n        self.center_inds = torch.from_numpy(input_list[ind])\\n        ind += 1\\n        self.input_inds = torch.from_numpy(input_list[ind])\\n\\n        return\\n\\n    def pin_memory(self):\\n        \"\"\"\\n        Manual pinning of the memory\\n        \"\"\"\\n\\n        self.points = [in_tensor.pin_memory() for in_tensor in self.points]\\n        self.neighbors = [in_tensor.pin_memory() for in_tensor in self.neighbors]\\n        self.pools = [in_tensor.pin_memory() for in_tensor in self.pools]\\n        self.upsamples = [in_tensor.pin_memory() for in_tensor in self.upsamples]\\n        self.lengths = [in_tensor.pin_memory() for in_tensor in self.lengths]\\n        self.features = self.features.pin_memory()\\n        self.labels = self.labels.pin_memory()\\n        self.scales = self.scales.pin_memory()\\n        self.rots = self.rots.pin_memory()\\n        self.cloud_inds = self.cloud_inds.pin_memory()\\n        self.center_inds = self.center_inds.pin_memory()\\n        self.input_inds = self.input_inds.pin_memory()\\n\\n        return self\\n\\n    def to(self, device):\\n\\n        self.points = [in_tensor.to(device) for in_tensor in self.points]\\n        self.neighbors = [in_tensor.to(device) for in_tensor in self.neighbors]\\n        self.pools = [in_tensor.to(device) for in_tensor in self.pools]\\n        self.upsamples = [in_tensor.to(device) for in_tensor in self.upsamples]\\n        self.lengths = [in_tensor.to(device) for in_tensor in self.lengths]\\n        self.features = self.features.to(device)\\n        self.labels = self.labels.to(device)\\n        self.scales = self.scales.to(device)\\n        self.rots = self.rots.to(device)\\n        self.cloud_inds = self.cloud_inds.to(device)\\n        self.center_inds = self.center_inds.to(device)\\n        self.input_inds = self.input_inds.to(device)\\n\\n        return self\\n\\n    def unstack_points(self, layer=None):\\n        \"\"\"Unstack the points\"\"\"\\n        return self.unstack_elements(\\'points\\', layer)\\n\\n    def unstack_neighbors(self, layer=None):\\n        \"\"\"Unstack the neighbors indices\"\"\"\\n        return self.unstack_elements(\\'neighbors\\', layer)\\n\\n    def unstack_pools(self, layer=None):\\n        \"\"\"Unstack the pooling indices\"\"\"\\n        return self.unstack_elements(\\'pools\\', layer)\\n\\n    def unstack_elements(self, element_name, layer=None, to_numpy=True):\\n        \"\"\"\\n        Return a list of the stacked elements in the batch at a certain layer. If no layer is given, then return all\\n        layers\\n        \"\"\"\\n\\n        if element_name == \\'points\\':\\n            elements = self.points\\n        elif element_name == \\'neighbors\\':\\n            elements = self.neighbors\\n        elif element_name == \\'pools\\':\\n            elements = self.pools[:-1]\\n        else:\\n            raise ValueError(\\'Unknown element name: {:s}\\'.format(element_name))\\n\\n        all_p_list = []\\n        for layer_i, layer_elems in enumerate(elements):\\n\\n            if layer is None or layer == layer_i:\\n\\n                i0 = 0\\n                p_list = []\\n                if element_name == \\'pools\\':\\n                    lengths = self.lengths[layer_i+1]\\n                else:\\n                    lengths = self.lengths[layer_i]\\n\\n                for b_i, length in enumerate(lengths):\\n\\n                    elem = layer_elems[i0:i0 + length]\\n                    if element_name == \\'neighbors\\':\\n                        elem[elem >= self.points[layer_i].shape[0]] = -1\\n                        elem[elem >= 0] -= i0\\n                    elif element_name == \\'pools\\':\\n                        elem[elem >= self.points[layer_i].shape[0]] = -1\\n                        elem[elem >= 0] -= torch.sum(self.lengths[layer_i][:b_i])\\n                    i0 += length\\n\\n                    if to_numpy:\\n                        p_list.append(elem.numpy())\\n                    else:\\n                        p_list.append(elem)\\n\\n                if layer == layer_i:\\n                    return p_list\\n\\n                all_p_list.append(p_list)\\n\\n        return all_p_list\\n\\n\\ndef LASCollate(batch_data):\\n    return LASCustomBatch(batch_data)\\n\\n\\n# ----------------------------------------------------------------------------------------------------------------------\\n#\\n#           Debug functions\\n#       \\\\*********************/\\n\\n\\ndef debug_upsampling(dataset, loader):\\n    \"\"\"Shows which labels are sampled according to strategy chosen\"\"\"\\n\\n\\n    for epoch in range(10):\\n\\n        for batch_i, batch in enumerate(loader):\\n\\n            pc1 = batch.points[1].numpy()\\n            pc2 = batch.points[2].numpy()\\n            up1 = batch.upsamples[1].numpy()\\n\\n            print(pc1.shape, \\'=>\\', pc2.shape)\\n            print(up1.shape, np.max(up1))\\n\\n            pc2 = np.vstack((pc2, np.zeros_like(pc2[:1, :])))\\n\\n            # Get neighbors distance\\n            p0 = pc1[10, :]\\n            neighbs0 = up1[10, :]\\n            neighbs0 = pc2[neighbs0, :] - p0\\n            d2 = np.sum(neighbs0 ** 2, axis=1)\\n\\n            print(neighbs0.shape)\\n            print(neighbs0[:5])\\n            print(d2[:5])\\n\\n            print(\\'******************\\')\\n        print(\\'*******************************************\\')\\n\\n    _, counts = np.unique(dataset.input_labels, return_counts=True)\\n    print(counts)\\n\\n\\ndef debug_timing(dataset, loader):\\n    \"\"\"Timing of generator function\"\"\"\\n\\n    t = [time.time()]\\n    last_display = time.time()\\n    mean_dt = np.zeros(2)\\n    estim_b = dataset.config.batch_num\\n    estim_N = 0\\n\\n    for epoch in range(10):\\n\\n        for batch_i, batch in enumerate(loader):\\n            # print(batch_i, tuple(points.shape),  tuple(normals.shape), labels, indices, in_sizes)\\n\\n            # New time\\n            t = t[-1:]\\n            t += [time.time()]\\n\\n            # Update estim_b (low pass filter)\\n            estim_b += (len(batch.cloud_inds) - estim_b) / 100\\n            estim_N += (batch.features.shape[0] - estim_N) / 10\\n\\n            # Pause simulating computations\\n            time.sleep(0.05)\\n            t += [time.time()]\\n\\n            # Average timing\\n            mean_dt = 0.9 * mean_dt + 0.1 * (np.array(t[1:]) - np.array(t[:-1]))\\n\\n            # Console display (only one per second)\\n            if (t[-1] - last_display) > -1.0:\\n                last_display = t[-1]\\n                message = \\'Step {:08d} -> (ms/batch) {:8.2f} {:8.2f} / batch = {:.2f} - {:.0f}\\'\\n                print(message.format(batch_i,\\n                                     1000 * mean_dt[0],\\n                                     1000 * mean_dt[1],\\n                                     estim_b,\\n                                     estim_N))\\n\\n        print(\\'************* Epoch ended *************\\')\\n\\n    _, counts = np.unique(dataset.input_labels, return_counts=True)\\n    print(counts)\\n\\n\\ndef debug_show_clouds(dataset, loader):\\n\\n\\n    for epoch in range(10):\\n\\n        clouds = []\\n        cloud_normals = []\\n        cloud_labels = []\\n\\n        L = dataset.config.num_layers\\n\\n        for batch_i, batch in enumerate(loader):\\n\\n            # Print characteristics of input tensors\\n            print(\\'\\\\nPoints tensors\\')\\n            for i in range(L):\\n                print(batch.points[i].dtype, batch.points[i].shape)\\n            print(\\'\\\\nNeigbors tensors\\')\\n            for i in range(L):\\n                print(batch.neighbors[i].dtype, batch.neighbors[i].shape)\\n            print(\\'\\\\nPools tensors\\')\\n            for i in range(L):\\n                print(batch.pools[i].dtype, batch.pools[i].shape)\\n            print(\\'\\\\nStack lengths\\')\\n            for i in range(L):\\n                print(batch.lengths[i].dtype, batch.lengths[i].shape)\\n            print(\\'\\\\nFeatures\\')\\n            print(batch.features.dtype, batch.features.shape)\\n            print(\\'\\\\nLabels\\')\\n            print(batch.labels.dtype, batch.labels.shape)\\n            print(\\'\\\\nAugment Scales\\')\\n            print(batch.scales.dtype, batch.scales.shape)\\n            print(\\'\\\\nAugment Rotations\\')\\n            print(batch.rots.dtype, batch.rots.shape)\\n            print(\\'\\\\nModel indices\\')\\n            print(batch.model_inds.dtype, batch.model_inds.shape)\\n\\n            print(\\'\\\\nAre input tensors pinned\\')\\n            print(batch.neighbors[0].is_pinned())\\n            print(batch.neighbors[-1].is_pinned())\\n            print(batch.points[0].is_pinned())\\n            print(batch.points[-1].is_pinned())\\n            print(batch.labels.is_pinned())\\n            print(batch.scales.is_pinned())\\n            print(batch.rots.is_pinned())\\n            print(batch.model_inds.is_pinned())\\n\\n            show_input_batch(batch)\\n\\n        print(\\'*******************************************\\')\\n\\n    _, counts = np.unique(dataset.input_labels, return_counts=True)\\n    print(counts)\\n\\n\\ndef debug_batch_and_neighbors_calib(dataset, loader):\\n    \"\"\"Timing of generator function\"\"\"\\n\\n    t = [time.time()]\\n    last_display = time.time()\\n    mean_dt = np.zeros(2)\\n\\n    for epoch in range(10):\\n\\n        for batch_i, input_list in enumerate(loader):\\n            # print(batch_i, tuple(points.shape),  tuple(normals.shape), labels, indices, in_sizes)\\n\\n            # New time\\n            t = t[-1:]\\n            t += [time.time()]\\n\\n            # Pause simulating computations\\n            time.sleep(0.01)\\n            t += [time.time()]\\n\\n            # Average timing\\n            mean_dt = 0.9 * mean_dt + 0.1 * (np.array(t[1:]) - np.array(t[:-1]))\\n\\n            # Console display (only one per second)\\n            if (t[-1] - last_display) > 1.0:\\n                last_display = t[-1]\\n                message = \\'Step {:08d} -> Average timings (ms/batch) {:8.2f} {:8.2f} \\'\\n                print(message.format(batch_i,\\n                                     1000 * mean_dt[0],\\n                                     1000 * mean_dt[1]))\\n\\n        print(\\'************* Epoch ended *************\\')\\n\\n    _, counts = np.unique(dataset.input_labels, return_counts=True)\\n    print(counts)\\n\\n\\n***'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Utility Functions\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-4-1106-preview\")\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "def get_completion(prompt: str, model: str = \"gpt-4-1106-preview\") -> str:\n",
        "    \"\"\"\n",
        "    Query your LLM model with your prompt.\n",
        "    Parameters:\n",
        "    prompt (str): The text prompt you want the LLM to respond to.\n",
        "    model (str, optional): The model to be used for generating the response. Default is \"gpt-3.5-turbo\".\n",
        "    Returns:\n",
        "    str: The generated text completion from the specified model.\n",
        "    \"\"\"\n",
        "    openai.api_key = \"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\"\n",
        "    num_tokens = num_tokens_from_string(prompt, \"gpt-4-1106-preview\")\n",
        "    if num_tokens > 128000:\n",
        "      print(str(num_tokens)+\" :Too Long For GPT 4\")\n",
        "      return 0\n",
        "\n",
        "    else:\n",
        "      print(\"Number of tokens: \"+str(num_tokens))\n",
        "      input_cost = (int(num_tokens)/1000)*0.01\n",
        "      print(\"Cost Incurred Input: \"+str(input_cost))\n",
        "      messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "      start = time.time()\n",
        "      response = openai.ChatCompletion.create(\n",
        "          model= model,\n",
        "          messages=messages,\n",
        "          temperature=0.5\n",
        "      )\n",
        "      output_cost = (int(num_tokens_from_string(response.choices[0].message[\"content\"], \"gpt-4-1106-preview\"))/1000)*0.03\n",
        "      print(\"Cost Incurred Output: \"+str((int(num_tokens_from_string(response.choices[0].message[\"content\"], \"gpt-4\"))/1000)*0.06))\n",
        "      return [response.choices[0].message[\"content\"], input_cost+output_cost, time.time()-start]\n",
        "\n",
        "def github_repo_inferencer(task='skill_extraction', input_to_llm='', readme=''):\n",
        "\n",
        "  class repo_details(BaseModel):\n",
        "      topics: list = Field(description=\"In Depth Documentation of Github Repo\")\n",
        "\n",
        "  class repo(BaseModel):\n",
        "      repo: List[repo_details]\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  You are an AI assistant tasked at analysis github repo contents and generating a well defined description of the repo with different features and functionalities explained.\n",
        "  The different folders are separated using '***' and the folder contents are separated using '\\n\\n'.\n",
        "\n",
        "  STEPS TO EVALUATE:\n",
        "  (1) Access each folder\n",
        "  (2) Access each file in the folder\n",
        "  (3) Evaluate contents of the file and generate brief description\n",
        "  (4) Generate a well defined description of the repo with different features and functionalities of all folders explained.\n",
        "\n",
        "  Github Repo Contents:\n",
        "  {input_to_llm}\n",
        "  \"\"\"\n",
        "  pydantic_object=repo\n",
        "  pydantic_parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
        "  format_instructions = pydantic_parser.get_format_instructions()\n",
        "  query = prompt\n",
        "  prompt = PromptTemplate(\n",
        "      template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "      input_variables=[\"query\"],\n",
        "      partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
        "  )\n",
        "  _input = prompt.format_prompt(query=query)\n",
        "  answer = get_completion(_input.to_string())\n",
        "  return answer\n",
        "\n",
        "github_repo_inferencer(input_to_llm=repo_contents_str, readme=repo_contents[repo_contents['file_name']=='README.md']['content'].iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpNJcUAOPlEG",
        "outputId": "42e8703d-dbe5-4585-9ff3-60a06679c09a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 92737\n",
            "Cost Incurred Input: 0.9273699999999999\n",
            "Cost Incurred Output: 0.01926\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['```json\\n{\\n  \"repo\": [\\n    {\\n      \"topics\": [\\n        \"The LAS dataset is a point cloud dataset designed for segmentation tasks, where each point in the cloud is associated with a label indicating the type of object or surface it belongs to.\",\\n        \"This dataset is managed by the LASDataset class, which handles loading, subsampling, and preparing the data for training or evaluation.\",\\n        \"The dataset is divided into training, validation, and test sets, with the option to use potentials for batch generation during training.\",\\n        \"Custom batch definition with memory pinning is implemented through the LASCustomBatch class, which optimizes data loading for GPU processing.\",\\n        \"The dataset includes a variety of classes, such as ground, vegetation, buildings, and water, with some classes being ignored during training for specific tasks.\",\\n        \"Data augmentation techniques, such as random rotations, scaling, and noise addition, are employed to improve the robustness of the model being trained.\",\\n        \"The LASSampler class is used to create batches of data by sampling points based on their \\'potential\\', which is a measure of how densely a region is sampled.\",\\n        \"For efficient nearest neighbor queries, a KDTree structure is used, which is precomputed for each subsampled version of the dataset.\",\\n        \"The LAS dataset supports on-the-fly data augmentation and can handle large point clouds by processing them in smaller, manageable batches.\",\\n        \"Performance metrics are evaluated on the original, unprocessed point clouds, ensuring that the model\\'s performance is assessed on realistic data.\"\\n      ]\\n    }\\n  ]\\n}\\n```',\n",
              " 0.9369999999999999,\n",
              " 34.0125298500061]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1 ##Utility Functions\n",
        "\n",
        "def github_repo_steps_to_execute(task='skill_extraction', input_to_llm=''):\n",
        "\n",
        "  class steps_list(BaseModel):\n",
        "      steps_list: list = Field(description=\"Steps to execute github repo contents\")\n",
        "\n",
        "  class steps(BaseModel):\n",
        "      steps: List[steps_list]\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  You are an AI assistant tasked at analysing github repo contents and generating a step by step manual to execute the repo contents.\n",
        "  The different folders are separated using '***' and the folder contents are separated using '\\n\\n'.\n",
        "\n",
        "  Github Repo Contents:\n",
        "  {input_to_llm}\n",
        "  \"\"\"\n",
        "  pydantic_object=steps\n",
        "  pydantic_parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
        "  format_instructions = pydantic_parser.get_format_instructions()\n",
        "  query = prompt\n",
        "  prompt = PromptTemplate(\n",
        "      template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "      input_variables=[\"query\"],\n",
        "      partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
        "  )\n",
        "  _input = prompt.format_prompt(query=query)\n",
        "  answer = get_completion(_input.to_string(),\"gpt-4-1106-preview\")\n",
        "  return answer\n",
        "\n",
        "github_repo_steps_to_execute(input_to_llm=repo_contents_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGt2GfE6S8kg",
        "outputId": "f6cc2e15-baa3-463a-dbe2-3037bccd14fb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 92675\n",
            "Cost Incurred Input: 0.92675\n",
            "Cost Incurred Output: 0.026099999999999998\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['```json\\n{\\n  \"properties\": {\\n    \"steps\": {\\n      \"title\": \"Steps\",\\n      \"type\": \"array\",\\n      \"items\": {\\n        \"$ref\": \"#/definitions/steps_list\"\\n      }\\n    }\\n  },\\n  \"required\": [\"steps\"],\\n  \"definitions\": {\\n    \"steps_list\": {\\n      \"title\": \"steps_list\",\\n      \"type\": \"object\",\\n      \"properties\": {\\n        \"steps_list\": {\\n          \"title\": \"Steps List\",\\n          \"description\": \"Steps to execute github repo contents\",\\n          \"type\": \"array\",\\n          \"items\": {}\\n        }\\n      },\\n      \"required\": [\"steps_list\"]\\n    }\\n  }\\n}\\n```\\n\\nThe output should be a JSON instance that conforms to the JSON schema above. The example provided in the user query is for a scenario where an AI assistant is tasked with analyzing github repo contents and generating a step-by-step manual to execute the repo contents.\\n\\nGiven the complexity of the task and the need to analyze the code to generate meaningful steps, it is not feasible to provide a detailed and accurate step-by-step manual solely based on the provided file contents without additional context or execution logic. However, we can provide a high-level example of what the steps might include for a generic Python project setup.\\n\\nHere is an example JSON instance conforming to the schema:\\n\\n```json\\n{\\n  \"steps\": {\\n    \"steps_list\": [\\n      \"Clone the repository to your local machine.\",\\n      \"Navigate to the repository directory.\",\\n      \"Create a virtual environment for the project.\",\\n      \"Activate the virtual environment.\",\\n      \"Install the required dependencies listed in the \\'requirements.txt\\' or \\'environment.yml\\' file.\",\\n      \"Compile any necessary extensions or modules using provided scripts such as \\'compile_wrappers.sh\\'.\",\\n      \"Execute the main script or application entry point, such as \\'train_LAS.py\\' or \\'test_LAS.py\\'.\",\\n      \"Follow any additional instructions provided in the \\'README.md\\' for specific usage or configurations.\"\\n    ]\\n  }\\n}\\n```\\n\\nPlease note that the actual steps may vary based on the specific project and its requirements.',\n",
              " 0.9398,\n",
              " 43.22076439857483]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports needed to run the code in this notebook\n",
        "import ast  # used for detecting whether generated Python code is valid\n",
        "import openai  # used for calling the OpenAI API\n",
        "\n",
        "color_prefix_by_role = {\n",
        "    \"system\": \"\\033[0m\",  # gray\n",
        "    \"user\": \"\\033[0m\",  # gray\n",
        "    \"assistant\": \"\\033[92m\",  # green\n",
        "}\n",
        "\n",
        "\n",
        "def print_messages(messages, color_prefix_by_role=color_prefix_by_role) -> None:\n",
        "    \"\"\"Prints messages sent to or from GPT.\"\"\"\n",
        "    for message in messages:\n",
        "        role = message[\"role\"]\n",
        "        color_prefix = color_prefix_by_role[role]\n",
        "        content = message[\"content\"]\n",
        "        print(f\"{color_prefix}\\n[{role}]\\n{content}\")\n",
        "\n",
        "\n",
        "def print_message_delta(delta, color_prefix_by_role=color_prefix_by_role) -> None:\n",
        "    \"\"\"Prints a chunk of messages streamed back from GPT.\"\"\"\n",
        "    if \"role\" in delta:\n",
        "        role = delta[\"role\"]\n",
        "        color_prefix = color_prefix_by_role[role]\n",
        "        print(f\"{color_prefix}\\n[{role}]\\n\", end=\"\")\n",
        "    elif \"content\" in delta:\n",
        "        content = delta[\"content\"]\n",
        "        print(content, end=\"\")\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "\n",
        "# example of a function that uses a multi-step prompt to write unit tests\n",
        "def unit_tests_from_function(\n",
        "    function_to_test: str,  # Python function to test, as a string\n",
        "    unit_test_package: str = \"pytest\",  # unit testing package; use the name as it appears in the import statement\n",
        "    approx_min_cases_to_cover: int = 7,  # minimum number of test case categories to cover (approximate)\n",
        "    print_text: bool = False,  # optionally prints text; helpful for understanding the function & debugging\n",
        "    explain_model: str = \"gpt-4\",  # model used to generate text plans in step 1\n",
        "    plan_model: str = \"gpt-4\",  # model used to generate text plans in steps 2 and 2b\n",
        "    execute_model: str = \"gpt-4\",  # model used to generate code in step 3\n",
        "    temperature: float = 0.4,  # temperature = 0 can sometimes get stuck in repetitive loops, so we use 0.4\n",
        "    reruns_if_fail: int = 1,  # if the output code cannot be parsed, this will re-run the function up to N times\n",
        ") -> str:\n",
        "    \"\"\"Returns a unit test for a given Python function, using a 3-step GPT prompt.\"\"\"\n",
        "\n",
        "    # Step 1: Generate an explanation of the function\n",
        "\n",
        "    # create a markdown-formatted message that asks GPT to explain the function, formatted as a bullet list\n",
        "    explain_system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a world-class Python developer with an eagle eye for unintended bugs and edge cases. You carefully explain code with great detail and accuracy. You organize your explanations in markdown-formatted, bulleted lists.\",\n",
        "    }\n",
        "    explain_user_message = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"Please explain the following Python function. Review what each element of the function is doing precisely and what the author's intentions may have been. Organize your explanation as a markdown-formatted, bulleted list.\n",
        "\n",
        "```python\n",
        "{function_to_test}\n",
        "```\"\"\",\n",
        "    }\n",
        "    explain_messages = [explain_system_message, explain_user_message]\n",
        "    if print_text:\n",
        "        print_messages(explain_messages)\n",
        "\n",
        "    explanation_response = openai.ChatCompletion.create(\n",
        "        model=explain_model,\n",
        "        messages=explain_messages,\n",
        "        temperature=temperature,\n",
        "        stream=True,\n",
        "    )\n",
        "    explanation = \"\"\n",
        "    for chunk in explanation_response:\n",
        "        delta = chunk[\"choices\"][0][\"delta\"]\n",
        "        if print_text:\n",
        "            print_message_delta(delta)\n",
        "        if \"content\" in delta:\n",
        "            explanation += delta[\"content\"]\n",
        "    explain_assistant_message = {\"role\": \"assistant\", \"content\": explanation}\n",
        "\n",
        "    # Step 2: Generate a plan to write a unit test\n",
        "\n",
        "    # Asks GPT to plan out cases the units tests should cover, formatted as a bullet list\n",
        "    plan_user_message = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"A good unit test suite should aim to:\n",
        "- Test the function's behavior for a wide range of possible inputs\n",
        "- Test edge cases that the author may not have foreseen\n",
        "- Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain\n",
        "- Be easy to read and understand, with clean code and descriptive names\n",
        "- Be deterministic, so that the tests always pass or fail in the same way\n",
        "\n",
        "To help unit test the function above, list diverse scenarios that the function should be able to handle (and under each scenario, include a few examples as sub-bullets).\"\"\",\n",
        "    }\n",
        "    plan_messages = [\n",
        "        explain_system_message,\n",
        "        explain_user_message,\n",
        "        explain_assistant_message,\n",
        "        plan_user_message,\n",
        "    ]\n",
        "    if print_text:\n",
        "        print_messages([plan_user_message])\n",
        "    plan_response = openai.ChatCompletion.create(\n",
        "        model=plan_model,\n",
        "        messages=plan_messages,\n",
        "        temperature=temperature,\n",
        "        stream=True,\n",
        "    )\n",
        "    plan = \"\"\n",
        "    for chunk in plan_response:\n",
        "        delta = chunk[\"choices\"][0][\"delta\"]\n",
        "        if print_text:\n",
        "            print_message_delta(delta)\n",
        "        if \"content\" in delta:\n",
        "            plan += delta[\"content\"]\n",
        "    plan_assistant_message = {\"role\": \"assistant\", \"content\": plan}\n",
        "\n",
        "    # Step 2b: If the plan is short, ask GPT to elaborate further\n",
        "    # this counts top-level bullets (e.g., categories), but not sub-bullets (e.g., test cases)\n",
        "    num_bullets = max(plan.count(\"\\n-\"), plan.count(\"\\n*\"))\n",
        "    elaboration_needed = num_bullets < approx_min_cases_to_cover\n",
        "    if elaboration_needed:\n",
        "        elaboration_user_message = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"In addition to those scenarios above, list a few rare or unexpected edge cases (and as before, under each edge case, include a few examples as sub-bullets).\"\"\",\n",
        "        }\n",
        "        elaboration_messages = [\n",
        "            explain_system_message,\n",
        "            explain_user_message,\n",
        "            explain_assistant_message,\n",
        "            plan_user_message,\n",
        "            plan_assistant_message,\n",
        "            elaboration_user_message,\n",
        "        ]\n",
        "        if print_text:\n",
        "            print_messages([elaboration_user_message])\n",
        "        elaboration_response = openai.ChatCompletion.create(\n",
        "            model=plan_model,\n",
        "            messages=elaboration_messages,\n",
        "            temperature=temperature,\n",
        "            stream=True,\n",
        "        )\n",
        "        elaboration = \"\"\n",
        "        for chunk in elaboration_response:\n",
        "            delta = chunk[\"choices\"][0][\"delta\"]\n",
        "            if print_text:\n",
        "                print_message_delta(delta)\n",
        "            if \"content\" in delta:\n",
        "                elaboration += delta[\"content\"]\n",
        "        elaboration_assistant_message = {\"role\": \"assistant\", \"content\": elaboration}\n",
        "\n",
        "    # Step 3: Generate the unit test\n",
        "\n",
        "    # create a markdown-formatted prompt that asks GPT to complete a unit test\n",
        "    package_comment = \"\"\n",
        "    if unit_test_package == \"pytest\":\n",
        "        package_comment = \"# below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\"\n",
        "    execute_system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a world-class Python developer with an eagle eye for unintended bugs and edge cases. You write careful, accurate unit tests. When asked to reply only with code, you write all of your code in a single block.\",\n",
        "    }\n",
        "    execute_user_message = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"Using Python and the `{unit_test_package}` package, write a suite of unit tests for the function, following the cases above. Include helpful comments to explain each line. Reply only with code, formatted as follows:\n",
        "\n",
        "```python\n",
        "# imports\n",
        "import {unit_test_package}  # used for our unit tests\n",
        "{{insert other imports as needed}}\n",
        "\n",
        "# function to test\n",
        "{function_to_test}\n",
        "\n",
        "# unit tests\n",
        "{package_comment}\n",
        "{{insert unit test code here}}\n",
        "```\"\"\",\n",
        "    }\n",
        "    execute_messages = [\n",
        "        execute_system_message,\n",
        "        explain_user_message,\n",
        "        explain_assistant_message,\n",
        "        plan_user_message,\n",
        "        plan_assistant_message,\n",
        "    ]\n",
        "    if elaboration_needed:\n",
        "        execute_messages += [elaboration_user_message, elaboration_assistant_message]\n",
        "    execute_messages += [execute_user_message]\n",
        "    if print_text:\n",
        "        print_messages([execute_system_message, execute_user_message])\n",
        "\n",
        "    execute_response = openai.ChatCompletion.create(\n",
        "        model=execute_model,\n",
        "        messages=execute_messages,\n",
        "        temperature=temperature,\n",
        "        stream=True,\n",
        "    )\n",
        "    execution = \"\"\n",
        "    for chunk in execute_response:\n",
        "        delta = chunk[\"choices\"][0][\"delta\"]\n",
        "        if print_text:\n",
        "            print_message_delta(delta)\n",
        "        if \"content\" in delta:\n",
        "            execution += delta[\"content\"]\n",
        "\n",
        "    # check the output for errors\n",
        "    code = execution.split(\"```python\")[1].split(\"```\")[0].strip()\n",
        "    try:\n",
        "        ast.parse(code)\n",
        "    except SyntaxError as e:\n",
        "        print(f\"Syntax error in generated code: {e}\")\n",
        "        if reruns_if_fail > 0:\n",
        "            print(\"Rerunning...\")\n",
        "            return unit_tests_from_function(\n",
        "                function_to_test=function_to_test,\n",
        "                unit_test_package=unit_test_package,\n",
        "                approx_min_cases_to_cover=approx_min_cases_to_cover,\n",
        "                print_text=print_text,\n",
        "                explain_model=explain_model,\n",
        "                plan_model=plan_model,\n",
        "                execute_model=execute_model,\n",
        "                temperature=temperature,\n",
        "                reruns_if_fail=reruns_if_fail\n",
        "                - 1,  # decrement rerun counter when calling again\n",
        "            )\n",
        "\n",
        "    # return the unit test as a string\n",
        "    return code\n",
        "\n",
        "example_function = repo_contents[repo_contents['file_name']=='las.py']['content'].iloc[0]\n",
        "\n",
        "unit_tests = unit_tests_from_function(\n",
        "    example_function,\n",
        "    approx_min_cases_to_cover=10,\n",
        "    print_text=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtMvABZNae-e",
        "outputId": "429c5444-c248-47cd-b01a-c7ba1432f8f2"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\n",
            "[system]\n",
            "You are a world-class Python developer with an eagle eye for unintended bugs and edge cases. You carefully explain code with great detail and accuracy. You organize your explanations in markdown-formatted, bulleted lists.\n",
            "\u001b[0m\n",
            "[user]\n",
            "Please explain the following Python function. Review what each element of the function is doing precisely and what the author's intentions may have been. Organize your explanation as a markdown-formatted, bulleted list.\n",
            "\n",
            "```python\n",
            "import json\n",
            "import numpy as np\n",
            "import pdal\n",
            "\n",
            "def read_las_points(filename):\n",
            "    p = pdal.Pipeline(json.dumps([filename]))\n",
            "    p.validate()\n",
            "    p.execute()\n",
            "    data = p.arrays[0]\n",
            "    points = np.vstack((data['X'], data['Y'], data['Z'])).T\n",
            "    return points\n",
            "\n",
            "def read_processed_las(filename):\n",
            "    p = pdal.Pipeline(json.dumps([filename]))\n",
            "    p.validate()\n",
            "    p.execute()\n",
            "    data = p.arrays[0]\n",
            "    points = np.vstack((data['X'], data['Y'], data['Z'])).T\n",
            "    # intensity = np.expand_dims(data['Intensity'], 1).astype(np.float32)\n",
            "    # intensity = np.minimum(intensity, 255.0)/255.0\n",
            "    # features = intensity\n",
            "    # rn = data['ReturnNumber'].astype(np.float32)\n",
            "    # nr = data['NumberOfReturns'].astype(np.float32)\n",
            "    features = np.vstack((data['Linearity'],data['Planarity'],data['Scattering'],data['Verticality'])).T\n",
            "    # features = np.vstack((data['Eigenvalue0'],data['Eigenvalue1'],data['Eigenvalue2'])).T\n",
            "    # features = np.vstack((data['X'], data['Y'], data['Z'])).T\n",
            "    labels = data['Classification']\n",
            "    return points, features, labels\n",
            "\n",
            "def read_raw_las(filename):\n",
            "    p = pdal.Pipeline(json.dumps([\n",
            "        # filename\n",
            "        filename,\n",
            "        {\n",
            "            \"type\":\"filters.range\",\n",
            "            \"limits\":\"Classification(:17]\"\n",
            "        },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[1:1]=0\"\n",
            "        # },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[2:2]=1\"\n",
            "        # },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[7:7]=2\"\n",
            "        # },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[9:9]=3\"\n",
            "        # },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[17:17]=4\"\n",
            "        # },\n",
            "        {\n",
            "            \"type\":\"filters.covariancefeatures\"\n",
            "        }\n",
            "        # {\n",
            "        #     \"type\":\"filters.eigenvalues\",\n",
            "        #     \"knn\":10\n",
            "        # }\n",
            "    ]))\n",
            "    p.validate()\n",
            "    p.execute()\n",
            "    data = p.arrays[0]\n",
            "    points = np.vstack((data['X'], data['Y'], data['Z'])).T\n",
            "    # intensity = np.expand_dims(data['Intensity'], 1).astype(np.float32)\n",
            "    # intensity = np.minimum(intensity, 255.0)/255.0\n",
            "    # features = intensity\n",
            "    # rn = data['ReturnNumber'].astype(np.float32)\n",
            "    # nr = data['NumberOfReturns'].astype(np.float32)\n",
            "    features = np.vstack((data['Linearity'],data['Planarity'],data['Scattering'],data['Verticality'])).T\n",
            "    # features = np.vstack((data['Eigenvalue0'],data['Eigenvalue1'],data['Eigenvalue2'])).T\n",
            "    # features = np.vstack((data['X'], data['Y'], data['Z'])).T\n",
            "    labels = data['Classification']\n",
            "    return points, features, labels\n",
            "\n",
            "def read_subsampled_las(filename, dl):\n",
            "    p = pdal.Pipeline(json.dumps([\n",
            "        # filename\n",
            "        filename,\n",
            "        {\n",
            "            \"type\":\"filters.range\",\n",
            "            \"limits\":\"Classification(:17]\"\n",
            "        },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[1:1]=0\"\n",
            "        # },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[2:2]=1\"\n",
            "        # },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[7:7]=2\"\n",
            "        # },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[9:9]=3\"\n",
            "        # },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[17:17]=4\"\n",
            "        # },\n",
            "        {\n",
            "            \"type\":\"filters.covariancefeatures\"\n",
            "        },\n",
            "        {\n",
            "            \"type\":\"filters.sample\",\n",
            "            \"radius\":dl\n",
            "        }\n",
            "        # {\n",
            "        #     \"type\":\"filters.eigenvalues\",\n",
            "        #     \"knn\":10\n",
            "        # }\n",
            "    ]))\n",
            "    p.validate()\n",
            "    p.execute()\n",
            "    data = p.arrays[0]\n",
            "    points = np.vstack((data['X'], data['Y'], data['Z'])).T\n",
            "    # intensity = np.expand_dims(data['Intensity'], 1).astype(np.float32)\n",
            "    # intensity = np.minimum(intensity, 255.0)/255.0\n",
            "    # features = intensity\n",
            "    # rn = data['ReturnNumber'].astype(np.float32)\n",
            "    # nr = data['NumberOfReturns'].astype(np.float32)\n",
            "    features = np.vstack((data['Linearity'],data['Planarity'],data['Scattering'],data['Verticality'])).T\n",
            "    # features = np.vstack((data['Eigenvalue0'],data['Eigenvalue1'],data['Eigenvalue2'])).T\n",
            "    # features = np.vstack((data['X'], data['Y'], data['Z'])).T\n",
            "    labels = data['Classification']\n",
            "    return points, features, labels\n",
            "\n",
            "def write_las(filename, array):\n",
            "    # merge the fields then\n",
            "    p = pdal.Pipeline(json.dumps([{\n",
            "        \"type\":\"writers.las\",\n",
            "        \"filename\":filename,\n",
            "        # \"offset_x\":\"auto\",\n",
            "        # \"offset_y\":\"auto\",\n",
            "        # \"offset_z\":\"auto\",\n",
            "        # \"scale_x\":0.01,\n",
            "        # \"scale_y\":0.01,\n",
            "        # \"scale_z\":0.01\n",
            "        \"forward\":\"all\",\n",
            "        \"minor_version\":4,\n",
            "        \"extra_dims\":\"all\"\n",
            "        }]), [array])\n",
            "    p.validate()\n",
            "    p.execute()\n",
            "    return True\n",
            "```\n",
            "\u001b[92m\n",
            "[assistant]\n",
            "- The code consists of several functions that are used to read and write data from/to LAS files. LAS files are a public file format for the interchange of 3-dimensional point cloud data, often used in lidar data processing.\n",
            "\n",
            "- **Import Statements:**\n",
            "  - `import json`: This imports the json module which provides methods to manipulate JSON objects.\n",
            "  - `import numpy as np`: This imports the numpy module, a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
            "  - `import pdal`: This imports the PDAL library (Point Data Abstraction Library). PDAL is a C++ BSD library for translating and manipulating point cloud data.\n",
            "\n",
            "- **Function `read_las_points(filename)`:**\n",
            "  - This function reads a LAS file and returns the X, Y, Z coordinates of the points in the file.\n",
            "  - `pdal.Pipeline(json.dumps([filename]))`: This creates a PDAL Pipeline object. The pipeline is defined by a JSON string, which in this case only contains the filename. The `json.dumps()` function converts a Python object into a JSON string.\n",
            "  - `p.validate()`: This validates the pipeline.\n",
            "  - `p.execute()`: This executes the pipeline.\n",
            "  - `data = p.arrays[0]`: This retrieves the first array of data from the pipeline. The data is a structured NumPy array.\n",
            "  - `points = np.vstack((data['X'], data['Y'], data['Z'])).T`: This stacks the X, Y, and Z values of the points into a 2D array and then transposes it so that each row represents a point and each column represents a coordinate.\n",
            "  - The function returns the points.\n",
            "\n",
            "- **Function `read_processed_las(filename)`:**\n",
            "  - This function is similar to `read_las_points()`, but it also retrieves additional features from the data such as 'Linearity', 'Planarity', 'Scattering', and 'Verticality'.\n",
            "  - The function returns the points, features, and labels (classification).\n",
            "\n",
            "- **Function `read_raw_las(filename)`:**\n",
            "  - This function reads a raw LAS file and applies a range filter and a covariance features filter to the data.\n",
            "  - The range filter restricts the data to points with a classification value of 17 or less.\n",
            "  - The covariance features filter computes features based on the covariance of the points.\n",
            "  - The function returns the points, features, and labels (classification).\n",
            "\n",
            "- **Function `read_subsampled_las(filename, dl)`:**\n",
            "  - This function is similar to `read_raw_las()`, but it also applies a sample filter to the data.\n",
            "  - The sample filter subsamples the points within a radius specified by the parameter `dl`.\n",
            "  - The function returns the points, features, and labels (classification).\n",
            "\n",
            "- **Function `write_las(filename, array)`:**\n",
            "  - This function writes data to a LAS file.\n",
            "  - The pipeline for this function includes a LAS writer filter, which writes the data to a LAS file.\n",
            "  - The `forward` option is set to \"all\", which means that all dimensions (including extra dimensions) are written to the file.\n",
            "  - The `minor_version` is set to 4, which specifies the minor version of the LAS format to write.\n",
            "  - The function returns True after successfully executing the pipeline.\u001b[0m\n",
            "[user]\n",
            "A good unit test suite should aim to:\n",
            "- Test the function's behavior for a wide range of possible inputs\n",
            "- Test edge cases that the author may not have foreseen\n",
            "- Take advantage of the features of `pytest` to make the tests easy to write and maintain\n",
            "- Be easy to read and understand, with clean code and descriptive names\n",
            "- Be deterministic, so that the tests always pass or fail in the same way\n",
            "\n",
            "To help unit test the function above, list diverse scenarios that the function should be able to handle (and under each scenario, include a few examples as sub-bullets).\n",
            "\u001b[92m\n",
            "[assistant]\n",
            "- **Function `read_las_points(filename)`:**\n",
            "\n",
            "  - Scenario: The LAS file contains valid data.\n",
            "    - Example: The file contains a point cloud of a small area with X, Y, Z coordinates.\n",
            "  - Scenario: The LAS file is empty or does not contain any points.\n",
            "    - Example: The file is empty or only contains header information.\n",
            "  - Scenario: The LAS file does not exist or the filename is incorrect.\n",
            "    - Example: The filename is misspelled or the file is in a different directory.\n",
            "\n",
            "- **Function `read_processed_las(filename)`:**\n",
            "\n",
            "  - Scenario: The LAS file contains valid data with additional features.\n",
            "    - Example: The file contains a point cloud with X, Y, Z coordinates and additional features such as 'Linearity', 'Planarity', 'Scattering', and 'Verticality'.\n",
            "  - Scenario: The LAS file does not contain the additional features.\n",
            "    - Example: The file only contains X, Y, Z coordinates without the additional features.\n",
            "  - Scenario: The LAS file is empty or does not contain any points.\n",
            "    - Example: The file is empty or only contains header information.\n",
            "\n",
            "- **Function `read_raw_las(filename)`:**\n",
            "\n",
            "  - Scenario: The LAS file contains raw data that needs to be filtered.\n",
            "    - Example: The file contains a point cloud with a wide range of classification values.\n",
            "  - Scenario: The LAS file does not contain any points that meet the filter criteria.\n",
            "    - Example: All points in the file have a classification value greater than 17.\n",
            "\n",
            "- **Function `read_subsampled_las(filename, dl)`:**\n",
            "\n",
            "  - Scenario: The LAS file contains raw data that needs to be subsampled.\n",
            "    - Example: The file contains a very dense point cloud that needs to be subsampled.\n",
            "  - Scenario: The radius for the sample filter is too large or too small.\n",
            "    - Example: The radius is set to a value that is larger than the extent of the point cloud or is set to 0.\n",
            "\n",
            "- **Function `write_las(filename, array)`:**\n",
            "\n",
            "  - Scenario: The data array is valid and can be written to a LAS file.\n",
            "    - Example: The array contains a point cloud with X, Y, Z coordinates and additional features.\n",
            "  - Scenario: The data array is empty or does not contain any points.\n",
            "    - Example: The array is empty or only contains header information.\n",
            "  - Scenario: The filename is already used by another file.\n",
            "    - Example: The filename is the same as an existing file in the same directory.\u001b[0m\n",
            "[user]\n",
            "In addition to those scenarios above, list a few rare or unexpected edge cases (and as before, under each edge case, include a few examples as sub-bullets).\n",
            "\u001b[92m\n",
            "[assistant]\n",
            "- **Function `read_las_points(filename)`:**\n",
            "\n",
            "  - Edge Case: The LAS file is corrupted or not properly formatted.\n",
            "    - Example: The file is not a valid LAS file or it has been corrupted during transfer.\n",
            "  - Edge Case: The LAS file is extremely large.\n",
            "    - Example: The file is several gigabytes in size, which might cause memory issues.\n",
            "\n",
            "- **Function `read_processed_las(filename)`:**\n",
            "\n",
            "  - Edge Case: The LAS file contains additional features that are not expected.\n",
            "    - Example: The file contains features that are not included in the function, causing the function to fail or skip these features.\n",
            "  - Edge Case: The additional features in the LAS file contain missing or NaN values.\n",
            "    - Example: Some points in the file have missing or NaN values for the 'Linearity', 'Planarity', 'Scattering', or 'Verticality' features.\n",
            "\n",
            "- **Function `read_raw_las(filename)`:**\n",
            "\n",
            "  - Edge Case: The LAS file contains points with negative or extremely large classification values.\n",
            "    - Example: The file contains points with a classification value of -1 or 1000, which might cause issues with the range filter.\n",
            "  \n",
            "- **Function `read_subsampled_las(filename, dl)`:**\n",
            "\n",
            "  - Edge Case: The LAS file contains points that are very far apart, causing the sample filter to return very few points.\n",
            "    - Example: The points in the file are spread out over a large area, and the radius for the sample filter is set to a small value.\n",
            "  \n",
            "- **Function `write_las(filename, array)`:**\n",
            "\n",
            "  - Edge Case: The data array contains non-standard dimensions or features.\n",
            "    - Example: The array contains dimensions that are not included in the standard LAS format, causing the writer to fail or skip these dimensions.\n",
            "  - Edge Case: The data array contains points with NaN or infinite values.\n",
            "    - Example: Some points in the array have NaN or infinite values for the X, Y, or Z coordinates or for the additional features.\u001b[0m\n",
            "[system]\n",
            "You are a world-class Python developer with an eagle eye for unintended bugs and edge cases. You write careful, accurate unit tests. When asked to reply only with code, you write all of your code in a single block.\n",
            "\u001b[0m\n",
            "[user]\n",
            "Using Python and the `pytest` package, write a suite of unit tests for the function, following the cases above. Include helpful comments to explain each line. Reply only with code, formatted as follows:\n",
            "\n",
            "```python\n",
            "# imports\n",
            "import pytest  # used for our unit tests\n",
            "{insert other imports as needed}\n",
            "\n",
            "# function to test\n",
            "import json\n",
            "import numpy as np\n",
            "import pdal\n",
            "\n",
            "def read_las_points(filename):\n",
            "    p = pdal.Pipeline(json.dumps([filename]))\n",
            "    p.validate()\n",
            "    p.execute()\n",
            "    data = p.arrays[0]\n",
            "    points = np.vstack((data['X'], data['Y'], data['Z'])).T\n",
            "    return points\n",
            "\n",
            "def read_processed_las(filename):\n",
            "    p = pdal.Pipeline(json.dumps([filename]))\n",
            "    p.validate()\n",
            "    p.execute()\n",
            "    data = p.arrays[0]\n",
            "    points = np.vstack((data['X'], data['Y'], data['Z'])).T\n",
            "    # intensity = np.expand_dims(data['Intensity'], 1).astype(np.float32)\n",
            "    # intensity = np.minimum(intensity, 255.0)/255.0\n",
            "    # features = intensity\n",
            "    # rn = data['ReturnNumber'].astype(np.float32)\n",
            "    # nr = data['NumberOfReturns'].astype(np.float32)\n",
            "    features = np.vstack((data['Linearity'],data['Planarity'],data['Scattering'],data['Verticality'])).T\n",
            "    # features = np.vstack((data['Eigenvalue0'],data['Eigenvalue1'],data['Eigenvalue2'])).T\n",
            "    # features = np.vstack((data['X'], data['Y'], data['Z'])).T\n",
            "    labels = data['Classification']\n",
            "    return points, features, labels\n",
            "\n",
            "def read_raw_las(filename):\n",
            "    p = pdal.Pipeline(json.dumps([\n",
            "        # filename\n",
            "        filename,\n",
            "        {\n",
            "            \"type\":\"filters.range\",\n",
            "            \"limits\":\"Classification(:17]\"\n",
            "        },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[1:1]=0\"\n",
            "        # },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[2:2]=1\"\n",
            "        # },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[7:7]=2\"\n",
            "        # },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[9:9]=3\"\n",
            "        # },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[17:17]=4\"\n",
            "        # },\n",
            "        {\n",
            "            \"type\":\"filters.covariancefeatures\"\n",
            "        }\n",
            "        # {\n",
            "        #     \"type\":\"filters.eigenvalues\",\n",
            "        #     \"knn\":10\n",
            "        # }\n",
            "    ]))\n",
            "    p.validate()\n",
            "    p.execute()\n",
            "    data = p.arrays[0]\n",
            "    points = np.vstack((data['X'], data['Y'], data['Z'])).T\n",
            "    # intensity = np.expand_dims(data['Intensity'], 1).astype(np.float32)\n",
            "    # intensity = np.minimum(intensity, 255.0)/255.0\n",
            "    # features = intensity\n",
            "    # rn = data['ReturnNumber'].astype(np.float32)\n",
            "    # nr = data['NumberOfReturns'].astype(np.float32)\n",
            "    features = np.vstack((data['Linearity'],data['Planarity'],data['Scattering'],data['Verticality'])).T\n",
            "    # features = np.vstack((data['Eigenvalue0'],data['Eigenvalue1'],data['Eigenvalue2'])).T\n",
            "    # features = np.vstack((data['X'], data['Y'], data['Z'])).T\n",
            "    labels = data['Classification']\n",
            "    return points, features, labels\n",
            "\n",
            "def read_subsampled_las(filename, dl):\n",
            "    p = pdal.Pipeline(json.dumps([\n",
            "        # filename\n",
            "        filename,\n",
            "        {\n",
            "            \"type\":\"filters.range\",\n",
            "            \"limits\":\"Classification(:17]\"\n",
            "        },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[1:1]=0\"\n",
            "        # },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[2:2]=1\"\n",
            "        # },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[7:7]=2\"\n",
            "        # },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[9:9]=3\"\n",
            "        # },\n",
            "        # {\n",
            "        #     \"type\":\"filters.assign\",\n",
            "        #     \"assignment\":\"Classification[17:17]=4\"\n",
            "        # },\n",
            "        {\n",
            "            \"type\":\"filters.covariancefeatures\"\n",
            "        },\n",
            "        {\n",
            "            \"type\":\"filters.sample\",\n",
            "            \"radius\":dl\n",
            "        }\n",
            "        # {\n",
            "        #     \"type\":\"filters.eigenvalues\",\n",
            "        #     \"knn\":10\n",
            "        # }\n",
            "    ]))\n",
            "    p.validate()\n",
            "    p.execute()\n",
            "    data = p.arrays[0]\n",
            "    points = np.vstack((data['X'], data['Y'], data['Z'])).T\n",
            "    # intensity = np.expand_dims(data['Intensity'], 1).astype(np.float32)\n",
            "    # intensity = np.minimum(intensity, 255.0)/255.0\n",
            "    # features = intensity\n",
            "    # rn = data['ReturnNumber'].astype(np.float32)\n",
            "    # nr = data['NumberOfReturns'].astype(np.float32)\n",
            "    features = np.vstack((data['Linearity'],data['Planarity'],data['Scattering'],data['Verticality'])).T\n",
            "    # features = np.vstack((data['Eigenvalue0'],data['Eigenvalue1'],data['Eigenvalue2'])).T\n",
            "    # features = np.vstack((data['X'], data['Y'], data['Z'])).T\n",
            "    labels = data['Classification']\n",
            "    return points, features, labels\n",
            "\n",
            "def write_las(filename, array):\n",
            "    # merge the fields then\n",
            "    p = pdal.Pipeline(json.dumps([{\n",
            "        \"type\":\"writers.las\",\n",
            "        \"filename\":filename,\n",
            "        # \"offset_x\":\"auto\",\n",
            "        # \"offset_y\":\"auto\",\n",
            "        # \"offset_z\":\"auto\",\n",
            "        # \"scale_x\":0.01,\n",
            "        # \"scale_y\":0.01,\n",
            "        # \"scale_z\":0.01\n",
            "        \"forward\":\"all\",\n",
            "        \"minor_version\":4,\n",
            "        \"extra_dims\":\"all\"\n",
            "        }]), [array])\n",
            "    p.validate()\n",
            "    p.execute()\n",
            "    return True\n",
            "\n",
            "# unit tests\n",
            "# below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\n",
            "{insert unit test code here}\n",
            "```\n",
            "\u001b[92m\n",
            "[assistant]\n",
            "```python\n",
            "# imports\n",
            "import pytest  # used for our unit tests\n",
            "\n",
            "# function to test\n",
            "# ... (functions are already defined above)\n",
            "\n",
            "# unit tests\n",
            "# below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\n",
            "\n",
            "# Test for the function read_las_points\n",
            "def test_read_las_points():\n",
            "    # Assuming we have a valid LAS file \"valid.las\" and an invalid LAS file \"invalid.las\"\n",
            "    assert isinstance(read_las_points(\"valid.las\"), np.ndarray)\n",
            "    with pytest.raises(Exception):\n",
            "        read_las_points(\"invalid.las\")\n",
            "\n",
            "# Test for the function read_processed_las\n",
            "def test_read_processed_las():\n",
            "    # Assuming we have a valid LAS file \"valid.las\" and an invalid LAS file \"invalid.las\"\n",
            "    points, features, labels = read_processed_las(\"valid.las\")\n",
            "    assert isinstance(points, np.ndarray)\n",
            "    assert isinstance(features, np.ndarray)\n",
            "    assert isinstance(labels, np.ndarray)\n",
            "    with pytest.raises(Exception):\n",
            "        read_processed_las(\"invalid.las\")\n",
            "\n",
            "# Test for the function read_raw_las\n",
            "def test_read_raw_las():\n",
            "    # Assuming we have a valid LAS file \"valid.las\" and an invalid LAS file \"invalid.las\"\n",
            "    points, features, labels = read_raw_las(\"valid.las\")\n",
            "    assert isinstance(points, np.ndarray)\n",
            "    assert isinstance(features, np.ndarray)\n",
            "    assert isinstance(labels, np.ndarray)\n",
            "    with pytest.raises(Exception):\n",
            "        read_raw_las(\"invalid.las\")\n",
            "\n",
            "# Test for the function read_subsampled_las\n",
            "def test_read_subsampled_las():\n",
            "    # Assuming we have a valid LAS file \"valid.las\" and an invalid LAS file \"invalid.las\"\n",
            "    points, features, labels = read_subsampled_las(\"valid.las\", 1.0)\n",
            "    assert isinstance(points, np.ndarray)\n",
            "    assert isinstance(features, np.ndarray)\n",
            "    assert isinstance(labels, np.ndarray)\n",
            "    with pytest.raises(Exception):\n",
            "        read_subsampled_las(\"invalid.las\", 1.0)\n",
            "\n",
            "# Test for the function write_las\n",
            "def test_write_las():\n",
            "    # Assuming we have a valid numpy array \"valid_array\"\n",
            "    valid_array = np.array([(0, 0, 0)], dtype=[('X', '<f8'), ('Y', '<f8'), ('Z', '<f8')])\n",
            "    assert write_las(\"test.las\", valid_array)\n",
            "    with pytest.raises(Exception):\n",
            "        write_las(\"test.las\", \"invalid_array\")\n",
            "```\n",
            "\n",
            "Please note that the actual tests would require valid and invalid LAS files and numpy arrays, which are not provided in this example. The above tests are just a starting point and may need to be expanded or modified based on the specific requirements and use cases."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unit_tests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "vcF3S7BNkTL7",
        "outputId": "3583ec58-1c6f-4e55-da42-8e849e488f40"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# imports\\nimport pytest  # used for our unit tests\\n\\n# function to test\\n# ... (functions are already defined above)\\n\\n# unit tests\\n# below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\\n\\n# Test for the function read_las_points\\ndef test_read_las_points():\\n    # Assuming we have a valid LAS file \"valid.las\" and an invalid LAS file \"invalid.las\"\\n    assert isinstance(read_las_points(\"valid.las\"), np.ndarray)\\n    with pytest.raises(Exception):\\n        read_las_points(\"invalid.las\")\\n\\n# Test for the function read_processed_las\\ndef test_read_processed_las():\\n    # Assuming we have a valid LAS file \"valid.las\" and an invalid LAS file \"invalid.las\"\\n    points, features, labels = read_processed_las(\"valid.las\")\\n    assert isinstance(points, np.ndarray)\\n    assert isinstance(features, np.ndarray)\\n    assert isinstance(labels, np.ndarray)\\n    with pytest.raises(Exception):\\n        read_processed_las(\"invalid.las\")\\n\\n# Test for the function read_raw_las\\ndef test_read_raw_las():\\n    # Assuming we have a valid LAS file \"valid.las\" and an invalid LAS file \"invalid.las\"\\n    points, features, labels = read_raw_las(\"valid.las\")\\n    assert isinstance(points, np.ndarray)\\n    assert isinstance(features, np.ndarray)\\n    assert isinstance(labels, np.ndarray)\\n    with pytest.raises(Exception):\\n        read_raw_las(\"invalid.las\")\\n\\n# Test for the function read_subsampled_las\\ndef test_read_subsampled_las():\\n    # Assuming we have a valid LAS file \"valid.las\" and an invalid LAS file \"invalid.las\"\\n    points, features, labels = read_subsampled_las(\"valid.las\", 1.0)\\n    assert isinstance(points, np.ndarray)\\n    assert isinstance(features, np.ndarray)\\n    assert isinstance(labels, np.ndarray)\\n    with pytest.raises(Exception):\\n        read_subsampled_las(\"invalid.las\", 1.0)\\n\\n# Test for the function write_las\\ndef test_write_las():\\n    # Assuming we have a valid numpy array \"valid_array\"\\n    valid_array = np.array([(0, 0, 0)], dtype=[(\\'X\\', \\'<f8\\'), (\\'Y\\', \\'<f8\\'), (\\'Z\\', \\'<f8\\')])\\n    assert write_las(\"test.las\", valid_array)\\n    with pytest.raises(Exception):\\n        write_las(\"test.las\", \"invalid_array\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5xveOQI3oTdh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}